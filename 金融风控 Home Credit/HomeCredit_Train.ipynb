{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuj0xYkoBRmM",
        "outputId": "49d49ff3-b5a1-4d4b-f81c-1e63a1812a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "token = {\"username\":\"****\",\"key\":\"******************\"}\n",
        "with open('/content/kaggle.json', 'w') as file:\n",
        "  json.dump(token, file)"
      ],
      "metadata": {
        "id": "iMJg7qEJBRdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v /content\n",
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eu1B8SsBRPG",
        "outputId": "e36bc49b-a9ef-49ac-a98e-5bf7787cb45a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- path is now set to: /content\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c home-credit-credit-risk-model-stability"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz8Vx_34B3jr",
        "outputId": "2b783799-9d18-4d1c-e273-f3b3af85620a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading home-credit-credit-risk-model-stability.zip to /content/competitions/home-credit-credit-risk-model-stability\n",
            "100% 3.14G/3.14G [01:19<00:00, 36.2MB/s]\n",
            "100% 3.14G/3.14G [01:19<00:00, 42.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/competitions/home-credit-credit-risk-model-stability/home-credit-credit-risk-model-stability.zip' -d '/content/home-credit'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmXTahA7B_6A",
        "outputId": "51432ff0-696c-4398-c01b-ab7832abcfd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/competitions/home-credit-credit-risk-model-stability/home-credit-credit-risk-model-stability.zip\n",
            "  inflating: /content/home-credit/csv_files/test/test_applprev_1_0.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_applprev_1_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_applprev_1_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_applprev_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_base.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_1_0.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_1_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_1_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_1_3.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_1_4.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_0.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_10.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_11.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_3.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_4.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_5.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_6.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_7.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_8.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_a_2_9.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_b_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_credit_bureau_b_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_debitcard_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_deposit_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_other_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_person_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_person_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_static_0_0.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_static_0_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_static_0_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_static_cb_0.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_tax_registry_a_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_tax_registry_b_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/test/test_tax_registry_c_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_applprev_1_0.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_applprev_1_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_applprev_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_base.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_1_0.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_1_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_1_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_1_3.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_0.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_10.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_3.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_4.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_5.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_6.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_7.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_8.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_a_2_9.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_b_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_credit_bureau_b_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_debitcard_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_deposit_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_other_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_person_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_person_2.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_static_0_0.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_static_0_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_static_cb_0.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_tax_registry_a_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_tax_registry_b_1.csv  \n",
            "  inflating: /content/home-credit/csv_files/train/train_tax_registry_c_1.csv  \n",
            "  inflating: /content/home-credit/feature_definitions.csv  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_applprev_1_0.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_applprev_1_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_applprev_1_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_applprev_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_base.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_1_0.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_1_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_1_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_1_3.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_1_4.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_0.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_10.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_11.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_3.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_4.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_5.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_6.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_7.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_8.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_a_2_9.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_b_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_credit_bureau_b_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_debitcard_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_deposit_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_other_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_person_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_person_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_static_0_0.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_static_0_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_static_0_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_static_cb_0.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_tax_registry_a_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_tax_registry_b_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/test/test_tax_registry_c_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_applprev_1_0.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_applprev_1_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_applprev_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_base.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_1_0.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_1_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_1_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_1_3.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_0.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_10.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_3.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_4.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_5.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_6.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_7.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_8.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_a_2_9.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_b_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_credit_bureau_b_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_debitcard_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_deposit_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_other_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_person_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_person_2.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_static_0_0.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_static_0_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_static_cb_0.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_tax_registry_a_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_tax_registry_b_1.parquet  \n",
            "  inflating: /content/home-credit/parquet_files/train/train_tax_registry_c_1.parquet  \n",
            "  inflating: /content/home-credit/sample_submission.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g0_3IFKqDnr",
        "outputId": "40339b00-9e39-4b9c-e10e-e5849326fbe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.3.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ-SzbJIqbbe",
        "outputId": "ab3da03f-eec1-496d-8cf8-e67e84bd58af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: polars 0.20.2\n",
            "Uninstalling polars-0.20.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/polars-0.20.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/polars/*\n",
            "    /usr/local/lib/python3.10/dist-packages/rust-toolchain.toml\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled polars-0.20.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hFound existing installation: lightgbm 4.1.0\n",
            "Uninstalling lightgbm-4.1.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/lightgbm-4.1.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/lightgbm/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled lightgbm-4.1.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip uninstall polars\n",
        "!pip install -q polars==0.20.18\n",
        "!pip uninstall lightgbm\n",
        "!pip install -q lightgbm==4.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8O3f6gGonTC"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import lightgbm as lgb  # type: ignore\n",
        "import numpy as np  # type: ignore\n",
        "import pandas as pd  # type: ignore\n",
        "import polars as pl  # type: ignore\n",
        "import warnings\n",
        "\n",
        "from catboost import CatBoostClassifier, Pool  # type: ignore\n",
        "from glob import glob\n",
        "from IPython.display import display  # type: ignore\n",
        "from pathlib import Path\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin  # type: ignore\n",
        "from sklearn.metrics import roc_auc_score  # type: ignore\n",
        "from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n",
        "from typing import Any\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ROOT = Path(\"/content/home-credit\")\n",
        "TRAIN_DIR = ROOT / \"parquet_files\" / \"train\"\n",
        "TEST_DIR = ROOT / \"parquet_files\" / \"test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-8TW5tIpPLv"
      },
      "outputs": [],
      "source": [
        "class Utility:\n",
        "    @staticmethod\n",
        "    def get_feat_defs(ending_with: str) -> None:\n",
        "        \"\"\"\n",
        "        Retrieves feature definitions from a CSV file based on the specified ending.\n",
        "\n",
        "        Args:\n",
        "        - ending_with (str): Ending to filter feature definitions.\n",
        "\n",
        "        Returns:\n",
        "        - pl.DataFrame: Filtered feature definitions.\n",
        "        \"\"\"\n",
        "        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n",
        "\n",
        "        filtered_feats: pl.DataFrame = feat_defs.filter(\n",
        "            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n",
        "        )\n",
        "\n",
        "        with pl.Config(fmt_str_lengths=200, tbl_rows=-1):\n",
        "            print(filtered_feats)\n",
        "\n",
        "        filtered_feats = None\n",
        "        feat_defs = None\n",
        "\n",
        "    @staticmethod\n",
        "    def find_index(lst: list[Any], item: Any) -> int | None:\n",
        "        \"\"\"\n",
        "        Finds the index of an item in a list.\n",
        "\n",
        "        Args:\n",
        "        - lst (list): List to search.\n",
        "        - item (Any): Item to find in the list.\n",
        "\n",
        "        Returns:\n",
        "        - int | None: Index of the item if found, otherwise None.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return lst.index(item)\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def dtype_to_str(dtype: pl.DataType) -> str:\n",
        "        \"\"\"\n",
        "        Converts Polars data type to string representation.\n",
        "\n",
        "        Args:\n",
        "        - dtype (pl.DataType): Polars data type.\n",
        "\n",
        "        Returns:\n",
        "        - str: String representation of the data type.\n",
        "        \"\"\"\n",
        "        dtype_map = {\n",
        "            pl.Decimal: \"Decimal\",\n",
        "            pl.Float32: \"Float32\",\n",
        "            pl.Float64: \"Float64\",\n",
        "            pl.UInt8: \"UInt8\",\n",
        "            pl.UInt16: \"UInt16\",\n",
        "            pl.UInt32: \"UInt32\",\n",
        "            pl.UInt64: \"UInt64\",\n",
        "            pl.Int8: \"Int8\",\n",
        "            pl.Int16: \"Int16\",\n",
        "            pl.Int32: \"Int32\",\n",
        "            pl.Int64: \"Int64\",\n",
        "            pl.Date: \"Date\",\n",
        "            pl.Datetime: \"Datetime\",\n",
        "            pl.Duration: \"Duration\",\n",
        "            pl.Time: \"Time\",\n",
        "            pl.Array: \"Array\",\n",
        "            pl.List: \"List\",\n",
        "            pl.Struct: \"Struct\",\n",
        "            pl.String: \"String\",\n",
        "            pl.Categorical: \"Categorical\",\n",
        "            pl.Enum: \"Enum\",\n",
        "            pl.Utf8: \"Utf8\",\n",
        "            pl.Binary: \"Binary\",\n",
        "            pl.Boolean: \"Boolean\",\n",
        "            pl.Null: \"Null\",\n",
        "            pl.Object: \"Object\",\n",
        "            pl.Unknown: \"Unknown\",\n",
        "        }\n",
        "\n",
        "        return dtype_map.get(dtype)\n",
        "\n",
        "    @staticmethod\n",
        "    def find_feat_occur(regex_path: str, ending_with: str) -> pl.DataFrame:\n",
        "        \"\"\"\n",
        "        Finds occurrences of features ending with a specific string in Parquet files.\n",
        "\n",
        "        Args:\n",
        "        - regex_path (str): Regular expression to match Parquet file paths.\n",
        "        - ending_with (str): Ending to filter feature names.\n",
        "\n",
        "        Returns:\n",
        "        - pl.DataFrame: DataFrame containing feature definitions, data types, and file locations.\n",
        "        \"\"\"\n",
        "        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\").filter(\n",
        "            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n",
        "        )\n",
        "        feat_defs.sort(by=[\"Variable\"])\n",
        "\n",
        "        feats: list[pl.String] = feat_defs[\"Variable\"].to_list()\n",
        "        feats.sort()\n",
        "\n",
        "        occurrences: list[list] = [[set(), set()] for _ in range(feat_defs.height)]\n",
        "\n",
        "        for path in glob(str(regex_path)):\n",
        "            df_schema: dict = pl.read_parquet_schema(path)\n",
        "\n",
        "            for feat, dtype in df_schema.items():\n",
        "                index: int = Utility.find_index(feats, feat)\n",
        "                if index != None:\n",
        "                    occurrences[index][0].add(Utility.dtype_to_str(dtype))\n",
        "                    occurrences[index][1].add(Path(path).stem)\n",
        "\n",
        "        data_types: list[str] = [None] * feat_defs.height\n",
        "        file_locs: list[str] = [None] * feat_defs.height\n",
        "\n",
        "        for i, feat in enumerate(feats):\n",
        "            data_types[i] = list(occurrences[i][0])\n",
        "            file_locs[i] = list(occurrences[i][1])\n",
        "\n",
        "        feat_defs = feat_defs.with_columns(pl.Series(data_types).alias(\"Data_Type(s)\"))\n",
        "        feat_defs = feat_defs.with_columns(pl.Series(file_locs).alias(\"File_Loc(s)\"))\n",
        "\n",
        "        return feat_defs\n",
        "\n",
        "    def reduce_memory_usage(df: pl.DataFrame, name) -> pl.DataFrame:\n",
        "        \"\"\"\n",
        "        Reduces memory usage of a DataFrame by converting column types.\n",
        "\n",
        "        Args:\n",
        "        - df (pl.DataFrame): DataFrame to optimize.\n",
        "        - name (str): Name of the DataFrame.\n",
        "\n",
        "        Returns:\n",
        "        - pl.DataFrame: Optimized DataFrame.\n",
        "        \"\"\"\n",
        "        print(\n",
        "            f\"Memory usage of dataframe \\\"{name}\\\" is {round(df.estimated_size('mb'), 4)} MB.\"\n",
        "        )\n",
        "\n",
        "        int_types = [\n",
        "            pl.Int8,\n",
        "            pl.Int16,\n",
        "            pl.Int32,\n",
        "            pl.Int64,\n",
        "            pl.UInt8,\n",
        "            pl.UInt16,\n",
        "            pl.UInt32,\n",
        "            pl.UInt64,\n",
        "        ]\n",
        "        float_types = [pl.Float32, pl.Float64]\n",
        "\n",
        "        for col in df.columns:\n",
        "            col_type = df[col].dtype\n",
        "            if col_type in int_types + float_types:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "\n",
        "                if c_min is not None and c_max is not None:\n",
        "                    if col_type in int_types:\n",
        "                        if c_min >= 0:\n",
        "                            if (\n",
        "                                c_min >= np.iinfo(np.uint8).min\n",
        "                                and c_max <= np.iinfo(np.uint8).max\n",
        "                            ):\n",
        "                                df = df.with_columns(df[col].cast(pl.UInt8))\n",
        "                            elif (\n",
        "                                c_min >= np.iinfo(np.uint16).min\n",
        "                                and c_max <= np.iinfo(np.uint16).max\n",
        "                            ):\n",
        "                                df = df.with_columns(df[col].cast(pl.UInt16))\n",
        "                            elif (\n",
        "                                c_min >= np.iinfo(np.uint32).min\n",
        "                                and c_max <= np.iinfo(np.uint32).max\n",
        "                            ):\n",
        "                                df = df.with_columns(df[col].cast(pl.UInt32))\n",
        "                            elif (\n",
        "                                c_min >= np.iinfo(np.uint64).min\n",
        "                                and c_max <= np.iinfo(np.uint64).max\n",
        "                            ):\n",
        "                                df = df.with_columns(df[col].cast(pl.UInt64))\n",
        "                        else:\n",
        "                            if (\n",
        "                                c_min >= np.iinfo(np.int8).min\n",
        "                                and c_max <= np.iinfo(np.int8).max\n",
        "                            ):\n",
        "                                df = df.with_columns(df[col].cast(pl.Int8))\n",
        "                            elif (\n",
        "                                c_min >= np.iinfo(np.int16).min\n",
        "                                and c_max <= np.iinfo(np.int16).max\n",
        "                            ):\n",
        "                                df = df.with_columns(df[col].cast(pl.Int16))\n",
        "                            elif (\n",
        "                                c_min >= np.iinfo(np.int32).min\n",
        "                                and c_max <= np.iinfo(np.int32).max\n",
        "                            ):\n",
        "                                df = df.with_columns(df[col].cast(pl.Int32))\n",
        "                            elif (\n",
        "                                c_min >= np.iinfo(np.int64).min\n",
        "                                and c_max <= np.iinfo(np.int64).max\n",
        "                            ):\n",
        "                                df = df.with_columns(df[col].cast(pl.Int64))\n",
        "                    elif col_type in float_types:\n",
        "                        if (\n",
        "                            c_min > np.finfo(np.float32).min\n",
        "                            and c_max < np.finfo(np.float32).max\n",
        "                        ):\n",
        "                            df = df.with_columns(df[col].cast(pl.Float32))\n",
        "\n",
        "        print(\n",
        "            f\"Memory usage of dataframe \\\"{name}\\\" became {round(df.estimated_size('mb'), 4)} MB.\"\n",
        "        )\n",
        "\n",
        "        return df\n",
        "\n",
        "    def to_pandas(df: pl.DataFrame, cat_cols: list[str] = None) -> (pd.DataFrame, list[str]):  # type: ignore\n",
        "        \"\"\"\n",
        "        Converts a Polars DataFrame to a Pandas DataFrame.\n",
        "\n",
        "        Args:\n",
        "        - df (pl.DataFrame): Polars DataFrame to convert.\n",
        "        - cat_cols (list[str]): List of categorical columns. Default is None.\n",
        "\n",
        "        Returns:\n",
        "        - (pd.DataFrame, list[str]): Tuple containing the converted Pandas DataFrame and categorical columns.\n",
        "        \"\"\"\n",
        "        df: pd.DataFrame = df.to_pandas()\n",
        "\n",
        "        if cat_cols is None:\n",
        "            cat_cols = list(df.select_dtypes(\"object\").columns)\n",
        "\n",
        "        df[cat_cols] = df[cat_cols].astype(\"str\")\n",
        "\n",
        "        return df, cat_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "331b9UCfpeGT"
      },
      "outputs": [],
      "source": [
        "class Aggregator:\n",
        "    @staticmethod\n",
        "    def max_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
        "        \"\"\"\n",
        "        Generates expressions for calculating maximum values for specific columns.\n",
        "\n",
        "        Args:\n",
        "        - df (pl.LazyFrame): Input LazyFrame.\n",
        "\n",
        "        Returns:\n",
        "        - list[pl.Series]: List of expressions for maximum values.\n",
        "        \"\"\"\n",
        "        cols: list[str] = [\n",
        "            col\n",
        "            for col in df.columns\n",
        "            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n",
        "        ]\n",
        "\n",
        "        expr_max: list[pl.Series] = [\n",
        "            pl.col(col).max().alias(f\"max_{col}\") for col in cols\n",
        "        ]\n",
        "\n",
        "        return expr_max\n",
        "\n",
        "    @staticmethod\n",
        "    def min_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
        "        \"\"\"\n",
        "        Generates expressions for calculating minimum values for specific columns.\n",
        "\n",
        "        Args:\n",
        "        - df (pl.LazyFrame): Input LazyFrame.\n",
        "\n",
        "        Returns:\n",
        "        - list[pl.Series]: List of expressions for minimum values.\n",
        "        \"\"\"\n",
        "        cols: list[str] = [\n",
        "            col\n",
        "            for col in df.columns\n",
        "            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n",
        "        ]\n",
        "\n",
        "        expr_min: list[pl.Series] = [\n",
        "            pl.col(col).min().alias(f\"min_{col}\") for col in cols\n",
        "        ]\n",
        "\n",
        "        return expr_min\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
        "        \"\"\"\n",
        "        Generates expressions for calculating mean values for specific columns.\n",
        "\n",
        "        Args:\n",
        "        - df (pl.LazyFrame): Input LazyFrame.\n",
        "\n",
        "        Returns:\n",
        "        - list[pl.Series]: List of expressions for mean values.\n",
        "        \"\"\"\n",
        "        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n",
        "\n",
        "        expr_mean: list[pl.Series] = [\n",
        "            pl.col(col).mean().alias(f\"mean_{col}\") for col in cols\n",
        "        ]\n",
        "\n",
        "        return expr_mean\n",
        "\n",
        "    @staticmethod\n",
        "    def var_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
        "        \"\"\"\n",
        "        Generates expressions for calculating variance for specific columns.\n",
        "\n",
        "        Args:\n",
        "        - df (pl.LazyFrame): Input LazyFrame.\n",
        "\n",
        "        Returns:\n",
        "        - list[pl.Series]: List of expressions for variance.\n",
        "        \"\"\"\n",
        "        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n",
        "\n",
        "        expr_mean: list[pl.Series] = [\n",
        "            pl.col(col).var().alias(f\"var_{col}\") for col in cols\n",
        "        ]\n",
        "\n",
        "        return expr_mean\n",
        "\n",
        "    @staticmethod\n",
        "    def mode_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
        "        \"\"\"\n",
        "        Generates expressions for calculating mode values for specific columns.\n",
        "\n",
        "        Args:\n",
        "        - df (pl.LazyFrame): Input LazyFrame.\n",
        "\n",
        "        Returns:\n",
        "        - list[pl.Series]: List of expressions for mode values.\n",
        "        \"\"\"\n",
        "        cols: list[str] = [col for col in df.columns if col.endswith(\"M\")]\n",
        "\n",
        "        expr_mode: list[pl.Series] = [\n",
        "            pl.col(col).drop_nulls().mode().first().alias(f\"mode_{col}\") for col in cols\n",
        "        ]\n",
        "\n",
        "        return expr_mode\n",
        "\n",
        "    @staticmethod\n",
        "    def get_exprs(df: pl.LazyFrame) -> list[pl.Series]:\n",
        "        \"\"\"\n",
        "        Combines expressions for maximum, mean, and variance calculations.\n",
        "\n",
        "        Args:\n",
        "        - df (pl.LazyFrame): Input LazyFrame.\n",
        "\n",
        "        Returns:\n",
        "        - list[pl.Series]: List of combined expressions.\n",
        "        \"\"\"\n",
        "        exprs = (\n",
        "            Aggregator.max_expr(df) + Aggregator.mean_expr(df) + Aggregator.var_expr(df)\n",
        "        )\n",
        "\n",
        "        return exprs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNDqhu3ipgmy"
      },
      "outputs": [],
      "source": [
        "class SchemaGen:\n",
        "    @staticmethod\n",
        "    def change_dtypes(df: pl.LazyFrame) -> pl.LazyFrame:\n",
        "        \"\"\"\n",
        "        Changes the data types of columns in the DataFrame.\n",
        "\n",
        "        Args:\n",
        "        - df (pl.LazyFrame): Input LazyFrame.\n",
        "\n",
        "        Returns:\n",
        "        - pl.LazyFrame: LazyFrame with modified data types.\n",
        "        \"\"\"\n",
        "        for col in df.columns:\n",
        "            if col == \"case_id\":\n",
        "                df = df.with_columns(pl.col(col).cast(pl.UInt32).alias(col))\n",
        "            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
        "                df = df.with_columns(pl.col(col).cast(pl.UInt16).alias(col))\n",
        "            elif col == \"date_decision\" or col[-1] == \"D\":\n",
        "                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n",
        "            elif col[-1] in [\"P\", \"A\"]:\n",
        "                df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n",
        "            elif col[-1] in (\"M\",):\n",
        "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def scan_files(glob_path: str, depth: int = None) -> pl.LazyFrame:\n",
        "        \"\"\"\n",
        "        Scans Parquet files matching the glob pattern and combines them into a LazyFrame.\n",
        "\n",
        "        Args:\n",
        "        - glob_path (str): Glob pattern to match Parquet files.\n",
        "        - depth (int, optional): Depth level for data aggregation. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "        - pl.LazyFrame: Combined LazyFrame.\n",
        "        \"\"\"\n",
        "        chunks: list[pl.LazyFrame] = []\n",
        "        for path in glob(str(glob_path)):\n",
        "            df: pl.LazyFrame = pl.scan_parquet(\n",
        "                path, low_memory=True, rechunk=True\n",
        "            ).pipe(SchemaGen.change_dtypes)\n",
        "            print(f\"File {Path(path).stem} loaded into memory.\")\n",
        "\n",
        "            if depth in (1, 2):\n",
        "                exprs: list[pl.Series] = Aggregator.get_exprs(df)\n",
        "                df = df.group_by(\"case_id\").agg(exprs)\n",
        "\n",
        "                del exprs\n",
        "                gc.collect()\n",
        "\n",
        "            chunks.append(df)\n",
        "\n",
        "        df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
        "\n",
        "        del chunks\n",
        "        gc.collect()\n",
        "\n",
        "        df = df.unique(subset=[\"case_id\"])\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def join_dataframes(\n",
        "        df_base: pl.LazyFrame,\n",
        "        depth_0: list[pl.LazyFrame],\n",
        "        depth_1: list[pl.LazyFrame],\n",
        "        depth_2: list[pl.LazyFrame],\n",
        "    ) -> pl.DataFrame:\n",
        "        \"\"\"\n",
        "        Joins multiple LazyFrames with a base LazyFrame.\n",
        "\n",
        "        Args:\n",
        "        - df_base (pl.LazyFrame): Base LazyFrame.\n",
        "        - depth_0 (list[pl.LazyFrame]): List of LazyFrames for depth 0.\n",
        "        - depth_1 (list[pl.LazyFrame]): List of LazyFrames for depth 1.\n",
        "        - depth_2 (list[pl.LazyFrame]): List of LazyFrames for depth 2.\n",
        "\n",
        "        Returns:\n",
        "        - pl.DataFrame: Joined DataFrame.\n",
        "        \"\"\"\n",
        "        for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
        "            df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
        "\n",
        "        return df_base.collect().pipe(Utility.reduce_memory_usage, \"df_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpXEa9v-piMt"
      },
      "outputs": [],
      "source": [
        "def filter_cols(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters columns in the DataFrame based on null percentage and unique values for string columns.\n",
        "\n",
        "    Args:\n",
        "    - df (pl.DataFrame): Input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    - pl.DataFrame: DataFrame with filtered columns.\n",
        "    \"\"\"\n",
        "    for col in df.columns:\n",
        "        if col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]:\n",
        "            null_pct = df[col].is_null().mean()\n",
        "\n",
        "            if null_pct > 0.95:\n",
        "                df = df.drop(col)\n",
        "\n",
        "    for col in df.columns:\n",
        "        if (col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]) & (\n",
        "            df[col].dtype == pl.String\n",
        "        ):\n",
        "            freq = df[col].n_unique()\n",
        "\n",
        "            if (freq > 200) | (freq == 1):\n",
        "                df = df.drop(col)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def transform_cols(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Transforms columns in the DataFrame according to predefined rules.\n",
        "\n",
        "    Args:\n",
        "    - df (pl.DataFrame): Input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    - pl.DataFrame: DataFrame with transformed columns.\n",
        "    \"\"\"\n",
        "    if \"riskassesment_302T\" in df.columns:\n",
        "        if df[\"riskassesment_302T\"].dtype == pl.Null:\n",
        "            df = df.with_columns(\n",
        "                [\n",
        "                    pl.Series(\n",
        "                        \"riskassesment_302T_rng\", df[\"riskassesment_302T\"], pl.UInt8\n",
        "                    ),\n",
        "                    pl.Series(\n",
        "                        \"riskassesment_302T_mean\", df[\"riskassesment_302T\"], pl.UInt8\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            pct_low: pl.Series = (\n",
        "                df[\"riskassesment_302T\"]\n",
        "                .str.split(\" - \")\n",
        "                .apply(lambda x: x[0].replace(\"%\", \"\"))\n",
        "                .cast(pl.UInt8)\n",
        "            )\n",
        "            pct_high: pl.Series = (\n",
        "                df[\"riskassesment_302T\"]\n",
        "                .str.split(\" - \")\n",
        "                .apply(lambda x: x[1].replace(\"%\", \"\"))\n",
        "                .cast(pl.UInt8)\n",
        "            )\n",
        "\n",
        "            diff: pl.Series = pct_high - pct_low\n",
        "            avg: pl.Series = ((pct_low + pct_high) / 2).cast(pl.Float32)\n",
        "\n",
        "            del pct_high, pct_low\n",
        "            gc.collect()\n",
        "\n",
        "            df = df.with_columns(\n",
        "                [\n",
        "                    diff.alias(\"riskassesment_302T_rng\"),\n",
        "                    avg.alias(\"riskassesment_302T_mean\"),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        df.drop(\"riskassesment_302T\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def handle_dates(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Handles date columns in the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    - df (pl.DataFrame): Input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    - pl.DataFrame: DataFrame with transformed date columns.\n",
        "    \"\"\"\n",
        "    for col in df.columns:\n",
        "        if col.endswith(\"D\"):\n",
        "            df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
        "            df = df.with_columns(pl.col(col).dt.total_days().cast(pl.Int32))\n",
        "\n",
        "    df = df.rename(\n",
        "        {\n",
        "            \"MONTH\": \"month\",\n",
        "            \"WEEK_NUM\": \"week_num\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    df = df.with_columns(\n",
        "        [\n",
        "            pl.col(\"date_decision\").dt.year().alias(\"year\").cast(pl.Int16),\n",
        "            pl.col(\"date_decision\").dt.day().alias(\"day\").cast(pl.UInt8),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return df.drop(\"date_decision\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GEQ24AvqpiTu",
        "outputId": "8d6461f6-c3e0-4d12-8e78-5e33ceb94a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File train_base loaded into memory.\n",
            "File train_static_cb_0 loaded into memory.\n",
            "File train_static_0_1 loaded into memory.\n",
            "File train_static_0_0 loaded into memory.\n",
            "File train_applprev_1_0 loaded into memory.\n",
            "File train_applprev_1_1 loaded into memory.\n",
            "File train_tax_registry_a_1 loaded into memory.\n",
            "File train_tax_registry_b_1 loaded into memory.\n",
            "File train_tax_registry_c_1 loaded into memory.\n",
            "File train_credit_bureau_a_1_2 loaded into memory.\n",
            "File train_credit_bureau_a_1_3 loaded into memory.\n",
            "File train_credit_bureau_a_1_0 loaded into memory.\n",
            "File train_credit_bureau_a_1_1 loaded into memory.\n",
            "File train_credit_bureau_b_1 loaded into memory.\n",
            "File train_other_1 loaded into memory.\n",
            "File train_person_1 loaded into memory.\n",
            "File train_deposit_1 loaded into memory.\n",
            "File train_debitcard_1 loaded into memory.\n",
            "File train_credit_bureau_a_2_8 loaded into memory.\n",
            "File train_credit_bureau_a_2_5 loaded into memory.\n",
            "File train_credit_bureau_a_2_9 loaded into memory.\n",
            "File train_credit_bureau_a_2_0 loaded into memory.\n",
            "File train_credit_bureau_a_2_1 loaded into memory.\n",
            "File train_credit_bureau_a_2_4 loaded into memory.\n",
            "File train_credit_bureau_a_2_2 loaded into memory.\n",
            "File train_credit_bureau_a_2_6 loaded into memory.\n",
            "File train_credit_bureau_a_2_7 loaded into memory.\n",
            "File train_credit_bureau_a_2_10 loaded into memory.\n",
            "File train_credit_bureau_a_2_3 loaded into memory.\n",
            "File train_credit_bureau_b_2 loaded into memory.\n",
            "Memory usage of dataframe \"df_train\" is 6783.1317 MB.\n",
            "Memory usage of dataframe \"df_train\" became 4174.0953 MB.\n",
            "Memory usage of dataframe \"df_train\" is 2870.9171 MB.\n",
            "Memory usage of dataframe \"df_train\" became 2665.6302 MB.\n",
            "Train data shape: (1526659, 472)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "shape: (10, 472)\n",
              "┌─────────┬────────┬──────────┬────────┬───┬─────────────────────┬────────────────────┬──────┬─────┐\n",
              "│ case_id ┆ month  ┆ week_num ┆ target ┆ … ┆ var_pmts_overdue_11 ┆ var_pmts_overdue_1 ┆ year ┆ day │\n",
              "│ ---     ┆ ---    ┆ ---      ┆ ---    ┆   ┆ 40A                 ┆ 152A               ┆ ---  ┆ --- │\n",
              "│ u32     ┆ u32    ┆ u8       ┆ u8     ┆   ┆ ---                 ┆ ---                ┆ u16  ┆ u8  │\n",
              "│         ┆        ┆          ┆        ┆   ┆ f32                 ┆ f32                ┆      ┆     │\n",
              "╞═════════╪════════╪══════════╪════════╪═══╪═════════════════════╪════════════════════╪══════╪═════╡\n",
              "│ 1356315 ┆ 201904 ┆ 17       ┆ 0      ┆ … ┆ null                ┆ null               ┆ 2019 ┆ 30  │\n",
              "│ 1385930 ┆ 201906 ┆ 21       ┆ 0      ┆ … ┆ 5006852.5           ┆ 1.7240264e7        ┆ 2019 ┆ 3   │\n",
              "│ 1625679 ┆ 201911 ┆ 45       ┆ 0      ┆ … ┆ 0.0                 ┆ 1.4223e9           ┆ 2019 ┆ 12  │\n",
              "│ 793459  ┆ 201909 ┆ 35       ┆ 1      ┆ … ┆ 6.0016316e7         ┆ 2.3076e9           ┆ 2019 ┆ 7   │\n",
              "│ 2560038 ┆ 201904 ┆ 13       ┆ 0      ┆ … ┆ 0.0                 ┆ null               ┆ 2019 ┆ 8   │\n",
              "│ 1006475 ┆ 202008 ┆ 85       ┆ 0      ┆ … ┆ null                ┆ null               ┆ 2020 ┆ 19  │\n",
              "│ 1545471 ┆ 201909 ┆ 37       ┆ 1      ┆ … ┆ 0.0                 ┆ 1.2893e6           ┆ 2019 ┆ 20  │\n",
              "│ 1788858 ┆ 202002 ┆ 58       ┆ 0      ┆ … ┆ 0.0                 ┆ 2.1974562e7        ┆ 2020 ┆ 17  │\n",
              "│ 766208  ┆ 201908 ┆ 31       ┆ 0      ┆ … ┆ null                ┆ 0.0                ┆ 2019 ┆ 10  │\n",
              "│ 1545706 ┆ 201909 ┆ 37       ┆ 0      ┆ … ┆ 0.0                 ┆ 0.0                ┆ 2019 ┆ 21  │\n",
              "└─────────┴────────┴──────────┴────────┴───┴─────────────────────┴────────────────────┴──────┴─────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10, 472)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>month</th><th>week_num</th><th>target</th><th>assignmentdate_238D</th><th>assignmentdate_4527235D</th><th>birthdate_574D</th><th>contractssum_5085716L</th><th>dateofbirth_337D</th><th>days120_123L</th><th>days180_256L</th><th>days30_165L</th><th>days360_512L</th><th>days90_310L</th><th>description_5085714M</th><th>education_1103M</th><th>education_88M</th><th>firstquarter_103L</th><th>fourthquarter_440L</th><th>maritalst_385M</th><th>maritalst_893M</th><th>numberofqueries_373L</th><th>pmtaverage_3A</th><th>pmtaverage_4527227A</th><th>pmtcount_4527229L</th><th>pmtcount_693L</th><th>pmtscount_423L</th><th>pmtssum_45A</th><th>requesttype_4525192L</th><th>responsedate_1012D</th><th>responsedate_4527233D</th><th>responsedate_4917613D</th><th>secondquarter_766L</th><th>thirdquarter_1082L</th><th>actualdpdtolerance_344P</th><th>amtinstpaidbefduel24m_4187115A</th><th>annuity_780A</th><th>&hellip;</th><th>mean_mainoccupationinc_384A</th><th>max_amount_416A</th><th>max_num_group1_10</th><th>max_openingdate_313D</th><th>mean_amount_416A</th><th>mean_openingdate_313D</th><th>max_num_group1_11</th><th>max_openingdate_857D</th><th>mean_openingdate_857D</th><th>max_collater_typofvalofguarant_298M</th><th>max_collater_typofvalofguarant_407M</th><th>max_collater_valueofguarantee_1124L</th><th>max_collater_valueofguarantee_876L</th><th>max_collaterals_typeofguarante_359M</th><th>max_collaterals_typeofguarante_669M</th><th>max_num_group1_12</th><th>max_num_group2</th><th>max_pmts_dpd_1073P</th><th>max_pmts_dpd_303P</th><th>max_pmts_month_158T</th><th>max_pmts_month_706T</th><th>max_pmts_overdue_1140A</th><th>max_pmts_overdue_1152A</th><th>max_pmts_year_1139T</th><th>max_pmts_year_507T</th><th>max_subjectroles_name_541M</th><th>max_subjectroles_name_838M</th><th>mean_pmts_dpd_1073P</th><th>mean_pmts_dpd_303P</th><th>mean_pmts_overdue_1140A</th><th>mean_pmts_overdue_1152A</th><th>var_pmts_dpd_1073P</th><th>var_pmts_dpd_303P</th><th>var_pmts_overdue_1140A</th><th>var_pmts_overdue_1152A</th><th>year</th><th>day</th></tr><tr><td>u32</td><td>u32</td><td>u8</td><td>u8</td><td>i16</td><td>u8</td><td>i16</td><td>f32</td><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>str</td><td>i8</td><td>u8</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>u8</td><td>i16</td><td>f32</td><td>i16</td><td>u8</td><td>i16</td><td>i16</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>u16</td><td>u8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>u16</td><td>u8</td></tr></thead><tbody><tr><td>1356315</td><td>201904</td><td>17</td><td>0</td><td>-2218</td><td>null</td><td>-23464</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>null</td><td>10348.866211</td><td>null</td><td>null</td><td>6.0</td><td>null</td><td>null</td><td>null</td><td>14</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>8338.600586</td><td>1760.800049</td><td>&hellip;</td><td>24000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2019</td><td>30</td></tr><tr><td>1385930</td><td>201906</td><td>21</td><td>0</td><td>null</td><td>null</td><td>-12906</td><td>null</td><td>-12906</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>4.0</td><td>2.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>5.0</td><td>4953.800293</td><td>null</td><td>14</td><td>null</td><td>null</td><td>3.0</td><td>0.0</td><td>0.0</td><td>8376.400391</td><td>2239.800049</td><td>&hellip;</td><td>37000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>2</td><td>35</td><td>64.0</td><td>387.0</td><td>12.0</td><td>12.0</td><td>9516.358398</td><td>10218.124023</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>9.25</td><td>75.404259</td><td>1498.074707</td><td>2883.991455</td><td>250.787231</td><td>13346.333008</td><td>5006852.5</td><td>1.7240264e7</td><td>2019</td><td>3</td></tr><tr><td>1625679</td><td>201911</td><td>45</td><td>0</td><td>null</td><td>14</td><td>null</td><td>null</td><td>-22049</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>1.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>null</td><td>14956.400391</td><td>6.0</td><td>null</td><td>null</td><td>null</td><td>&quot;PENSION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>1.0</td><td>2.0</td><td>0.0</td><td>0.0</td><td>5845.399902</td><td>&hellip;</td><td>21000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>23100.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>2</td><td>35</td><td>0.0</td><td>1117.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>92337.085938</td><td>2020.0</td><td>2019.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>108.534485</td><td>0.0</td><td>24829.701172</td><td>0.0</td><td>104126.390625</td><td>0.0</td><td>1.4223e9</td><td>2019</td><td>12</td></tr><tr><td>793459</td><td>201909</td><td>35</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-8864</td><td>5.0</td><td>5.0</td><td>2.0</td><td>5.0</td><td>5.0</td><td>&quot;a55475b1&quot;</td><td>&quot;6b2ae0fa&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>1.0</td><td>&quot;a7fcb6e5&quot;</td><td>&quot;a55475b1&quot;</td><td>5.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>6.0</td><td>12147.400391</td><td>&quot;DEDUCTION_6&quot;</td><td>14</td><td>14</td><td>null</td><td>3.0</td><td>10.0</td><td>null</td><td>null</td><td>3500.0</td><td>&hellip;</td><td>100000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>1</td><td>35</td><td>45.0</td><td>485.0</td><td>12.0</td><td>12.0</td><td>29205.841797</td><td>167980.0</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>4.416667</td><td>183.703705</td><td>2857.158447</td><td>46900.175781</td><td>146.340576</td><td>25000.677734</td><td>6.0016316e7</td><td>2.3076e9</td><td>2019</td><td>7</td></tr><tr><td>2560038</td><td>201904</td><td>13</td><td>0</td><td>-2720</td><td>null</td><td>-25756</td><td>null</td><td>-25756</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>0.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>5861.300293</td><td>null</td><td>null</td><td>5.0</td><td>null</td><td>null</td><td>null</td><td>14</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>17129.0</td><td>3782.400146</td><td>&hellip;</td><td>50000.0</td><td>270.216003</td><td>0</td><td>-1858</td><td>270.216003</td><td>-1858</td><td>0</td><td>-1858</td><td>-1858</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;c7a5ad39&quot;</td><td>0</td><td>23</td><td>0.0</td><td>null</td><td>12.0</td><td>null</td><td>0.0</td><td>null</td><td>2020.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>2019</td><td>8</td></tr><tr><td>1006475</td><td>202008</td><td>85</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>3848.400146</td><td>&hellip;</td><td>30000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2020</td><td>19</td></tr><tr><td>1545471</td><td>201909</td><td>37</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-10703</td><td>9.0</td><td>10.0</td><td>0.0</td><td>14.0</td><td>9.0</td><td>&quot;a55475b1&quot;</td><td>&quot;39a0853f&quot;</td><td>&quot;a55475b1&quot;</td><td>1.0</td><td>5.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>14.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>12.0</td><td>13937.0</td><td>&quot;DEDUCTION_6&quot;</td><td>14</td><td>14</td><td>null</td><td>4.0</td><td>14.0</td><td>0.0</td><td>19133.492188</td><td>1284.800049</td><td>&hellip;</td><td>40000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>11</td><td>35</td><td>0.0</td><td>22.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>6455.10791</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>0.345133</td><td>0.0</td><td>320.84024</td><td>0.0</td><td>4.549463</td><td>0.0</td><td>1.2893e6</td><td>2019</td><td>20</td></tr><tr><td>1788858</td><td>202002</td><td>58</td><td>0</td><td>null</td><td>14</td><td>null</td><td>null</td><td>-22723</td><td>0.0</td><td>1.0</td><td>0.0</td><td>6.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;6b2ae0fa&quot;</td><td>&quot;a55475b1&quot;</td><td>1.0</td><td>2.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>6.0</td><td>null</td><td>7306.200195</td><td>6.0</td><td>null</td><td>null</td><td>null</td><td>&quot;PENSION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>6.0</td><td>3.0</td><td>0.0</td><td>144987.75</td><td>2133.199951</td><td>&hellip;</td><td>56000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>6</td><td>35</td><td>0.0</td><td>28.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>12631.799805</td><td>2021.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>0.686047</td><td>0.0</td><td>2571.783691</td><td>0.0</td><td>11.088509</td><td>0.0</td><td>2.1974562e7</td><td>2020</td><td>17</td></tr><tr><td>766208</td><td>201908</td><td>31</td><td>0</td><td>null</td><td>null</td><td>-11878</td><td>null</td><td>-11878</td><td>4.0</td><td>6.0</td><td>1.0</td><td>11.0</td><td>3.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>6.0</td><td>12.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>11.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>6.0</td><td>7200.0</td><td>null</td><td>14</td><td>null</td><td>null</td><td>5.0</td><td>8.0</td><td>null</td><td>null</td><td>10836.400391</td><td>&hellip;</td><td>130000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>null</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;a55475b1&quot;</td><td>2</td><td>35</td><td>null</td><td>0.0</td><td>null</td><td>12.0</td><td>null</td><td>0.0</td><td>null</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;a55475b1&quot;</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>2019</td><td>10</td></tr><tr><td>1545706</td><td>201909</td><td>37</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-9820</td><td>1.0</td><td>1.0</td><td>0.0</td><td>3.0</td><td>1.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>4.0</td><td>3.0</td><td>&quot;a7fcb6e5&quot;</td><td>&quot;a55475b1&quot;</td><td>3.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;DEDUCTION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>3.0</td><td>5.0</td><td>0.0</td><td>51751.800781</td><td>5310.800293</td><td>&hellip;</td><td>40000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>2</td><td>35</td><td>0.0</td><td>0.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>0.0</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2019</td><td>21</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "data_store: dict = {\n",
        "    \"df_base\": SchemaGen.scan_files(TRAIN_DIR / \"train_base.parquet\"),\n",
        "    \"depth_0\": [\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
        "    ],\n",
        "    \"depth_1\": [\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
        "    ],\n",
        "    \"depth_2\": [\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
        "        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
        "    ],\n",
        "}\n",
        "\n",
        "df_train: pl.LazyFrame = (\n",
        "    SchemaGen.join_dataframes(**data_store)\n",
        "    .pipe(filter_cols)\n",
        "    .pipe(transform_cols)\n",
        "    .pipe(handle_dates)\n",
        "    .pipe(Utility.reduce_memory_usage, \"df_train\")\n",
        ")\n",
        "\n",
        "del data_store\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Train data shape: {df_train.shape}\")\n",
        "display(df_train.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua_B1p_lpiWj",
        "outputId": "32f56613-d9fe-4601-97b5-f054a41bc50c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File test_base loaded into memory.\n",
            "File test_static_cb_0 loaded into memory.\n",
            "File test_static_0_0 loaded into memory.\n",
            "File test_static_0_2 loaded into memory.\n",
            "File test_static_0_1 loaded into memory.\n",
            "File test_applprev_1_2 loaded into memory.\n",
            "File test_applprev_1_1 loaded into memory.\n",
            "File test_applprev_1_0 loaded into memory.\n",
            "File test_tax_registry_a_1 loaded into memory.\n",
            "File test_tax_registry_b_1 loaded into memory.\n",
            "File test_tax_registry_c_1 loaded into memory.\n",
            "File test_credit_bureau_a_1_1 loaded into memory.\n",
            "File test_credit_bureau_a_1_0 loaded into memory.\n",
            "File test_credit_bureau_a_1_2 loaded into memory.\n",
            "File test_credit_bureau_a_1_3 loaded into memory.\n",
            "File test_credit_bureau_a_1_4 loaded into memory.\n",
            "File test_credit_bureau_b_1 loaded into memory.\n",
            "File test_other_1 loaded into memory.\n",
            "File test_person_1 loaded into memory.\n",
            "File test_deposit_1 loaded into memory.\n",
            "File test_debitcard_1 loaded into memory.\n",
            "File test_credit_bureau_a_2_2 loaded into memory.\n",
            "File test_credit_bureau_a_2_0 loaded into memory.\n",
            "File test_credit_bureau_a_2_8 loaded into memory.\n",
            "File test_credit_bureau_a_2_3 loaded into memory.\n",
            "File test_credit_bureau_a_2_4 loaded into memory.\n",
            "File test_credit_bureau_a_2_10 loaded into memory.\n",
            "File test_credit_bureau_a_2_1 loaded into memory.\n",
            "File test_credit_bureau_a_2_9 loaded into memory.\n",
            "File test_credit_bureau_a_2_11 loaded into memory.\n",
            "File test_credit_bureau_a_2_7 loaded into memory.\n",
            "File test_credit_bureau_a_2_5 loaded into memory.\n",
            "File test_credit_bureau_a_2_6 loaded into memory.\n",
            "File test_credit_bureau_b_2 loaded into memory.\n",
            "Memory usage of dataframe \"df_train\" is 0.0432 MB.\n",
            "Memory usage of dataframe \"df_train\" became 0.0311 MB.\n",
            "Memory usage of dataframe \"df_test\" is 0.0184 MB.\n",
            "Memory usage of dataframe \"df_test\" became 0.0172 MB.\n",
            "Test data shape: (10, 471)\n"
          ]
        }
      ],
      "source": [
        "data_store: dict = {\n",
        "    \"df_base\": SchemaGen.scan_files(TEST_DIR / \"test_base.parquet\"),\n",
        "    \"depth_0\": [\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
        "    ],\n",
        "    \"depth_1\": [\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_other_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_person_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
        "    ],\n",
        "    \"depth_2\": [\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
        "        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
        "    ],\n",
        "}\n",
        "\n",
        "df_test: pl.DataFrame = (\n",
        "    SchemaGen.join_dataframes(**data_store)\n",
        "    .pipe(transform_cols)\n",
        "    .pipe(handle_dates)\n",
        "    .select([col for col in df_train.columns if col != \"target\"])\n",
        "    .pipe(Utility.reduce_memory_usage, \"df_test\")\n",
        ")\n",
        "\n",
        "del data_store\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Test data shape: {df_test.shape}\")\n",
        "\n",
        "df_test.write_parquet(\"test_final.parquet\", compression=\"lz4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-5ORGIxpiZK"
      },
      "outputs": [],
      "source": [
        "df_train, cat_cols = Utility.to_pandas(df_train)\n",
        "df_test, cat_cols = Utility.to_pandas(df_test, cat_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZYnF8e4pib3"
      },
      "outputs": [],
      "source": [
        "class VotingModel(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    A voting ensemble model that combines predictions from multiple estimators.\n",
        "\n",
        "    Parameters:\n",
        "    - estimators (list): List of base estimators.\n",
        "\n",
        "    Attributes:\n",
        "    - estimators (list): List of base estimators.\n",
        "\n",
        "    Methods:\n",
        "    - fit(X, y=None): Fit the model to the training data.\n",
        "    - predict(X): Predict class labels for samples.\n",
        "    - predict_proba(X): Predict class probabilities for samples.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, estimators: list[BaseEstimator]):\n",
        "        \"\"\"\n",
        "        Initialize the VotingModel with a list of base estimators.\n",
        "\n",
        "        Args:\n",
        "        - estimators (list): List of base estimators.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.estimators = estimators\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the model to the training data.\n",
        "\n",
        "        Args:\n",
        "        - X: Input features.\n",
        "        - y: Target labels (ignored).\n",
        "\n",
        "        Returns:\n",
        "        - self: Returns the instance itself.\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples.\n",
        "\n",
        "        Args:\n",
        "        - X: Input features.\n",
        "\n",
        "        Returns:\n",
        "        - numpy.ndarray: Predicted class labels.\n",
        "        \"\"\"\n",
        "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
        "        return np.mean(y_preds, axis=0)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict class probabilities for samples.\n",
        "\n",
        "        Args:\n",
        "        - X: Input features.\n",
        "\n",
        "        Returns:\n",
        "        - numpy.ndarray: Predicted class probabilities.\n",
        "        \"\"\"\n",
        "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
        "        return np.mean(y_preds, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKUOfXIlpien"
      },
      "outputs": [],
      "source": [
        "df_subm: pd.DataFrame = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
        "df_subm = df_subm.set_index(\"case_id\")\n",
        "est_cnt: int = 6000\n",
        "\n",
        "device: str = \"GPU\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R2dtNGyvpuTe",
        "outputId": "efbcf76f-d0df-4d59-ba93-e9689608b7b2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.6664945\tbest: 0.6664945 (0)\ttotal: 1.14s\tremaining: 1h 50m 27s\n",
            "100:\ttest: 0.8223869\tbest: 0.8223869 (100)\ttotal: 1m 29s\tremaining: 1h 23m 59s\n",
            "200:\ttest: 0.8343622\tbest: 0.8343622 (200)\ttotal: 2m 55s\tremaining: 1h 21m 18s\n",
            "300:\ttest: 0.8385859\tbest: 0.8385859 (300)\ttotal: 4m 18s\tremaining: 1h 18m 49s\n",
            "400:\ttest: 0.8411116\tbest: 0.8411116 (400)\ttotal: 5m 40s\tremaining: 1h 16m 26s\n",
            "500:\ttest: 0.8426304\tbest: 0.8426304 (500)\ttotal: 7m 2s\tremaining: 1h 14m 27s\n",
            "600:\ttest: 0.8435009\tbest: 0.8435009 (600)\ttotal: 8m 22s\tremaining: 1h 12m 29s\n",
            "700:\ttest: 0.8442255\tbest: 0.8442255 (700)\ttotal: 9m 42s\tremaining: 1h 10m 36s\n",
            "800:\ttest: 0.8447797\tbest: 0.8447797 (800)\ttotal: 11m 2s\tremaining: 1h 8m 55s\n",
            "900:\ttest: 0.8453462\tbest: 0.8453462 (900)\ttotal: 12m 21s\tremaining: 1h 7m 10s\n",
            "1000:\ttest: 0.8457715\tbest: 0.8457715 (1000)\ttotal: 13m 40s\tremaining: 1h 5m 34s\n",
            "1100:\ttest: 0.8461790\tbest: 0.8461790 (1100)\ttotal: 15m\tremaining: 1h 4m 2s\n",
            "1200:\ttest: 0.8467236\tbest: 0.8467236 (1200)\ttotal: 16m 20s\tremaining: 1h 2m 35s\n",
            "1300:\ttest: 0.8470083\tbest: 0.8470083 (1300)\ttotal: 17m 39s\tremaining: 1h 1m 5s\n",
            "1400:\ttest: 0.8474126\tbest: 0.8474126 (1400)\ttotal: 18m 59s\tremaining: 59m 37s\n",
            "1500:\ttest: 0.8477775\tbest: 0.8477775 (1500)\ttotal: 20m 18s\tremaining: 58m 10s\n",
            "1600:\ttest: 0.8480513\tbest: 0.8480513 (1600)\ttotal: 21m 38s\tremaining: 56m 45s\n",
            "1700:\ttest: 0.8482390\tbest: 0.8482390 (1700)\ttotal: 22m 58s\tremaining: 55m 21s\n",
            "1800:\ttest: 0.8485013\tbest: 0.8485013 (1800)\ttotal: 24m 17s\tremaining: 53m 56s\n",
            "1900:\ttest: 0.8487410\tbest: 0.8487410 (1900)\ttotal: 25m 37s\tremaining: 52m 33s\n",
            "2000:\ttest: 0.8489890\tbest: 0.8489890 (2000)\ttotal: 26m 55s\tremaining: 51m 7s\n",
            "2100:\ttest: 0.8492762\tbest: 0.8492762 (2100)\ttotal: 28m 15s\tremaining: 49m 45s\n",
            "2200:\ttest: 0.8494506\tbest: 0.8494506 (2200)\ttotal: 29m 34s\tremaining: 48m 21s\n",
            "2300:\ttest: 0.8496934\tbest: 0.8496934 (2300)\ttotal: 30m 54s\tremaining: 47m\n",
            "2400:\ttest: 0.8499280\tbest: 0.8499280 (2400)\ttotal: 32m 13s\tremaining: 45m 37s\n",
            "2500:\ttest: 0.8500517\tbest: 0.8500517 (2500)\ttotal: 33m 32s\tremaining: 44m 14s\n",
            "2600:\ttest: 0.8501998\tbest: 0.8501998 (2600)\ttotal: 34m 52s\tremaining: 42m 53s\n",
            "2700:\ttest: 0.8503395\tbest: 0.8503395 (2700)\ttotal: 36m 11s\tremaining: 41m 31s\n",
            "2800:\ttest: 0.8505126\tbest: 0.8505126 (2800)\ttotal: 37m 30s\tremaining: 40m 9s\n",
            "2900:\ttest: 0.8506935\tbest: 0.8506935 (2900)\ttotal: 38m 48s\tremaining: 38m 47s\n",
            "3000:\ttest: 0.8508509\tbest: 0.8508509 (3000)\ttotal: 40m 8s\tremaining: 37m 26s\n",
            "3100:\ttest: 0.8509824\tbest: 0.8509824 (3100)\ttotal: 41m 27s\tremaining: 36m 4s\n",
            "3200:\ttest: 0.8511108\tbest: 0.8511108 (3200)\ttotal: 42m 46s\tremaining: 34m 44s\n",
            "3300:\ttest: 0.8511748\tbest: 0.8511748 (3300)\ttotal: 44m 5s\tremaining: 33m 22s\n",
            "3400:\ttest: 0.8513151\tbest: 0.8513151 (3400)\ttotal: 45m 25s\tremaining: 32m 2s\n",
            "3500:\ttest: 0.8514424\tbest: 0.8514424 (3500)\ttotal: 46m 44s\tremaining: 30m 41s\n",
            "3600:\ttest: 0.8515712\tbest: 0.8515712 (3600)\ttotal: 48m 3s\tremaining: 29m 21s\n",
            "3700:\ttest: 0.8517017\tbest: 0.8517017 (3700)\ttotal: 49m 22s\tremaining: 28m\n",
            "3800:\ttest: 0.8517821\tbest: 0.8517821 (3800)\ttotal: 50m 41s\tremaining: 26m 39s\n",
            "3900:\ttest: 0.8518852\tbest: 0.8518852 (3900)\ttotal: 52m\tremaining: 25m 19s\n",
            "4000:\ttest: 0.8519720\tbest: 0.8519770 (3990)\ttotal: 53m 20s\tremaining: 23m 59s\n",
            "4100:\ttest: 0.8520262\tbest: 0.8520262 (4100)\ttotal: 54m 38s\tremaining: 22m 38s\n",
            "4200:\ttest: 0.8521239\tbest: 0.8521239 (4200)\ttotal: 55m 59s\tremaining: 21m 18s\n",
            "4300:\ttest: 0.8521942\tbest: 0.8522059 (4270)\ttotal: 57m 20s\tremaining: 19m 58s\n",
            "4400:\ttest: 0.8523022\tbest: 0.8523022 (4400)\ttotal: 58m 39s\tremaining: 18m 38s\n",
            "4500:\ttest: 0.8524034\tbest: 0.8524034 (4500)\ttotal: 59m 58s\tremaining: 17m 18s\n",
            "4600:\ttest: 0.8524684\tbest: 0.8524684 (4600)\ttotal: 1h 1m 19s\tremaining: 15m 58s\n",
            "4700:\ttest: 0.8525744\tbest: 0.8525744 (4700)\ttotal: 1h 2m 38s\tremaining: 14m 38s\n",
            "4800:\ttest: 0.8526503\tbest: 0.8526503 (4800)\ttotal: 1h 3m 59s\tremaining: 13m 18s\n",
            "4900:\ttest: 0.8527117\tbest: 0.8527117 (4900)\ttotal: 1h 5m 19s\tremaining: 11m 58s\n",
            "5000:\ttest: 0.8528064\tbest: 0.8528064 (5000)\ttotal: 1h 6m 38s\tremaining: 10m 38s\n",
            "5100:\ttest: 0.8528764\tbest: 0.8528782 (5090)\ttotal: 1h 7m 58s\tremaining: 9m 18s\n",
            "5200:\ttest: 0.8529648\tbest: 0.8529648 (5200)\ttotal: 1h 9m 17s\tremaining: 7m 58s\n",
            "5300:\ttest: 0.8531101\tbest: 0.8531101 (5300)\ttotal: 1h 10m 36s\tremaining: 6m 38s\n",
            "5400:\ttest: 0.8531571\tbest: 0.8531580 (5395)\ttotal: 1h 11m 56s\tremaining: 5m 18s\n",
            "5500:\ttest: 0.8532513\tbest: 0.8532513 (5500)\ttotal: 1h 13m 17s\tremaining: 3m 59s\n",
            "5600:\ttest: 0.8533091\tbest: 0.8533091 (5600)\ttotal: 1h 14m 37s\tremaining: 2m 39s\n",
            "5700:\ttest: 0.8533401\tbest: 0.8533401 (5700)\ttotal: 1h 15m 57s\tremaining: 1m 19s\n",
            "5799:\ttest: 0.8534392\tbest: 0.8534408 (5785)\ttotal: 1h 17m 16s\tremaining: 0us\n",
            "bestTest = 0.8534407616\n",
            "bestIteration = 5785\n",
            "Shrink model to first 5786 iterations.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LightGBMError",
          "evalue": "No OpenCL device found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9c1c09d336a6>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1185\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1188\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[1;32m   3435\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m             \u001b[0mparams_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_param_dict_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3437\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[0m\u001b[1;32m   3438\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3439\u001b[0m                 \u001b[0m_c_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \"\"\"\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLightGBMError\u001b[0m: No OpenCL device found"
          ]
        }
      ],
      "source": [
        "X = df_train.drop(columns=[\"target\", \"case_id\", \"week_num\"])\n",
        "y = df_train[\"target\"]\n",
        "\n",
        "weeks = df_train[\"week_num\"]\n",
        "\n",
        "del df_train\n",
        "gc.collect()\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
        "\n",
        "params1 = {\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"colsample_bynode\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"device\": device,\n",
        "    \"extra_trees\": True,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"l1_regularization\": 0.1,\n",
        "    \"l2_regularization\": 10,\n",
        "    \"max_depth\": 18,\n",
        "    \"metric\": \"auc\",\n",
        "    \"n_estimators\": 2000,\n",
        "    \"num_leaves\": 58,\n",
        "    \"objective\": \"binary\",\n",
        "    \"random_state\": 42,\n",
        "    \"verbose\": -1,\n",
        "}\n",
        "\n",
        "params2 = {\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"colsample_bynode\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"device\": device,\n",
        "    \"extra_trees\": True,\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"l1_regularization\": 0.1,\n",
        "    \"l2_regularization\": 10,\n",
        "    \"max_depth\": 16,\n",
        "    \"metric\": \"auc\",\n",
        "    \"n_estimators\": 2000,\n",
        "    \"num_leaves\": 52,\n",
        "    \"objective\": \"binary\",\n",
        "    \"random_state\": 42,\n",
        "    \"verbose\": -1,\n",
        "}\n",
        "\n",
        "fitted_models_cat = []\n",
        "fitted_models_lgb = []\n",
        "\n",
        "cv_scores_cat = []\n",
        "cv_scores_lgb = []\n",
        "\n",
        "iter_cnt = 0\n",
        "for idx_train, idx_valid in cv.split(X, y, groups=weeks):\n",
        "    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
        "    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
        "\n",
        "    train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n",
        "    val_pool = Pool(X_valid, y_valid, cat_features=cat_cols)\n",
        "\n",
        "    clf = CatBoostClassifier(\n",
        "        best_model_min_trees = 1000,\n",
        "        boosting_type = \"Plain\",\n",
        "        eval_metric = \"AUC\",\n",
        "        iterations = 5800,\n",
        "        learning_rate = 0.05,\n",
        "        l2_leaf_reg = 10,\n",
        "        max_leaves = 64,\n",
        "        random_seed = 42,\n",
        "        task_type = \"GPU\",\n",
        "        use_best_model = True\n",
        "    )\n",
        "\n",
        "    clf.fit(train_pool, eval_set=val_pool, verbose=100)\n",
        "    fitted_models_cat.append(clf)\n",
        "\n",
        "    y_pred_valid = clf.predict_proba(X_valid)[:, 1]\n",
        "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
        "    cv_scores_cat.append(auc_score)\n",
        "\n",
        "    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n",
        "    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n",
        "\n",
        "    if iter_cnt % 2 == 0:\n",
        "        model = lgb.LGBMClassifier(**params1)\n",
        "    else:\n",
        "        model = lgb.LGBMClassifier(**params2)\n",
        "\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)],\n",
        "    )\n",
        "    fitted_models_lgb.append(model)\n",
        "\n",
        "    y_pred_valid = model.predict_proba(X_valid)[:, 1]\n",
        "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
        "    cv_scores_lgb.append(auc_score)\n",
        "\n",
        "    iter_cnt += 1\n",
        "\n",
        "model = VotingModel(fitted_models_cat + fitted_models_lgb)\n",
        "\n",
        "print(f\"\\nCV AUC scores for CatBoost: {cv_scores_cat}\")\n",
        "print(f\"Maximum CV AUC score for Catboost: {max(cv_scores_cat)}\", end=\"\\n\\n\")\n",
        "\n",
        "\n",
        "print(f\"CV AUC scores for LGBM: {cv_scores_lgb}\")\n",
        "print(f\"Maximum CV AUC score for LGBM: {max(cv_scores_lgb)}\", end=\"\\n\\n\")\n",
        "\n",
        "del X, y\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYnyfs2kpual"
      },
      "outputs": [],
      "source": [
        "X_test: pd.DataFrame = df_test.drop(columns=[\"week_num\"]).set_index(\"case_id\")\n",
        "\n",
        "X_test[cat_cols] = X_test[cat_cols].astype(\"category\")\n",
        "\n",
        "y_pred: pd.Series = pd.Series(model.predict_proba(X_test)[:, 1], index=X_test.index)\n",
        "\n",
        "df_subm[\"score\"] = y_pred\n",
        "\n",
        "display(df_subm)\n",
        "\n",
        "df_subm.to_csv(\"submission.csv\")\n",
        "\n",
        "del X_test, y_pred, df_subm\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFeQ5KuApudh"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(fitted_models_lgb, 'lgb_models.joblib')\n",
        "joblib.dump(fitted_models_cat, 'cat_models.joblib')\n",
        "\n",
        "notebook_info = {\n",
        "    'description': 'Add notebook info dict to store cols and cat_cols',\n",
        "    'cols': X.columns.to_list(),\n",
        "    'cat_cols': cat_cols,\n",
        "}\n",
        "joblib.dump(notebook_info, 'notebook.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67WikKSRpugN"
      },
      "outputs": [],
      "source": [
        "# !mkdir '/content/drive/MyDrive/20240527-HOME CREDICT/weight_v3/cat_baseline1/'\n",
        "# !cp '/content/lgb_models.joblib' '/content/drive/MyDrive/20240527-HOME CREDICT/weight_v3/cat_baseline1/'\n",
        "# !cp '/content/notebook_info.joblib' '/content/drive/MyDrive/20240527-HOME CREDICT/weight_v3/cat_baseline1/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9g9dBFUvC4S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}