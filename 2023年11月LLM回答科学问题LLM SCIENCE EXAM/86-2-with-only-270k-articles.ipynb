{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I previously shared a [notebook](url) (see the discussion [here](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/441128)) that found a cluster of relevant Wikipedia STEM articles, resulting in around 270K STEM articles for which the resulting dataset is released [here.](https://www.kaggle.com/datasets/mbanaei/stem-wiki-cohere-no-emb)\n\nHowever, due to issues with WikiExtractor, there're cases in which some numbers or even paragraphs are missing from the final Wiki parsing. Therefore,  for the same set of  articles, I used Wiki API to gather the articles' contexts (see discussion [here](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/442483)), for which the resulting dataset is released [here](https://www.kaggle.com/datasets/mbanaei/all-paraphs-parsed-expanded).\n\nIn order to show that the found articles cover not only the train dataset articles but also a majority of LB gold articles, I release this notebook that uses a simple retrieval model (without any prior indexing) together with a model that is trained only on the RACE dataset. (not fine-tuned on any competition-similar dataset).\n\nThe main design choices for the notebook are:\n- Using a simple TF-IDF to retrieve contexts from both datasets for every given question.\n- Although the majority of high-performing public models use DeBERTa-V3 to do the inference in their pipeline, I used a LongFormer Large model, which enables us to have a much longer prefix context given limited GPU memory. More specifically, as opposed to many public notebooks, there's no splitting to sentence level, and the whole paragraph is retrieved and passed to the classifier as a context (the main reason that we don't get OOM and also have relatively fast inference is that in LongFormer full attention is not computed as opposed to standard models like BERT).\n- I use a fall-back model (based on a public notebook that uses an openbook approach and performs 81.5 on LB) that is used for prediction when there's low confidence in the main model's output for the top choice.\n\nP.S: Although the model's performance is relatively good compared to other public notebooks, many design choices can be revised to improve both inference time and performance. (e.g., currently, context retrieval seems to be the inference bottleneck as no prior indexing is used).","metadata":{}},{"cell_type":"code","source":"!cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n!cp /kaggle/input/backup-806/util_openbook.py .","metadata":{"execution":{"iopub.status.busy":"2023-08-29T13:41:10.470251Z","iopub.execute_input":"2023-08-29T13:41:10.47071Z","iopub.status.idle":"2023-08-29T13:41:46.496147Z","shell.execute_reply.started":"2023-08-29T13:41:10.470675Z","shell.execute_reply":"2023-08-29T13:41:46.494943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# installing offline dependencies\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-08-29T13:41:52.41787Z","iopub.execute_input":"2023-08-29T13:41:52.418247Z","iopub.status.idle":"2023-08-29T13:43:54.355399Z","shell.execute_reply.started":"2023-08-29T13:41:52.418215Z","shell.execute_reply":"2023-08-29T13:43:54.354189Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from util_openbook import get_contexts, generate_openbook_output\nimport pickle\n\nget_contexts()\ngenerate_openbook_output()\n\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-08-29T13:44:53.233905Z","iopub.execute_input":"2023-08-29T13:44:53.234278Z","iopub.status.idle":"2023-08-29T13:55:59.072609Z","shell.execute_reply.started":"2023-08-29T13:44:53.234248Z","shell.execute_reply":"2023-08-29T13:55:59.071407Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nbackup_model_predictions = pd.read_csv(\"submission_backup.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-29T14:10:13.829974Z","iopub.execute_input":"2023-08-29T14:10:13.830365Z","iopub.status.idle":"2023-08-29T14:10:13.839005Z","shell.execute_reply.started":"2023-08-29T14:10:13.830336Z","shell.execute_reply":"2023-08-29T14:10:13.838006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\n\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-11T16:44:53.203928Z","iopub.execute_input":"2023-08-11T16:44:53.204295Z","iopub.status.idle":"2023-08-11T16:44:53.360032Z","shell.execute_reply.started":"2023-08-11T16:44:53.204267Z","shell.execute_reply":"2023-08-11T16:44:53.359072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:45:00.735581Z","iopub.execute_input":"2023-08-11T16:45:00.736996Z","iopub.status.idle":"2023-08-11T16:45:15.400251Z","shell.execute_reply.started":"2023-08-11T16:45:00.736955Z","shell.execute_reply":"2023-08-11T16:45:15.398987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SplitList(mylist, chunk_size):\n    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n\ndef get_relevant_documents_parsed(df_valid):\n    df_chunk_size=600\n    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n    modified_texts = paraphs_parsed_dataset.map(lambda example:\n                                             {'temp_text':\n                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n                                             num_proc=2)[\"temp_text\"]\n    \n    all_articles_indices = []\n    all_articles_values = []\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n    \n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    top_per_query = article_indices_array.shape[1]\n    articles_flatten = [(\n                         articles_values_array[index],\n                         paraphs_parsed_dataset[idx.item()][\"title\"],\n                         paraphs_parsed_dataset[idx.item()][\"text\"],\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles\n\n\n\ndef get_relevant_documents(df_valid):\n    df_chunk_size=800\n    \n    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n    modified_texts = cohere_dataset_filtered.map(lambda example:\n                                             {'temp_text':\n                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n                                             num_proc=2)[\"temp_text\"]\n    \n    all_articles_indices = []\n    all_articles_values = []\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n    \n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    top_per_query = article_indices_array.shape[1]\n    articles_flatten = [(\n                         articles_values_array[index],\n                         cohere_dataset_filtered[idx.item()][\"title\"],\n                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles\n\n\n\ndef retrieval(df_valid, modified_texts):\n    \n    corpus_df_valid = df_valid.apply(lambda row:\n                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n                                     axis=1).values\n    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n                                 stop_words=stop_words)\n    vectorizer1.fit(corpus_df_valid)\n    vocab_df_valid = vectorizer1.get_feature_names_out()\n    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n                                 stop_words=stop_words,\n                                 vocabulary=vocab_df_valid)\n    vectorizer.fit(modified_texts[:500000])\n    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n    \n    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n\n    chunk_size = 100000\n    top_per_chunk = 10\n    top_per_query = 10\n\n    all_chunk_top_indices = []\n    all_chunk_top_values = []\n\n    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n\n        all_chunk_top_indices.append(chunk_top_indices + idx)\n        all_chunk_top_values.append(chunk_top_values)\n\n    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n    \n    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n    \n    return articles_indices, merged_top_scores\n\n\ndef prepare_answering_input(\n        tokenizer, \n        question,  \n        options,   \n        context,   \n        max_seq_length=4096,\n    ):\n    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n    c_plus_q_4 = [c_plus_q] * len(options)\n    tokenized_examples = tokenizer(\n        c_plus_q_4, options,\n        max_length=max_seq_length,\n        padding=\"longest\",\n        truncation=False,\n        return_tensors=\"pt\",\n    )\n    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n    example_encoded = {\n        \"input_ids\": input_ids.to(model.device.index),\n        \"attention_mask\": attention_mask.to(model.device.index),\n    }\n    return example_encoded\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:45:37.824922Z","iopub.execute_input":"2023-08-11T16:45:37.825306Z","iopub.status.idle":"2023-08-11T16:45:37.850706Z","shell.execute_reply.started":"2023-08-11T16:45:37.825276Z","shell.execute_reply":"2023-08-11T16:45:37.84937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = ['each', 'you', 'the', 'use', 'used',\n                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n                  'did', 'theirs', 'can', 'those',\n                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n                  'yours', 'but', 'being', \"wasn't\", 'be']","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:45:45.644645Z","iopub.execute_input":"2023-08-11T16:45:45.645019Z","iopub.status.idle":"2023-08-11T16:45:45.654919Z","shell.execute_reply.started":"2023-08-11T16:45:45.644991Z","shell.execute_reply":"2023-08-11T16:45:45.653722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:45:50.623189Z","iopub.execute_input":"2023-08-11T16:45:50.623548Z","iopub.status.idle":"2023-08-11T16:45:50.641881Z","shell.execute_reply.started":"2023-08-11T16:45:50.623519Z","shell.execute_reply":"2023-08-11T16:45:50.640966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retrieved_articles = get_relevant_documents(df_valid)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:45:52.935248Z","iopub.execute_input":"2023-08-11T16:45:52.93561Z","iopub.status.idle":"2023-08-11T17:03:23.254145Z","shell.execute_reply.started":"2023-08-11T16:45:52.935582Z","shell.execute_reply":"2023-08-11T17:03:23.25295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = LongformerTokenizer.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\")\nmodel = LongformerForMultipleChoice.from_pretrained(\"/kaggle/input/longformer-race-model/longformer_qa_model\").cuda()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T17:03:23.2586Z","iopub.execute_input":"2023-08-11T17:03:23.259633Z","iopub.status.idle":"2023-08-11T17:03:45.509905Z","shell.execute_reply.started":"2023-08-11T17:03:23.259593Z","shell.execute_reply":"2023-08-11T17:03:45.508872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nsubmit_ids = []\n\nfor index in tqdm(range(df_valid.shape[0])):\n    columns = df_valid.iloc[index].values\n    submit_ids.append(columns[0])\n    question = columns[1]\n    options = [columns[2], columns[3], columns[4], columns[5], columns[6]]\n    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n    context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n    inputs1 = prepare_answering_input(\n        tokenizer=tokenizer, question=question,\n        options=options, context=context1,\n        )\n    inputs2 = prepare_answering_input(\n        tokenizer=tokenizer, question=question,\n        options=options, context=context2,\n        )\n    \n    with torch.no_grad():\n        outputs1 = model(**inputs1)    \n        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n        \n    with torch.no_grad():\n        outputs2 = model(**inputs2)\n        losses2 = -outputs2.logits[0].detach().cpu().numpy()\n        probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n        \n    probability_ = (probability1 + probability2)/2\n\n    if probability_.max() > 0.4:\n        predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n    else:\n        predict = backup_model_predictions.iloc[index].prediction.replace(\" \",\"\")\n    predictions.append(predict)\n\npredictions = [\" \".join(i) for i in predictions]","metadata":{"execution":{"iopub.status.busy":"2023-08-11T17:11:57.829337Z","iopub.execute_input":"2023-08-11T17:11:57.829758Z","iopub.status.idle":"2023-08-11T17:13:53.780911Z","shell.execute_reply.started":"2023-08-11T17:11:57.829727Z","shell.execute_reply":"2023-08-11T17:13:53.779904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'id':submit_ids,'prediction':predictions}).to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T17:09:27.495047Z","iopub.execute_input":"2023-08-11T17:09:27.495432Z","iopub.status.idle":"2023-08-11T17:09:27.510631Z","shell.execute_reply.started":"2023-08-11T17:09:27.495397Z","shell.execute_reply":"2023-08-11T17:09:27.509601Z"},"trusted":true},"execution_count":null,"outputs":[]}]}