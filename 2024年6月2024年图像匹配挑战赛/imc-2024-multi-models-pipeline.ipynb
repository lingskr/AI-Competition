{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb725f62",
   "metadata": {
    "papermill": {
     "duration": 0.012875,
     "end_time": "2024-05-23T06:34:32.072897",
     "exception": false,
     "start_time": "2024-05-23T06:34:32.060022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Updated information:\n",
    "- Version5:\n",
    "  - Image matching methods\n",
    "      - fixed: Superpoint + LightGlue ... bug fixed\n",
    "- Version4:\n",
    "  - Image matching methods\n",
    "     - added: MatchFormer\n",
    "     - added: SIFT + LightGlue\n",
    "     - added: DISK + LightGlue\n",
    "     - modified: Aliked + LightGlue ... speeding up by cuda cache of keypoints/descriptors\n",
    "     - modified: Superpoint + LightGlue ... speeding up by cuda cache of keypoints/descriptors\n",
    "     - modified: DogHardNet + LightGlue ... speeding up by cuda cache of keypoints/descriptors\n",
    "     - modified: Superpoint + SuperGlue ... added `torch.no_grad()` and speeding up by cuda cache of keypoints/descriptors\n",
    "  - Configuration\n",
    "     - added: CAMERA_MODEL = \"simple-radial\" or \"simple-pinhole\"\n",
    "     - added: ROTATION_CORRECTION ... `check_orientation` (LightGlue series only are supported. Others image matching methods are under construction.)\n",
    "         - https://github.com/ternaus/check_orientation\n",
    "     - added: DRY_RUN ... to run pipeline with only 10 images\n",
    "  - Pipeline\n",
    "     - Parallel execution of image matching and COLMAP processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e007a5f",
   "metadata": {
    "papermill": {
     "duration": 0.011876,
     "end_time": "2024-05-23T06:34:32.097036",
     "exception": false,
     "start_time": "2024-05-23T06:34:32.085160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c6e815",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:34:32.122562Z",
     "iopub.status.busy": "2024-05-23T06:34:32.122214Z",
     "iopub.status.idle": "2024-05-23T06:36:45.295639Z",
     "shell.execute_reply": "2024-05-23T06:36:45.294444Z"
    },
    "papermill": {
     "duration": 133.189112,
     "end_time": "2024-05-23T06:36:45.298172",
     "exception": false,
     "start_time": "2024-05-23T06:34:32.109060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp310-cp310-manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: pycolmap\r\n",
      "Successfully installed pycolmap-0.4.0\r\n",
      "Processing /kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: safetensors\r\n",
      "  Attempting uninstall: safetensors\r\n",
      "    Found existing installation: safetensors 0.4.3\r\n",
      "    Uninstalling safetensors-0.4.3:\r\n",
      "      Successfully uninstalled safetensors-0.4.3\r\n",
      "Successfully installed safetensors-0.4.1\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Installing collected packages: lightglue\r\n",
      "Successfully installed lightglue-0.0\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --no-deps /kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!python -m pip install --no-deps /kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/dependencies-imc/transformers/ transformers > /dev/null\n",
    "!python -m pip install  --no-deps /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\n",
    "\n",
    "# dkm\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/dkm-dependencies/packages einops > /dev/null\n",
    "\n",
    "# match former\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/matchformer-dependencies yacs > /dev/null\n",
    "\n",
    "# lightglue models\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/pytorch-lightglue-models/* /root/.cache/torch/hub/checkpoints/\n",
    "\n",
    "# dkm model\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/dkm-dependencies/DKMv3_outdoor.pth /root/.cache/torch/hub/checkpoints/\n",
    "\n",
    "# check rotation\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/pkg-check-orientation/ check_orientation==0.0.5 > /dev/null\n",
    "!cp /kaggle/input/pkg-check-orientation/2020-11-16_resnext50_32x4d.zip /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7288e848",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:45.327747Z",
     "iopub.status.busy": "2024-05-23T06:36:45.327441Z",
     "iopub.status.idle": "2024-05-23T06:36:45.333521Z",
     "shell.execute_reply": "2024-05-23T06:36:45.332685Z"
    },
    "papermill": {
     "duration": 0.022505,
     "end_time": "2024-05-23T06:36:45.335390",
     "exception": false,
     "start_time": "2024-05-23T06:36:45.312885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a2a8fa",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:45.362693Z",
     "iopub.status.busy": "2024-05-23T06:36:45.362418Z",
     "iopub.status.idle": "2024-05-23T06:36:54.846127Z",
     "shell.execute_reply": "2024-05-23T06:36:54.844858Z"
    },
    "papermill": {
     "duration": 9.500474,
     "end_time": "2024-05-23T06:36:54.848805",
     "exception": false,
     "start_time": "2024-05-23T06:36:45.348331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from fastprogress import progress_bar\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "\n",
    "# CV/ML\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# 3D reconstruction\n",
    "import pycolmap\n",
    "\n",
    "import glob\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# dkm\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/dkm-dependencies/DKM/')\n",
    "from dkm.utils.utils import tensor_to_pil, get_tuple_transform_ops\n",
    "from dkm import DKMv3_outdoor\n",
    "\n",
    "# LoFTR\n",
    "from kornia.feature import LoFTR\n",
    "\n",
    "# LightGlue\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, SuperPoint, DoGHardNet, LightGlue, DISK, SIFT\n",
    "from lightglue.utils import load_image, rbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92608cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:54.885361Z",
     "iopub.status.busy": "2024-05-23T06:36:54.884981Z",
     "iopub.status.idle": "2024-05-23T06:36:54.890632Z",
     "shell.execute_reply": "2024-05-23T06:36:54.889517Z"
    },
    "papermill": {
     "duration": 0.027172,
     "end_time": "2024-05-23T06:36:54.893218",
     "exception": false,
     "start_time": "2024-05-23T06:36:54.866046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kornia version 0.7.2\n",
      "Pycolmap version 0.4.0\n"
     ]
    }
   ],
   "source": [
    "print('Kornia version', K.__version__)\n",
    "print('Pycolmap version', pycolmap.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b2726",
   "metadata": {
    "papermill": {
     "duration": 0.016902,
     "end_time": "2024-05-23T06:36:54.927080",
     "exception": false,
     "start_time": "2024-05-23T06:36:54.910178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3375f85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:54.965316Z",
     "iopub.status.busy": "2024-05-23T06:36:54.964203Z",
     "iopub.status.idle": "2024-05-23T06:36:54.982325Z",
     "shell.execute_reply": "2024-05-23T06:36:54.981282Z"
    },
    "papermill": {
     "duration": 0.040539,
     "end_time": "2024-05-23T06:36:54.985036",
     "exception": false,
     "start_time": "2024-05-23T06:36:54.944497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = False\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        \n",
    "        # When merging keypoints, it is enable to filtering matches with cv2.findFundamentalMatrix.\n",
    "        \"filter_FundamentalMatrix\" : False,\n",
    "        \"filter_iterations\" : 10,\n",
    "        \"filter_threshold\" : 8,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    use_aliked_lightglue = True\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = False\n",
    "    use_disk_lightglue = False\n",
    "    use_sift_lightglue = False\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = False\n",
    "    use_matchformer = False\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.005,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_disk_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_sift_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_loftr = {\n",
    "        \"resize_small_edge_to\" : 750,\n",
    "        \"min_matches\" : 15,\n",
    "    }\n",
    "    \n",
    "    params_dkm = {\n",
    "        \"num_features\" : 2048,\n",
    "        \"detection_threshold\" : 0.4,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : (540, 720),    \n",
    "    }\n",
    "    \n",
    "    # superpoint + superglue  ...  https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/416873\n",
    "    params_sg1 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1088,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg2 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1280,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1376,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sgs = [params_sg1, params_sg2, params_sg3]\n",
    "    \n",
    "    params_matchformer = {\n",
    "        \"detection_threshold\" : 0.15,\n",
    "        \"resize_to\" : (560, 750),\n",
    "        \"num_features\" : 2000,\n",
    "        \"min_matches\" : 15, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3cc3589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:55.027620Z",
     "iopub.status.busy": "2024-05-23T06:36:55.027170Z",
     "iopub.status.idle": "2024-05-23T06:36:55.032144Z",
     "shell.execute_reply": "2024-05-23T06:36:55.030956Z"
    },
    "papermill": {
     "duration": 0.029383,
     "end_time": "2024-05-23T06:36:55.034724",
     "exception": false,
     "start_time": "2024-05-23T06:36:55.005341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5516cc5",
   "metadata": {
    "papermill": {
     "duration": 0.01644,
     "end_time": "2024-05-23T06:36:55.067717",
     "exception": false,
     "start_time": "2024-05-23T06:36:55.051277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# COLMAP utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792b1e97",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:55.104641Z",
     "iopub.status.busy": "2024-05-23T06:36:55.104220Z",
     "iopub.status.idle": "2024-05-23T06:36:55.134644Z",
     "shell.execute_reply": "2024-05-23T06:36:55.133711Z"
    },
    "papermill": {
     "duration": 0.052337,
     "end_time": "2024-05-23T06:36:55.136818",
     "exception": false,
     "start_time": "2024-05-23T06:36:55.084481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to manipulate a colmap database.\n",
    "# Forked from https://github.com/colmap/colmap/blob/dev/scripts/python/database.py\n",
    "\n",
    "# Copyright (c) 2018, ETH Zurich and UNC Chapel Hill.\n",
    "# All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "#\n",
    "#     * Redistributions of source code must retain the above copyright\n",
    "#       notice, this list of conditions and the following disclaimer.\n",
    "#\n",
    "#     * Redistributions in binary form must reproduce the above copyright\n",
    "#       notice, this list of conditions and the following disclaimer in the\n",
    "#       documentation and/or other materials provided with the distribution.\n",
    "#\n",
    "#     * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of\n",
    "#       its contributors may be used to endorse or promote products derived\n",
    "#       from this software without specific prior written permission.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n",
    "# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
    "# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
    "# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
    "# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
    "# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
    "# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
    "# POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "# Author: Johannes L. Schoenberger (jsch-at-demuc-dot-de)\n",
    "\n",
    "# This script is based on an original implementation by True Price.\n",
    "\n",
    "import sys\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "IS_PYTHON3 = sys.version_info[0] >= 3\n",
    "\n",
    "MAX_IMAGE_ID = 2**31 - 1\n",
    "\n",
    "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n",
    "    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    model INTEGER NOT NULL,\n",
    "    width INTEGER NOT NULL,\n",
    "    height INTEGER NOT NULL,\n",
    "    params BLOB,\n",
    "    prior_focal_length INTEGER NOT NULL)\"\"\"\n",
    "\n",
    "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n",
    "\n",
    "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
    "    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    name TEXT NOT NULL UNIQUE,\n",
    "    camera_id INTEGER NOT NULL,\n",
    "    prior_qw REAL,\n",
    "    prior_qx REAL,\n",
    "    prior_qy REAL,\n",
    "    prior_qz REAL,\n",
    "    prior_tx REAL,\n",
    "    prior_ty REAL,\n",
    "    prior_tz REAL,\n",
    "    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
    "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n",
    "\"\"\".format(MAX_IMAGE_ID)\n",
    "\n",
    "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    config INTEGER NOT NULL,\n",
    "    F BLOB,\n",
    "    E BLOB,\n",
    "    H BLOB)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB)\"\"\"\n",
    "\n",
    "CREATE_NAME_INDEX = \\\n",
    "    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n",
    "\n",
    "CREATE_ALL = \"; \".join([\n",
    "    CREATE_CAMERAS_TABLE,\n",
    "    CREATE_IMAGES_TABLE,\n",
    "    CREATE_KEYPOINTS_TABLE,\n",
    "    CREATE_DESCRIPTORS_TABLE,\n",
    "    CREATE_MATCHES_TABLE,\n",
    "    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
    "    CREATE_NAME_INDEX\n",
    "])\n",
    "\n",
    "\n",
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "\n",
    "\n",
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n",
    "\n",
    "\n",
    "def array_to_blob(array):\n",
    "    if IS_PYTHON3:\n",
    "        return array.tostring()\n",
    "    else:\n",
    "        return np.getbuffer(array)\n",
    "\n",
    "\n",
    "def blob_to_array(blob, dtype, shape=(-1,)):\n",
    "    if IS_PYTHON3:\n",
    "        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n",
    "    else:\n",
    "        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n",
    "\n",
    "\n",
    "class COLMAPDatabase(sqlite3.Connection):\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(database_path):\n",
    "        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n",
    "        self.create_cameras_table = \\\n",
    "            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n",
    "        self.create_descriptors_table = \\\n",
    "            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n",
    "        self.create_images_table = \\\n",
    "            lambda: self.executescript(CREATE_IMAGES_TABLE)\n",
    "        self.create_two_view_geometries_table = \\\n",
    "            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n",
    "        self.create_keypoints_table = \\\n",
    "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n",
    "        self.create_matches_table = \\\n",
    "            lambda: self.executescript(CREATE_MATCHES_TABLE)\n",
    "        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n",
    "\n",
    "    def add_camera(self, model, width, height, params,\n",
    "                   prior_focal_length=False, camera_id=None):\n",
    "        params = np.asarray(params, np.float64)\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (camera_id, model, width, height, array_to_blob(params),\n",
    "             prior_focal_length))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_image(self, name, camera_id,\n",
    "                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n",
    "             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_keypoints(self, image_id, keypoints):\n",
    "        assert(len(keypoints.shape) == 2)\n",
    "        assert(keypoints.shape[1] in [2, 4, 6])\n",
    "\n",
    "        keypoints = np.asarray(keypoints, np.float32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n",
    "\n",
    "    def add_descriptors(self, image_id, descriptors):\n",
    "        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n",
    "        self.execute(\n",
    "            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n",
    "\n",
    "    def add_matches(self, image_id1, image_id2, matches):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n",
    "\n",
    "    def add_two_view_geometry(self, image_id1, image_id2, matches,\n",
    "                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        F = np.asarray(F, dtype=np.float64)\n",
    "        E = np.asarray(E, dtype=np.float64)\n",
    "        H = np.asarray(H, dtype=np.float64)\n",
    "        self.execute(\n",
    "            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n",
    "             array_to_blob(F), array_to_blob(E), array_to_blob(H)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11387e27",
   "metadata": {
    "papermill": {
     "duration": 0.01356,
     "end_time": "2024-05-23T06:36:55.164421",
     "exception": false,
     "start_time": "2024-05-23T06:36:55.150861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# h5 to colmap db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c5f5f8",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:55.196169Z",
     "iopub.status.busy": "2024-05-23T06:36:55.195783Z",
     "iopub.status.idle": "2024-05-23T06:36:55.220819Z",
     "shell.execute_reply": "2024-05-23T06:36:55.219656Z"
    },
    "papermill": {
     "duration": 0.043308,
     "end_time": "2024-05-23T06:36:55.223129",
     "exception": false,
     "start_time": "2024-05-23T06:36:55.179821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to interface DISK with Colmap.\n",
    "# Forked from https://github.com/cvlab-epfl/disk/blob/37f1f7e971cea3055bb5ccfc4cf28bfd643fa339/colmap/h5_to_db.py\n",
    "\n",
    "#  Copyright [2020] [Michał Tyszkiewicz, Pascal Fua, Eduard Trulls]\n",
    "#\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#   you may not use this file except in compliance with the License.\n",
    "#   You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "\n",
    "import os, argparse, h5py, warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "\n",
    "def get_focal(image_path, err_on_default=False):\n",
    "    image         = Image.open(image_path)\n",
    "    max_size      = max(image.size)\n",
    "\n",
    "    exif = image.getexif()\n",
    "    focal = None\n",
    "    if exif is not None:\n",
    "        focal_35mm = None\n",
    "        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n",
    "        for tag, value in exif.items():\n",
    "            focal_35mm = None\n",
    "            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n",
    "                focal_35mm = float(value)\n",
    "                break\n",
    "\n",
    "        if focal_35mm is not None:\n",
    "            focal = focal_35mm / 35. * max_size\n",
    "    \n",
    "    if focal is None:\n",
    "        if err_on_default:\n",
    "            raise RuntimeError(\"Failed to find focal length\")\n",
    "\n",
    "        # failed to find it in exif, use prior\n",
    "        FOCAL_PRIOR = 1.2\n",
    "        focal = FOCAL_PRIOR * max_size\n",
    "\n",
    "    return focal\n",
    "\n",
    "def create_camera(db, image_path, camera_model):\n",
    "    image         = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "\n",
    "    focal = get_focal(image_path)\n",
    "\n",
    "    if camera_model == 'simple-pinhole':\n",
    "        model = 0 # simple pinhole\n",
    "        param_arr = np.array([focal, width / 2, height / 2])\n",
    "    if camera_model == 'pinhole':\n",
    "        model = 1 # pinhole\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2])\n",
    "    elif camera_model == 'simple-radial':\n",
    "        model = 2 # simple radial\n",
    "        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n",
    "    elif camera_model == 'opencv':\n",
    "        model = 4 # opencv\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n",
    "         \n",
    "    return db.add_camera(model, width, height, param_arr)\n",
    "\n",
    "\n",
    "def add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n",
    "    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n",
    "\n",
    "    camera_id = None\n",
    "    fname_to_id = {}\n",
    "    for filename in tqdm(list(keypoint_f.keys())):\n",
    "        keypoints = keypoint_f[filename][()]\n",
    "\n",
    "        fname_with_ext = filename# + img_ext\n",
    "        path = os.path.join(image_path, fname_with_ext)\n",
    "        if not os.path.isfile(path):\n",
    "            raise IOError(f'Invalid image path {path}')\n",
    "\n",
    "        if camera_id is None or not single_camera:\n",
    "            camera_id = create_camera(db, path, camera_model)\n",
    "        image_id = db.add_image(fname_with_ext, camera_id)\n",
    "        fname_to_id[filename] = image_id\n",
    "\n",
    "        db.add_keypoints(image_id, keypoints)\n",
    "\n",
    "    return fname_to_id\n",
    "\n",
    "def add_matches(db, h5_path, fname_to_id):\n",
    "    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n",
    "    \n",
    "    added = set()\n",
    "    n_keys = len(match_file.keys())\n",
    "    n_total = (n_keys * (n_keys - 1)) // 2\n",
    "\n",
    "    with tqdm(total=n_total) as pbar:\n",
    "        for key_1 in match_file.keys():\n",
    "            group = match_file[key_1]\n",
    "            for key_2 in group.keys():\n",
    "                id_1 = fname_to_id[key_1]\n",
    "                id_2 = fname_to_id[key_2]\n",
    "\n",
    "                pair_id = image_ids_to_pair_id(id_1, id_2)\n",
    "                if pair_id in added:\n",
    "                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n",
    "                    continue\n",
    "            \n",
    "                matches = group[key_2][()]\n",
    "                db.add_matches(id_1, id_2, matches)\n",
    "\n",
    "                added.add(pair_id)\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "def import_into_colmap(img_dir,\n",
    "                       feature_dir ='.featureout',\n",
    "                       database_path = 'colmap.db',\n",
    "                       img_ext='.jpg'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, CONFIG.CAMERA_MODEL, single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36598fbb",
   "metadata": {
    "papermill": {
     "duration": 0.013926,
     "end_time": "2024-05-23T06:36:55.252814",
     "exception": false,
     "start_time": "2024-05-23T06:36:55.238888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Rotation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "807b79a9",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:55.279800Z",
     "iopub.status.busy": "2024-05-23T06:36:55.279496Z",
     "iopub.status.idle": "2024-05-23T06:36:55.938622Z",
     "shell.execute_reply": "2024-05-23T06:36:55.937784Z"
    },
    "papermill": {
     "duration": 0.675207,
     "end_time": "2024-05-23T06:36:55.940969",
     "exception": false,
     "start_time": "2024-05-23T06:36:55.265762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swsl_resnext50_32x4d to current resnext50_32x4d.fb_swsl_ig1b_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image as T_read_image\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision import transforms as T\n",
    "from check_orientation.pre_trained_models import create_model\n",
    "\n",
    "def convert_rot_k(index):\n",
    "    if index == 0:\n",
    "        return 0\n",
    "    elif index == 1:\n",
    "        return 3\n",
    "    elif index == 2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "class CheckRotationDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.transform = transform\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgPath = self.files[idx]\n",
    "        image = T_read_image(imgPath, mode=ImageReadMode.RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def get_CheckRotation_dataloader(images, batch_size=1):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ConvertImageDtype(torch.float),\n",
    "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    dataset = CheckRotationDataset(images, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def exec_rotation_detection(img_files, device):\n",
    "    model = create_model(\"swsl_resnext50_32x4d\")\n",
    "    model.eval().to(device);\n",
    "    \n",
    "    dataloader = get_CheckRotation_dataloader(img_files)\n",
    "    \n",
    "    rots = []\n",
    "    for idx, image in enumerate(dataloader):\n",
    "        image = image.to(torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image).detach().cpu().numpy()\n",
    "            detected_rot = prediction[0].argmax()\n",
    "            rot_k = convert_rot_k(detected_rot)\n",
    "            rots.append(rot_k)\n",
    "            print(f\"{os.path.basename(img_files[idx])} > rot_k={rot_k}\")\n",
    "    return rots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa105233",
   "metadata": {
    "papermill": {
     "duration": 0.012936,
     "end_time": "2024-05-23T06:36:55.967774",
     "exception": false,
     "start_time": "2024-05-23T06:36:55.954838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3fdaba",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:55.995128Z",
     "iopub.status.busy": "2024-05-23T06:36:55.994821Z",
     "iopub.status.idle": "2024-05-23T06:36:56.012350Z",
     "shell.execute_reply": "2024-05-23T06:36:56.011505Z"
    },
    "papermill": {
     "duration": 0.033458,
     "end_time": "2024-05-23T06:36:56.014202",
     "exception": false,
     "start_time": "2024-05-23T06:36:55.980744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will use ViT global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, model,\n",
    "                    device =  torch.device('cpu')):\n",
    "    model = model.eval()\n",
    "    model= model.to(device)\n",
    "    config = resolve_data_config({}, model=model)\n",
    "    transform = create_transform(**config)\n",
    "    global_descs_convnext=[]\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        img = Image.open(img_fname_full).convert('RGB')\n",
    "        timg = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            desc = model.forward_features(timg.to(device)).mean(dim=(-1,2))#\n",
    "            #print (desc.shape)\n",
    "            desc = desc.view(1, -1)\n",
    "            desc_norm = F.normalize(desc, dim=1, p=2)\n",
    "        #print (desc_norm)\n",
    "        global_descs_convnext.append(desc_norm.detach().cpu())\n",
    "    global_descs_all = torch.cat(global_descs_convnext, dim=0)\n",
    "    return global_descs_all.to(torch.float32)\n",
    "\n",
    "def convert_1d_to_2d(idx, num_images):\n",
    "    idx1 = idx // num_images\n",
    "    idx2 = idx % num_images\n",
    "    return (idx1, idx2)\n",
    "\n",
    "def get_pairs_from_distancematrix(mat):\n",
    "    pairs = [ convert_1d_to_2d(idx, mat.shape[0]) for idx in np.argsort(mat.flatten())]\n",
    "    pairs = [ pair for pair in pairs if pair[0] < pair[1] ]\n",
    "    return pairs\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames, model, device):\n",
    "    #index_pairs = []\n",
    "    #for i in range(len(img_fnames)):\n",
    "    #    for j in range(i+1, len(img_fnames)):\n",
    "    #        index_pairs.append((i,j))\n",
    "    #return index_pairs\n",
    "    descs = get_global_desc(img_fnames, model, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    matching_list = get_pairs_from_distancematrix(dm)\n",
    "    return matching_list\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 20,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "\n",
    "    model = timm.create_model('tf_efficientnet_b7',\n",
    "                              checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n",
    "    model.eval()\n",
    "    descs = get_global_desc(fnames, model, device=device)\n",
    "\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames, model, device)\n",
    "    \n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # removing half\n",
    "    mask = dm <= sim_th\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < 1000:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582c108",
   "metadata": {
    "papermill": {
     "duration": 0.012688,
     "end_time": "2024-05-23T06:36:56.040250",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.027562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: LightGlue series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b99161e",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.067597Z",
     "iopub.status.busy": "2024-05-23T06:36:56.067341Z",
     "iopub.status.idle": "2024-05-23T06:36:56.091382Z",
     "shell.execute_reply": "2024-05-23T06:36:56.090637Z"
    },
    "papermill": {
     "duration": 0.040702,
     "end_time": "2024-05-23T06:36:56.093836",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.053134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,verbose=True\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    \n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = extractor_class(\n",
    "        max_num_keypoints=num_features, detection_threshold=detection_threshold, resize=resize_to\n",
    "    ).eval().to(device, dtype)\n",
    "        \n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "            kpts = convert_coord(kpts, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "            print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "):\n",
    "    t=time()\n",
    "    detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea771cbd",
   "metadata": {
    "papermill": {
     "duration": 0.012771,
     "end_time": "2024-05-23T06:36:56.120427",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.107656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e25aac27",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.148330Z",
     "iopub.status.busy": "2024-05-23T06:36:56.148046Z",
     "iopub.status.idle": "2024-05-23T06:36:56.211131Z",
     "shell.execute_reply": "2024-05-23T06:36:56.210412Z"
    },
    "papermill": {
     "duration": 0.079252,
     "end_time": "2024-05-23T06:36:56.213066",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.133814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../input/super-glue-pretrained-network\")\n",
    "from models.matching import Matching\n",
    "from models.superpoint import SuperPoint as SG_SuperPoint\n",
    "from models.superglue import SuperGlue\n",
    "from models.utils import (compute_pose_error, compute_epipolar_error,\n",
    "                          estimate_pose, make_matching_plot,\n",
    "                          error_colormap, AverageTimer, pose_auc, read_image,\n",
    "                          process_resize, frame2tensor,\n",
    "                          rotate_intrinsics, rotate_pose_inplane,\n",
    "                          scale_intrinsics)\n",
    "\n",
    "from torch.nn import functional as torchF  # For resizing tensor\n",
    "\n",
    "def sg_imread(path):\n",
    "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "    return image\n",
    "\n",
    "# Preprocess\n",
    "def sg_read_image(image, device, resize):\n",
    "    w, h = image.shape[1], image.shape[0]\n",
    "    w_new, h_new = process_resize(w, h, [resize,])\n",
    "    \n",
    "    unit_shape = 8\n",
    "    w_new = w_new // unit_shape * unit_shape\n",
    "    h_new = h_new // unit_shape * unit_shape\n",
    "    \n",
    "    scales = (float(w) / float(w_new), float(h) / float(h_new))\n",
    "    image = cv2.resize(image.astype('float32'), (w_new, h_new))\n",
    "\n",
    "    inp = frame2tensor(image, \"cpu\")\n",
    "    return image, inp, scales, (h, w)\n",
    "\n",
    "class SGDataset(Dataset):\n",
    "    def __init__(self, img_fnames, resize_to, device):\n",
    "        self.img_fnames = img_fnames\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_fnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.img_fnames[idx]\n",
    "        im = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
    "        _, image, scale, ori_shape = sg_read_image(im, self.device, self.resize_to)\n",
    "        return image, torch.tensor([idx]), torch.tensor(ori_shape)\n",
    "\n",
    "def get_superglue_dataloader(img_fnames, resize_to, device, batch_size=1):\n",
    "    dataset = SGDataset(img_fnames, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def detect_superglue(\n",
    "    img_fnames, index_pairs, feature_dir, device, sg_config, file_keypoints, \n",
    "    resize_to=750, min_matches=15\n",
    "):    \n",
    "    t=time()\n",
    "\n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "    dataloader = get_superglue_dataloader( img_fnames, resize_to, device)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    superpoint = SG_SuperPoint(sg_config[\"superpoint\"]).eval().to(device)\n",
    "    dict_features_cuda = {}\n",
    "    dict_shapes = {}\n",
    "    dict_images = {}\n",
    "    for X in dataloader:\n",
    "        image, idx, ori_shape = X\n",
    "        image = image[0].to(device)\n",
    "        fname = img_fnames[idx]\n",
    "        key = fname.split('/')[-1]\n",
    "        \n",
    "        with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            pred = superpoint({'image': image})\n",
    "            dict_features_cuda[key] = pred\n",
    "            dict_shapes[key] = ori_shape\n",
    "            dict_images[key] = image.half()\n",
    "    del superpoint\n",
    "    gc.collect()\n",
    "    \n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    superglue = SuperGlue(sg_config[\"superglue\"]).eval().to(device)\n",
    "    weights = sg_config[\"superglue\"][\"weights\"]\n",
    "    cnt_pairs = 0\n",
    "    \n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for idx, (fname1, fname2) in enumerate(zip(fnames1, fnames2)):\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            data = {\"image0\": dict_images[key1], \"image1\": dict_images[key2]}\n",
    "            data = {**data, **{k+'0': v for k, v in dict_features_cuda[key1].items()}}\n",
    "            data = {**data, **{k+'1': v for k, v in dict_features_cuda[key2].items()}}\n",
    "            for k in data:\n",
    "                if isinstance(data[k], (list, tuple)):\n",
    "                    data[k] = torch.stack(data[k])\n",
    "            with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                pred = {**data, **superglue(data)}\n",
    "                pred = {k: v[0].detach().cpu().numpy().copy() for k, v in pred.items()}\n",
    "            mkpts1, mkpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n",
    "            matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n",
    "\n",
    "            valid = matches > -1\n",
    "            mkpts1 = mkpts1[valid]\n",
    "            mkpts2 = mkpts2[matches[valid]]\n",
    "            mconf = conf[valid]\n",
    "\n",
    "            ori_shape_1 = dict_shapes[key1][0].numpy()\n",
    "            ori_shape_2 = dict_shapes[key2][0].numpy()\n",
    "            \n",
    "            # Scaling coords\n",
    "            mkpts1[:,0] = mkpts1[:,0] * ori_shape_1[1] / dict_images[key1].shape[3]   # X\n",
    "            mkpts1[:,1] = mkpts1[:,1] * ori_shape_1[0] / dict_images[key1].shape[2]   # Y\n",
    "            mkpts2[:,0] = mkpts2[:,0] * ori_shape_2[1] / dict_images[key2].shape[3]   # X\n",
    "            mkpts2[:,1] = mkpts2[:,1] * ori_shape_2[0] / dict_images[key2].shape[2]   # Y  \n",
    "            \n",
    "            n_matches = mconf.shape[0]\n",
    "            \n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(superglue/{resize_to}/{weights})')            \n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')            \n",
    "\n",
    "    del superglue\n",
    "    del dict_features_cuda\n",
    "    del dict_images\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da86a891",
   "metadata": {
    "papermill": {
     "duration": 0.012937,
     "end_time": "2024-05-23T06:36:56.239461",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.226524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: DKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64df301d",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.266899Z",
     "iopub.status.busy": "2024-05-23T06:36:56.266633Z",
     "iopub.status.idle": "2024-05-23T06:36:56.294003Z",
     "shell.execute_reply": "2024-05-23T06:36:56.293086Z"
    },
    "papermill": {
     "duration": 0.04326,
     "end_time": "2024-05-23T06:36:56.295816",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.252556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DKMDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, resize_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        self.test_transform = get_tuple_transform_ops(\n",
    "            resize=self.resize_to, normalize=True\n",
    "        )\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "                \n",
    "        im1, im2 = Image.open(fname1), Image.open(fname2)\n",
    "        ori_shape_1 = im1.size\n",
    "        ori_shape_2 = im2.size\n",
    "        image1, image2 = self.test_transform((im1, im2))\n",
    "        return image1, image2, torch.tensor([idx]), torch.tensor(ori_shape_1), torch.tensor(ori_shape_2)\n",
    "\n",
    "def get_dkm_dataloader(images1, images2, resize_to, device, batch_size=4):\n",
    "    dataset = DKMDataset(images1, images2, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def get_dkm_mkpts(dkm_model, bimgs1, bimgs2, shapes1, shapes2, detection_threshold=0.5, num_features = 2000, min_matches=15):\n",
    "    dense_matches, dense_certainty = dkm_model.match(bimgs1, bimgs2, batched=True)\n",
    "    print(\"***\", dense_matches.shape, dense_certainty.shape)\n",
    "\n",
    "    store_mkpts1, store_mkpts2, store_mconf = [], [], []\n",
    "    # drop low confidence pairs\n",
    "    for b in range(dense_matches.shape[0]):\n",
    "        u_dense_matches = dense_matches[b, dense_certainty[b,...].sqrt() >= detection_threshold, :]\n",
    "        u_dense_certainty = dense_certainty[b, dense_certainty[b,...].sqrt() >= detection_threshold]\n",
    "    \n",
    "        if u_dense_matches.shape[0] > num_features:\n",
    "            u_dense_matches, u_dense_certainty = dkm_model.sample( u_dense_matches, u_dense_certainty, num=num_features)\n",
    "        \n",
    "        u_dense_matches = u_dense_matches.reshape((-1, 4))\n",
    "        u_dense_certainty = u_dense_certainty.reshape((-1,))\n",
    "    \n",
    "        mkpts1 = u_dense_matches[:, :2]\n",
    "        mkpts2 = u_dense_matches[:, 2:]\n",
    "        \n",
    "        w1, h1 = shapes1[b, :]\n",
    "        w2, h2 = shapes2[b, :]\n",
    "\n",
    "        mkpts1[:, 0] = ((mkpts1[:, 0] + 1)/2) * w1\n",
    "        mkpts1[:, 1] = ((mkpts1[:, 1] + 1)/2) * h1\n",
    "\n",
    "        mkpts2[:, 0] = ((mkpts2[:, 0] + 1)/2) * w2\n",
    "        mkpts2[:, 1] = ((mkpts2[:, 1] + 1)/2) * h2\n",
    "\n",
    "        mkpts1 = mkpts1.cpu().detach().numpy()\n",
    "        mkpts2 = mkpts2.cpu().detach().numpy()\n",
    "        mconf  = u_dense_certainty.sqrt().cpu().detach().numpy()\n",
    "\n",
    "        \n",
    "        if mconf.shape[0] > min_matches:\n",
    "            try:\n",
    "                # calc Fundamental matrix from keypoints\n",
    "                F, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.200, 0.999, 2000)\n",
    "                inliers = inliers > 0\n",
    "                mkpts1 = mkpts1[inliers[:,0]]\n",
    "                mkpts2 = mkpts2[inliers[:,0]]\n",
    "                mconf  = mconf[inliers[:,0]]\n",
    "                #print(\"---\", mconf.shape)\n",
    "                if mconf.shape[0] > 3000:\n",
    "                    rand_idx = np.random.choice(range(mconf.shape[0]), 3000, replace=False)\n",
    "                    mkpts1 = mkpts1[rand_idx, :]\n",
    "                    mkpts2 = mkpts2[rand_idx, :]\n",
    "                    mconf  = mconf[rand_idx]\n",
    "            except:\n",
    "                mkpts1 = np.empty((0,2))\n",
    "                mkpts2 = np.empty((0,2))\n",
    "                mconf = np.empty((0,))\n",
    "        \n",
    "        store_mkpts1.append(mkpts1)\n",
    "        store_mkpts2.append(mkpts2)\n",
    "        store_mconf.append(mconf)\n",
    "    return store_mkpts1, store_mkpts2, store_mconf\n",
    "\n",
    "def detect_dkm(\n",
    "    img_fnames, index_pairs, feature_dir, device, \n",
    "    resize_to=(540, 720), \n",
    "    detection_threshold=0.4, \n",
    "    num_features=2000, \n",
    "    min_matches=15,\n",
    "):\n",
    "    t=time()\n",
    "    dkm_model = DKMv3_outdoor(device=device)\n",
    "    dkm_model.upsample_preds=False\n",
    "\n",
    "    fnames1, fnames2 = [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(f'{feature_dir}/matches_dkm.h5', mode='w') as f_match:    \n",
    "        dataloader = get_dkm_dataloader(fnames1, fnames2, resize_to, device, batch_size=4)\n",
    "        for X in tqdm(dataloader):\n",
    "            images1, images2, idxs, shapes1, shapes2 = X\n",
    "            store_mkpts1, store_mkpts2, store_mconf = get_dkm_mkpts(\n",
    "                dkm_model, images1.to(device), images2.to(device), shapes1, shapes2, \n",
    "                detection_threshold=detection_threshold, num_features = num_features, min_matches=min_matches,\n",
    "            )\n",
    "            \n",
    "            for b in range(images1.shape[0]):\n",
    "                mkpts1 = store_mkpts1[b]\n",
    "                mkpts2 = store_mkpts2[b]\n",
    "                mconf = store_mconf[b]\n",
    "                file1 = fnames1[idxs[b]]\n",
    "                file2 = fnames2[idxs[b]]\n",
    "                key1, key2 = file1.split('/')[-1], file2.split('/')[-1]\n",
    "            \n",
    "                n_matches = mconf.shape[0]\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(dkm)')            \n",
    "\n",
    "                group  = f_match.require_group(key1)\n",
    "                if n_matches >= min_matches:\n",
    "                    group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                    cnt_pairs+=1\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc9061",
   "metadata": {
    "papermill": {
     "duration": 0.012606,
     "end_time": "2024-05-23T06:36:56.321315",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.308709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: LoFTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bd3d6a6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.348528Z",
     "iopub.status.busy": "2024-05-23T06:36:56.348111Z",
     "iopub.status.idle": "2024-05-23T06:36:56.370283Z",
     "shell.execute_reply": "2024-05-23T06:36:56.369476Z"
    },
    "papermill": {
     "duration": 0.037977,
     "end_time": "2024-05-23T06:36:56.372152",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.334175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoFTRDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, idxs1, idxs2, resize_small_edge_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.keys1 = [ fname.split('/')[-1] for fname in fnames1 ]\n",
    "        self.keys2 = [ fname.split('/')[-1] for fname in fnames2 ]\n",
    "        self.idxs1 = idxs1\n",
    "        self.idxs2 = idxs2\n",
    "        self.resize_small_edge_to = resize_small_edge_to\n",
    "        self.device = device\n",
    "        self.round_unit = 16\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images1)\n",
    "\n",
    "    def load_torch_image(self, fname, device):\n",
    "        img = cv2.imread(fname)\n",
    "        original_shape = img.shape\n",
    "        ratio = self.resize_small_edge_to / min([img.shape[0], img.shape[1]])\n",
    "        w = int(img.shape[1] * ratio) # int( (img.shape[1] * ratio) // self.round_unit * self.round_unit )\n",
    "        h = int(img.shape[0] * ratio) # int( (img.shape[0] * ratio) // self.round_unit * self.round_unit )\n",
    "        img_resized = cv2.resize(img, (w, h))\n",
    "        img_resized = K.image_to_tensor(img_resized, False).float() /255.\n",
    "        img_resized = K.color.bgr_to_rgb(img_resized)\n",
    "        img_resized = K.color.rgb_to_grayscale(img_resized)\n",
    "        return img_resized.to(device), original_shape\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "        image1, ori_shape_1 = self.load_torch_image(fname1, device)\n",
    "        image2, ori_shape_2 = self.load_torch_image(fname2, device)\n",
    "\n",
    "        return image1, image2, self.keys1[idx], self.keys2[idx], self.idxs1[idx], self.idxs2[idx], ori_shape_1, ori_shape_2\n",
    "\n",
    "def get_loftr_dataloader(images1, images2, idxs1, idxs2, resize_small_edge_to, device, batch_size=1):\n",
    "    dataset = LoFTRDataset(images1, images2, idxs1, idxs2, resize_small_edge_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataset\n",
    "    \n",
    "def detect_loftr(img_fnames, index_pairs, feature_dir, device, file_keypoints, resize_small_edge_to=750, min_matches=15):\n",
    "    t=time()\n",
    "\n",
    "    matcher = LoFTR(pretrained=None)\n",
    "    matcher.load_state_dict(torch.load(\"../input/loftr/pytorch/outdoor/1/loftr_outdoor.ckpt\")['state_dict'])\n",
    "    matcher = matcher.to(device).eval()\n",
    "\n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "        \n",
    "    dataloader = get_loftr_dataloader( fnames1, fnames2, idxs1, idxs2, resize_small_edge_to, device)\n",
    "\n",
    "    cnt_pairs = 0\n",
    "\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:    \n",
    "        store_mkpts = {}\n",
    "        for X in tqdm(dataloader):\n",
    "            image1, image2, key1, key2, idx1, idx2, ori_shape_1, ori_shape_2 = X\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                correspondences = matcher( {\"image0\": image1.to(device),\"image1\": image2.to(device)} )\n",
    "                mkpts1 = correspondences['keypoints0'].cpu().numpy()\n",
    "                mkpts2 = correspondences['keypoints1'].cpu().numpy()\n",
    "                mconf  = correspondences['confidence'].cpu().numpy()\n",
    "\n",
    "            mkpts1[:,0] *= (float(ori_shape_1[1]) / float(image1.shape[3]))\n",
    "            mkpts1[:,1] *= (float(ori_shape_1[0]) / float(image1.shape[2]))\n",
    "\n",
    "            mkpts2[:,0] *= (float(ori_shape_2[1]) / float(image2.shape[3]))\n",
    "            mkpts2[:,1] *= (float(ori_shape_2[0]) / float(image2.shape[2]))\n",
    "            \n",
    "            n_matches = mconf.shape[0]\n",
    "            \n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(loftr)')\n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d30bf",
   "metadata": {
    "papermill": {
     "duration": 0.01268,
     "end_time": "2024-05-23T06:36:56.398169",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.385489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: DKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74571e75",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.425420Z",
     "iopub.status.busy": "2024-05-23T06:36:56.425058Z",
     "iopub.status.idle": "2024-05-23T06:36:56.451423Z",
     "shell.execute_reply": "2024-05-23T06:36:56.450547Z"
    },
    "papermill": {
     "duration": 0.042299,
     "end_time": "2024-05-23T06:36:56.453398",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.411099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DKMDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, resize_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        self.test_transform = get_tuple_transform_ops(\n",
    "            resize=self.resize_to, normalize=True\n",
    "        )\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "                \n",
    "        im1, im2 = Image.open(fname1), Image.open(fname2)\n",
    "        ori_shape_1 = im1.size\n",
    "        ori_shape_2 = im2.size\n",
    "        image1, image2 = self.test_transform((im1, im2))\n",
    "        return image1, image2, torch.tensor([idx]), torch.tensor(ori_shape_1), torch.tensor(ori_shape_2)\n",
    "\n",
    "def get_dkm_dataloader(images1, images2, resize_to, device, batch_size=4):\n",
    "    dataset = DKMDataset(images1, images2, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def get_dkm_mkpts(dkm_model, bimgs1, bimgs2, shapes1, shapes2, detection_threshold=0.5, num_features = 2000, min_matches=15):\n",
    "    dense_matches, dense_certainty = dkm_model.match(bimgs1, bimgs2, batched=True)\n",
    "\n",
    "    store_mkpts1, store_mkpts2, store_mconf = [], [], []\n",
    "    # drop low confidence pairs\n",
    "    for b in range(dense_matches.shape[0]):\n",
    "        u_dense_matches = dense_matches[b, dense_certainty[b,...].sqrt() >= detection_threshold, :]\n",
    "        u_dense_certainty = dense_certainty[b, dense_certainty[b,...].sqrt() >= detection_threshold]\n",
    "    \n",
    "        if u_dense_matches.shape[0] > num_features:\n",
    "            u_dense_matches, u_dense_certainty = dkm_model.sample( u_dense_matches, u_dense_certainty, num=num_features)\n",
    "        \n",
    "        u_dense_matches = u_dense_matches.reshape((-1, 4))\n",
    "        u_dense_certainty = u_dense_certainty.reshape((-1,))\n",
    "    \n",
    "        mkpts1 = u_dense_matches[:, :2]\n",
    "        mkpts2 = u_dense_matches[:, 2:]\n",
    "        \n",
    "        w1, h1 = shapes1[b, :]\n",
    "        w2, h2 = shapes2[b, :]\n",
    "\n",
    "        mkpts1[:, 0] = ((mkpts1[:, 0] + 1)/2) * w1\n",
    "        mkpts1[:, 1] = ((mkpts1[:, 1] + 1)/2) * h1\n",
    "\n",
    "        mkpts2[:, 0] = ((mkpts2[:, 0] + 1)/2) * w2\n",
    "        mkpts2[:, 1] = ((mkpts2[:, 1] + 1)/2) * h2\n",
    "\n",
    "        mkpts1 = mkpts1.cpu().detach().numpy()\n",
    "        mkpts2 = mkpts2.cpu().detach().numpy()\n",
    "        mconf  = u_dense_certainty.sqrt().cpu().detach().numpy()\n",
    "\n",
    "        if mconf.shape[0] > min_matches:\n",
    "            try:\n",
    "                # calc Fundamental matrix from keypoints\n",
    "                F, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.200, 0.999, 2000)\n",
    "                inliers = inliers > 0\n",
    "                mkpts1 = mkpts1[inliers[:,0]]\n",
    "                mkpts2 = mkpts2[inliers[:,0]]\n",
    "                mconf  = mconf[inliers[:,0]]\n",
    "            except:\n",
    "                pass\n",
    "        store_mkpts1.append(mkpts1)\n",
    "        store_mkpts2.append(mkpts2)\n",
    "        store_mconf.append(mconf)\n",
    "    return store_mkpts1, store_mkpts2, store_mconf\n",
    "\n",
    "def detect_dkm(\n",
    "    img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "    resize_to=(540, 720), \n",
    "    detection_threshold=0.4, \n",
    "    num_features=2000, \n",
    "    min_matches=15\n",
    "):\n",
    "    t=time()\n",
    "    dkm_model = DKMv3_outdoor(device=device)\n",
    "    dkm_model.upsample_preds=False\n",
    "\n",
    "    fnames1, fnames2 = [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:    \n",
    "        dataloader = get_dkm_dataloader(fnames1, fnames2, resize_to, device, batch_size=4)\n",
    "        for X in tqdm(dataloader):\n",
    "            images1, images2, idxs, shapes1, shapes2 = X\n",
    "            store_mkpts1, store_mkpts2, store_mconf = get_dkm_mkpts(\n",
    "                dkm_model, images1.to(device), images2.to(device), shapes1, shapes2, \n",
    "                detection_threshold=detection_threshold, num_features = num_features, min_matches=min_matches,\n",
    "            )\n",
    "            \n",
    "            for b in range(images1.shape[0]):\n",
    "                mkpts1 = store_mkpts1[b]\n",
    "                mkpts2 = store_mkpts2[b]\n",
    "                mconf = store_mconf[b]\n",
    "                file1 = fnames1[idxs[b]]\n",
    "                file2 = fnames2[idxs[b]]\n",
    "                key1, key2 = file1.split('/')[-1], file2.split('/')[-1]\n",
    "            \n",
    "                n_matches = mconf.shape[0]\n",
    "\n",
    "                group  = f_match.require_group(key1)\n",
    "                if n_matches >= min_matches:\n",
    "                    group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                    cnt_pairs+=1\n",
    "                    print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(dkm)')\n",
    "                else:\n",
    "                    print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede114e",
   "metadata": {
    "papermill": {
     "duration": 0.012803,
     "end_time": "2024-05-23T06:36:56.479444",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.466641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: MatchFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b193c2b",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.506295Z",
     "iopub.status.busy": "2024-05-23T06:36:56.505998Z",
     "iopub.status.idle": "2024-05-23T06:36:56.533297Z",
     "shell.execute_reply": "2024-05-23T06:36:56.532394Z"
    },
    "papermill": {
     "duration": 0.042993,
     "end_time": "2024-05-23T06:36:56.535285",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.492292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MatchFormerDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, idxs1, idxs2, resize_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.keys1 = [ fname.split('/')[-1] for fname in fnames1 ]\n",
    "        self.keys2 = [ fname.split('/')[-1] for fname in fnames2 ]\n",
    "        self.idxs1 = idxs1\n",
    "        self.idxs2 = idxs2\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        self.round_unit = 16\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images1)\n",
    "\n",
    "    def load_torch_image(self, fname, device):\n",
    "        img = cv2.imread(fname)\n",
    "        original_shape = img.shape\n",
    "        #ratio = self.resize_long_edge_to / max([img.shape[0], img.shape[1]])\n",
    "        #w = int(img.shape[1] * ratio)\n",
    "        #h = int(img.shape[0] * ratio)\n",
    "        img_resized = cv2.resize(img, self.resize_to)\n",
    "        img_resized = K.image_to_tensor(img_resized, False).float() /255.\n",
    "        img_resized = K.color.bgr_to_rgb(img_resized)\n",
    "        img_resized = K.color.rgb_to_grayscale(img_resized)\n",
    "        return img_resized.to(device), original_shape\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "        image1, ori_shape_1 = self.load_torch_image(fname1, device)\n",
    "        image2, ori_shape_2 = self.load_torch_image(fname2, device)\n",
    "\n",
    "        return image1, image2, self.keys1[idx], self.keys2[idx], self.idxs1[idx], self.idxs2[idx], ori_shape_1, ori_shape_2\n",
    "\n",
    "def get_matchformer_dataloader(images1, images2, idxs1, idxs2, resize_to, device, batch_size=1):\n",
    "    dataset = MatchFormerDataset(images1, images2, idxs1, idxs2, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataset\n",
    "    \n",
    "def detect_matchformer(\n",
    "    img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "    resize_to=(560, 750), \n",
    "    detection_threshold=0.4, \n",
    "    num_features=2000, \n",
    "    min_matches=15\n",
    "):\n",
    "    t=time()\n",
    "\n",
    "    sys.path.append('/kaggle/input/matchformer/MatchFormer-main')\n",
    "\n",
    "    from yacs.config import CfgNode as CN\n",
    "    from model.matchformer import Matchformer\n",
    "    from config import defaultmf\n",
    "\n",
    "    cfg = defaultmf.get_cfg_defaults()\n",
    "    cfg.MATCHFORMER.BACKBONE_TYPE = 'largela'\n",
    "    cfg.MATCHFORMER.SCENS = 'outdoor'\n",
    "    cfg.MATCHFORMER.RESOLUTION = (8,2)\n",
    "    cfg.MATCHFORMER.MATCH_COARSE.THR = detection_threshold\n",
    "\n",
    "    def lower_config(yacs_cfg):\n",
    "        if not isinstance(yacs_cfg, CN):\n",
    "            return yacs_cfg\n",
    "        return {k.lower(): lower_config(v) for k, v in yacs_cfg.items()}\n",
    "\n",
    "    _cfg = lower_config(cfg)\n",
    "\n",
    "    matcher_mf = Matchformer(_cfg['matchformer'])\n",
    "\n",
    "    pretrained_ckpt = '/kaggle/input/matchformer/outdoor-large-LA.ckpt'\n",
    "    matcher_mf.load_state_dict({k.replace('matcher.',''):v  for k,v in torch.load(pretrained_ckpt, map_location='cpu').items()})\n",
    "    matcher_mf = matcher_mf.to(device).eval()\n",
    "    \n",
    "    \n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:    \n",
    "        dataloader = get_matchformer_dataloader(fnames1, fnames2, idxs1, idxs2, resize_to, device, batch_size=1)\n",
    "        for X in tqdm(dataloader):\n",
    "            image1, image2, key1, key2, idx1, idx2, ori_shape_1, ori_shape_2 = X\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            #print(image1.shape, image2.shape)\n",
    "            input_dict = {\n",
    "                \"image0\": image1, \n",
    "                \"image1\": image2\n",
    "            }\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                matcher_mf(input_dict)\n",
    "\n",
    "            conf = input_dict['mconf'].to('cpu').numpy()\n",
    "            mkpts1 = input_dict['mkpts0_f'].to('cpu').numpy()\n",
    "            mkpts2 = input_dict['mkpts1_f'].to('cpu').numpy()\n",
    "\n",
    "            sorted_idx = np.argsort(-conf)\n",
    "            if len(conf) > num_features:\n",
    "                mkpts1 = mkpts1[sorted_idx[:num_features], :]\n",
    "                mkpts2 = mkpts2[sorted_idx[:num_features], :]\n",
    "\n",
    "            mkpts1[:,0] = mkpts1[:,0] * ori_shape_1[1] / image1.shape[3]\n",
    "            mkpts1[:,1] = mkpts1[:,1] * ori_shape_1[0] / image1.shape[2]\n",
    "\n",
    "            mkpts2[:,0] = mkpts2[:,0] * ori_shape_2[1] / image2.shape[3]\n",
    "            mkpts2[:,1] = mkpts2[:,1] * ori_shape_2[0] / image2.shape[2]\n",
    "                \n",
    "            n_matches = mkpts1.shape[0]\n",
    "\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(MatchFormer)')\n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41412b9a",
   "metadata": {
    "papermill": {
     "duration": 0.015388,
     "end_time": "2024-05-23T06:36:56.564457",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.549069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df910f47",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.594274Z",
     "iopub.status.busy": "2024-05-23T06:36:56.593945Z",
     "iopub.status.idle": "2024-05-23T06:36:56.644302Z",
     "shell.execute_reply": "2024-05-23T06:36:56.643547Z"
    },
    "papermill": {
     "duration": 0.067491,
     "end_time": "2024-05-23T06:36:56.646248",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.578757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 0.15, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                #print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            \n",
    "            print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "        \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e047747",
   "metadata": {
    "papermill": {
     "duration": 0.013265,
     "end_time": "2024-05-23T06:36:56.672973",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.659708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "932e787c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.700469Z",
     "iopub.status.busy": "2024-05-23T06:36:56.700188Z",
     "iopub.status.idle": "2024-05-23T06:36:56.737464Z",
     "shell.execute_reply": "2024-05-23T06:36:56.736543Z"
    },
    "papermill": {
     "duration": 0.054349,
     "end_time": "2024-05-23T06:36:56.740398",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.686049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, \n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_disk_lightglue:\n",
    "        model_name = \"disk\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_disk_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_disk_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_disk_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_disk_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_sift_lightglue:\n",
    "        model_name = \"sift\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_sift_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_sift_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_sift_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_sift_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_loftr:\n",
    "        file_keypoints = f'{feature_dir}/matches_loftr_{CONFIG.params_loftr[\"resize_small_edge_to\"]}pix.h5'\n",
    "        t = detect_loftr(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_small_edge_to=CONFIG.params_loftr[\"resize_small_edge_to\"],\n",
    "            min_matches=CONFIG.params_loftr[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_dkm:\n",
    "        file_keypoints = f'{feature_dir}/matches_dkm.h5'\n",
    "        t = detect_dkm(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_dkm[\"resize_to\"], \n",
    "            detection_threshold=CONFIG.params_dkm[\"detection_threshold\"], \n",
    "            num_features=CONFIG.params_dkm[\"num_features\"], \n",
    "            min_matches=CONFIG.params_dkm[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_matchformer:\n",
    "        file_keypoints = f'{feature_dir}/matches_matchformer_{CONFIG.params_matchformer[\"resize_to\"]}pix.h5'\n",
    "        t = detect_matchformer(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_matchformer[\"resize_to\"],\n",
    "            num_features=CONFIG.params_matchformer[\"num_features\"], \n",
    "            min_matches=CONFIG.params_matchformer[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7952470b",
   "metadata": {
    "papermill": {
     "duration": 0.017054,
     "end_time": "2024-05-23T06:36:56.775639",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.758585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reconstruction wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92644b37",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.804057Z",
     "iopub.status.busy": "2024-05-23T06:36:56.803253Z",
     "iopub.status.idle": "2024-05-23T06:36:56.816534Z",
     "shell.execute_reply": "2024-05-23T06:36:56.815830Z"
    },
    "papermill": {
     "duration": 0.029083,
     "end_time": "2024-05-23T06:36:56.818418",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.789335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_from_db(dataset, scene, feature_dir, img_dir, timings, image_paths):\n",
    "    scene_result = {}\n",
    "    #############################################################\n",
    "    # regist keypoints from h5 into colmap db\n",
    "    #############################################################\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "\n",
    "    #############################################################\n",
    "    # Calculate fundamental matrix with colmap api\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    options = pycolmap.SiftMatchingOptions()\n",
    "    options.confidence = 0.9999\n",
    "    options.max_num_trials = 20000\n",
    "    pycolmap.match_exhaustive(database_path, sift_options=options)\n",
    "    t=time() - t \n",
    "    timings['RANSAC'].append(t)\n",
    "    print(f'RANSAC in  {t:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Execute bundle adjustmnet with colmap api\n",
    "    # --> Bundle adjustment Calcs Camera matrix, R and t\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "    mapper_options.min_model_size = 3\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n",
    "    print(maps)\n",
    "    clear_output(wait=False)\n",
    "    t=time() - t\n",
    "    timings['Reconstruction'].append(t)\n",
    "    print(f'Reconstruction done in  {t:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Extract R,t from maps \n",
    "    #############################################################            \n",
    "    imgs_registered  = 0\n",
    "    best_idx = None\n",
    "    list_num_images = []            \n",
    "    print (\"Looking for the best reconstruction\")\n",
    "    if isinstance(maps, dict):\n",
    "        for idx1, rec in maps.items():\n",
    "            print (idx1, rec.summary())\n",
    "            list_num_images.append( len(rec.images) )\n",
    "            if len(rec.images) > imgs_registered:\n",
    "                imgs_registered = len(rec.images)\n",
    "                best_idx = idx1\n",
    "    list_num_images = np.array(list_num_images)\n",
    "    print(f\"list_num_images = {list_num_images}\")\n",
    "    if best_idx is not None:\n",
    "        print (maps[best_idx].summary())\n",
    "        for k, im in maps[best_idx].images.items():\n",
    "            key1 = f'test/{dataset}/images/{im.name}'\n",
    "            scene_result[key1] = {}\n",
    "            scene_result[key1][\"R\"] = deepcopy(im.rotmat())\n",
    "            scene_result[key1][\"t\"] = deepcopy(np.array(im.tvec))\n",
    "\n",
    "    print(f'Registered: {dataset} / {scene} -> {len(scene_result)} images')\n",
    "    print(f'Total: {dataset} / {scene} -> {len(image_paths)} images')\n",
    "    print(timings)\n",
    "    return scene_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37472b60",
   "metadata": {
    "papermill": {
     "duration": 0.012607,
     "end_time": "2024-05-23T06:36:56.843901",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.831294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a98e291",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.870926Z",
     "iopub.status.busy": "2024-05-23T06:36:56.870660Z",
     "iopub.status.idle": "2024-05-23T06:36:56.879906Z",
     "shell.execute_reply": "2024-05-23T06:36:56.879193Z"
    },
    "papermill": {
     "duration": 0.024909,
     "end_time": "2024-05-23T06:36:56.881805",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.856896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def arr_to_str(a):\n",
    "    return ';'.join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "# Function to create a submission file.\n",
    "def create_submission(out_results, data_dict):\n",
    "    with open(f'submission.csv', 'w') as f:\n",
    "        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in data_dict:\n",
    "            if dataset in out_results:\n",
    "                res = out_results[dataset]\n",
    "            else:\n",
    "                res = {}\n",
    "            for scene in data_dict[dataset]:\n",
    "                if scene in res:\n",
    "                    scene_res = res[scene]\n",
    "                else:\n",
    "                    scene_res = {\"R\":{}, \"t\":{}}\n",
    "                for image in data_dict[dataset][scene]:\n",
    "                    if image in scene_res:\n",
    "                        print (image)\n",
    "                        R = scene_res[image]['R'].reshape(-1)\n",
    "                        T = scene_res[image]['t'].reshape(-1)\n",
    "                    else:\n",
    "                        R = np.eye(3).reshape(-1)\n",
    "                        T = np.zeros((3))\n",
    "                    f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884b0dd3",
   "metadata": {
    "papermill": {
     "duration": 0.01281,
     "end_time": "2024-05-23T06:36:56.950547",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.937737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fb09545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:56.977858Z",
     "iopub.status.busy": "2024-05-23T06:36:56.977513Z",
     "iopub.status.idle": "2024-05-23T06:36:56.988477Z",
     "shell.execute_reply": "2024-05-23T06:36:56.987455Z"
    },
    "papermill": {
     "duration": 0.026866,
     "end_time": "2024-05-23T06:36:56.990383",
     "exception": false,
     "start_time": "2024-05-23T06:36:56.963517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "church / church -> 41 images\n"
     ]
    }
   ],
   "source": [
    "src = '/kaggle/input/image-matching-challenge-2024'\n",
    "\n",
    "# Get data from csv.\n",
    "data_dict = {}\n",
    "with open(f'{src}/sample_submission.csv', 'r') as f:\n",
    "    for i, l in enumerate(f):\n",
    "        # Skip header.\n",
    "        if l and i > 0:\n",
    "            image, dataset, scene, _, _ = l.strip().split(',')\n",
    "            if dataset not in data_dict:\n",
    "                data_dict[dataset] = {}\n",
    "            if scene not in data_dict[dataset]:\n",
    "                data_dict[dataset][scene] = []\n",
    "            data_dict[dataset][scene].append(image)\n",
    "            \n",
    "            if CONFIG.DRY_RUN:\n",
    "                if len(data_dict[dataset][scene]) == CONFIG.DRY_RUN_MAX_IMAGES:\n",
    "                    break\n",
    "                    \n",
    "for dataset in data_dict:\n",
    "    for scene in data_dict[dataset]:\n",
    "        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06cbe5b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:36:57.018452Z",
     "iopub.status.busy": "2024-05-23T06:36:57.018165Z",
     "iopub.status.idle": "2024-05-23T06:52:43.113626Z",
     "shell.execute_reply": "2024-05-23T06:52:43.112491Z"
    },
    "papermill": {
     "duration": 946.112573,
     "end_time": "2024-05-23T06:52:43.116399",
     "exception": false,
     "start_time": "2024-05-23T06:36:57.003826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction done in  334.1044 sec\n",
      "Looking for the best reconstruction\n",
      "0 Reconstruction:\n",
      "\tnum_reg_images = 38\n",
      "\tnum_cameras = 38\n",
      "\tnum_points3D = 21916\n",
      "\tnum_observations = 128855\n",
      "\tmean_track_length = 5.87949\n",
      "\tmean_observations_per_image = 3390.92\n",
      "\tmean_reprojection_error = 0.953341\n",
      "list_num_images = [38]\n",
      "Reconstruction:\n",
      "\tnum_reg_images = 38\n",
      "\tnum_cameras = 38\n",
      "\tnum_points3D = 21916\n",
      "\tnum_observations = 128855\n",
      "\tmean_track_length = 5.87949\n",
      "\tmean_observations_per_image = 3390.92\n",
      "\tmean_reprojection_error = 0.953341\n",
      "Registered: church / church -> 38 images\n",
      "Total: church / church -> 41 images\n",
      "{'rotation_detection': [5.0067901611328125e-06], 'shortlisting': [12.882001399993896], 'feature_detection': [], 'feature_matching': [580.8252444267273], 'RANSAC': [7.839468240737915], 'Reconstruction': [334.1043744087219]}\n",
      "  => Merged observations: 0\n",
      "  => Filtered observations: 3\n",
      "  => Changed observations: 0.000062\n",
      "  => Filtered images: 3\n",
      "\n",
      "==============================================================================\n",
      "Finding good initial image pair\n",
      "==============================================================================\n",
      "\n",
      "  => No good initial image pair found.\n",
      "\n",
      "Elapsed time: 5.568 [minutes]\n",
      "test/church/images/00046.png\n",
      "test/church/images/00090.png\n",
      "test/church/images/00092.png\n",
      "test/church/images/00087.png\n",
      "test/church/images/00050.png\n",
      "test/church/images/00068.png\n",
      "test/church/images/00083.png\n",
      "test/church/images/00096.png\n",
      "test/church/images/00069.png\n",
      "test/church/images/00081.png\n",
      "test/church/images/00042.png\n",
      "test/church/images/00018.png\n",
      "test/church/images/00030.png\n",
      "test/church/images/00024.png\n",
      "test/church/images/00032.png\n",
      "test/church/images/00026.png\n",
      "test/church/images/00037.png\n",
      "test/church/images/00008.png\n",
      "test/church/images/00035.png\n",
      "test/church/images/00021.png\n",
      "test/church/images/00010.png\n",
      "test/church/images/00039.png\n",
      "test/church/images/00011.png\n",
      "test/church/images/00013.png\n",
      "test/church/images/00006.png\n",
      "test/church/images/00012.png\n",
      "test/church/images/00029.png\n",
      "test/church/images/00001.png\n",
      "test/church/images/00072.png\n",
      "test/church/images/00066.png\n",
      "test/church/images/00058.png\n",
      "test/church/images/00059.png\n",
      "test/church/images/00111.png\n",
      "test/church/images/00061.png\n",
      "test/church/images/00074.png\n",
      "test/church/images/00102.png\n",
      "test/church/images/00076.png\n",
      "test/church/images/00063.png\n"
     ]
    }
   ],
   "source": [
    "out_results = {}\n",
    "timings = {\n",
    "    \"rotation_detection\" : [],\n",
    "    \"shortlisting\":[],\n",
    "   \"feature_detection\": [],\n",
    "   \"feature_matching\":[],\n",
    "   \"RANSAC\": [],\n",
    "   \"Reconstruction\": []\n",
    "}\n",
    "\n",
    "gc.collect()\n",
    "datasets = []\n",
    "for dataset in data_dict:\n",
    "    datasets.append(dataset)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    futures = defaultdict(dict)\n",
    "    for dataset in datasets:\n",
    "        print(dataset)\n",
    "        if dataset not in out_results:\n",
    "            out_results[dataset] = {}\n",
    "        for scene in data_dict[dataset]:\n",
    "            print(scene)\n",
    "            # Fail gently if the notebook has not been submitted and the test data is not populated.\n",
    "            # You may want to run this on the training data in that case?\n",
    "            img_dir = f'{src}/test/{dataset}/images'\n",
    "            if not os.path.exists(img_dir):\n",
    "                continue\n",
    "\n",
    "            out_results[dataset][scene] = {}\n",
    "            img_fnames = [f'{src}/{x}' for x in data_dict[dataset][scene]]\n",
    "            print (f\"Got {len(img_fnames)} images\")\n",
    "            feature_dir = f'featureout/{dataset}_{scene}'\n",
    "            if not os.path.isdir(feature_dir):\n",
    "                os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "            if CONFIG.ROTATION_CORRECTION:\n",
    "                rots = exec_rotation_detection(img_fnames, device)\n",
    "            else:\n",
    "                rots = [ 0 for fname in img_fnames ]\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print (f'rotation_detection for {len(img_fnames)} images : {t:.4f} sec')\n",
    "            gc.collect()\n",
    "            \n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            t=time()\n",
    "            index_pairs = get_image_pairs_shortlist(img_fnames,\n",
    "                                  sim_th = 1.0, # should be strict\n",
    "                                  min_pairs = 50, # we select at least min_pairs PER IMAGE with biggest similarity\n",
    "                                  exhaustive_if_less = 50,\n",
    "                                  device=device)\n",
    "            t=time() -t \n",
    "            timings['shortlisting'].append(t)\n",
    "            print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################            \n",
    "            keypoints_timings = wrapper_keypoints(\n",
    "                img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "            )\n",
    "            timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            futures[dataset][scene] = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                dataset, scene, feature_dir, img_dir, timings, data_dict[dataset][scene])\n",
    "                \n",
    "    #############################################################\n",
    "    # reconstruction results\n",
    "    #############################################################            \n",
    "    for dataset in datasets:\n",
    "        for scene in data_dict[dataset]:\n",
    "            # wait to complete COLMAP reconstruction\n",
    "            result = futures[dataset][scene].result()\n",
    "            if result is not None:\n",
    "                out_results[dataset][scene] = result   # get R and t from result\n",
    "    \n",
    "    create_submission(out_results, data_dict)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0e968",
   "metadata": {
    "papermill": {
     "duration": 0.014573,
     "end_time": "2024-05-23T06:52:43.146903",
     "exception": false,
     "start_time": "2024-05-23T06:52:43.132330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d658f1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:52:43.178177Z",
     "iopub.status.busy": "2024-05-23T06:52:43.177772Z",
     "iopub.status.idle": "2024-05-23T06:52:44.166858Z",
     "shell.execute_reply": "2024-05-23T06:52:44.165700Z"
    },
    "papermill": {
     "duration": 1.007697,
     "end_time": "2024-05-23T06:52:44.169357",
     "exception": false,
     "start_time": "2024-05-23T06:52:43.161660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path,dataset,scene,rotation_matrix,translation_vector\r\n",
      "test/church/images/00046.png,church,church,-0.1934582420516795;-0.32234877195183576;-0.926641882176398;0.37943407172792265;0.8464111477256597;-0.37365485975389257;0.9047672041990933;-0.4238861146898715;-0.041435105633770064,5.469910185570828;4.764421349166646;10.621431410645076\r\n",
      "test/church/images/00090.png,church,church,0.9995961947604299;-0.0027834061659106355;-0.02827896869732483;-0.001992337367574268;0.9858766660489172;-0.16746112961545923;0.028345687719342323;0.16744984917994832;0.9854730184014833,-0.01965796999787507;0.046381355732174126;-0.5463681614882365\r\n",
      "test/church/images/00092.png,church,church,0.947253082219023;-0.12045275177538549;-0.2969894489982736;0.14394489470436203;0.9878574070864125;0.05846033315899046;0.28634151902075067;-0.09812684573676766;0.9530895323266548,0.09312470155267524;-0.045058364855617226;-0.550595108861928\r\n",
      "test/church/images/00087.png,church,church,0.8921212461638776;-0.15808494651077395;-0.4232361419227993;0.18372728060342644;0.9827698501464938;0.020191780632425785;0.41275170321281535;-0.09577354190862912;0.9057943807317398,0.7842359888542322;-0.002083805233506131;-0.6100274072432639\r\n",
      "test/church/images/00050.png,church,church,0.9231469515284493;0.17256455098650728;0.343542110412328;-0.20193238996649707;0.9780531466960732;0.051335680772031836;-0.32714372340937486;-0.11676265661903441;0.937733153009488,-2.830346806539392;0.44000524061319585;0.9675941575321951\r\n",
      "test/church/images/00068.png,church,church,0.5741915211801517;-0.207634861149528;-0.7919544566704828;0.5251815999800475;0.835481251704482;0.161726204100354;0.6280831028193807;-0.5087817238126073;0.5887722594238378,4.639410767035592;0.4312225671371951;0.6465356647379165\r\n",
      "test/church/images/00083.png,church,church,0.9357887593056442;-0.1417565151987003;-0.3228071999753471;0.175796901117789;0.9812751577562983;0.07870523697781645;0.30560570595881453;-0.130399981476504;0.9431866184994049,1.3923320280211278;-0.028533225510995228;-0.5893867367327877\r\n",
      "test/church/images/00096.png,church,church,0.9428154557778876;0.08904698275647083;0.32120032877987975;-0.012471078036581755;0.9724027849953408;-0.23297488265027708;-0.3330818046050243;0.21564660580513126;0.9179068868059477,-0.9978950096418929;0.0341782555852912;-0.980981829870245\r\n",
      "test/church/images/00069.png,church,church,0.5865630690968993;-0.20423210649868678;-0.7837301912308442;0.3462569205208406;0.9380244295827158;0.014707633985281965;0.7321542945056395;-0.2799989574643624;0.6209272685716256,4.668092737167107;0.22572955151850796;0.7622427538333476\r\n",
      "test/church/images/00081.png,church,church,0.8905421472810967;-0.1297154616316359;-0.4360144297264313;0.16055753809896567;0.986424452935963;0.034468472690058795;0.4256242014683525;-0.10070103109540507;0.8992793456211199,1.4493598179195466;0.03313041942327589;-0.33826668648472863\r\n",
      "test/church/images/00042.png,church,church,0.7266640093219272;0.1919733663447735;0.6596253816905756;-0.2506883414949056;0.9680518234710593;-0.005569785713167921;-0.6396208040665311;-0.1613130301265599;0.7515739040950477,-4.764572833850006;2.5380720320672108;6.924701917060741\r\n",
      "test/church/images/00018.png,church,church,0.984641972705851;-0.05868351554844323;-0.16442758463471352;0.08277533028157905;0.9861488588898115;0.14373125201951747;0.1537154197948461;-0.1551343711535202;0.9758611051804945,0.22584191360520217;1.2253386155304733;2.862229924547331\r\n",
      "test/church/images/00030.png,church,church,0.9464998130079764;0.07340918384917301;0.31424384751728096;-0.15545250291632612;0.9570703277933403;0.24464445016104017;-0.28279431273085287;-0.2804059189534814;0.917278527658278,-2.757097856296847;0.2470005224582368;0.7534305592589401\r\n",
      "test/church/images/00024.png,church,church,0.811983362690261;0.20938750928901298;0.5448301475395425;-0.22862413174750254;0.9729487440004759;-0.033192588488951597;-0.5370419211727486;-0.09760948981343864;0.8378892303887451,-0.0026308863121863022;0.566576359131124;2.030380304599531\r\n",
      "test/church/images/00032.png,church,church,0.9744293461161043;0.08213080398651235;0.20914583539786394;-0.10500941105326345;0.989356432812848;0.10073168539284627;-0.19864660333866288;-0.12011819132989679;0.9726824492575397,-1.048695261248123;0.5654500928697269;1.614196319801139\r\n",
      "test/church/images/00026.png,church,church,0.9647166395606472;0.13814713043164667;0.2241365113235989;-0.18353215995616362;0.9632216070537225;0.19626533562162582;-0.18877963771720013;-0.2304766930913967;0.9545903531489394,-2.102136215650979;0.6252089318983701;1.079797224675417\r\n",
      "test/church/images/00037.png,church,church,-0.7841415914709771;0.19304397376545693;0.589793174526634;-0.08584189732294525;0.9075105698655508;-0.4111638776039308;-0.6146162487189546;-0.3730396624692136;-0.6950455215563187,-0.563609676550303;0.41352309516257474;3.442653716853926\r\n",
      "test/church/images/00008.png,church,church,0.9999685836539296;0.00015015750063286437;0.007925222891434859;-0.00015375661584344873;0.9999998853361097;0.00045352692364754755;-0.007925153882228645;-0.0004547312309403541;0.9999684920813509,-0.8199490519207552;1.4098343951672445;5.200209774759451\r\n",
      "test/church/images/00035.png,church,church,-0.46645141267618384;0.20831516356830387;0.8596673032283498;-0.34757575267224755;0.8505334289970219;-0.3946947964096923;-0.8133966902850746;-0.48290545532855456;-0.32432721971368417,-0.6111343048946064;1.180074220190174;3.3929952458051647\r\n",
      "test/church/images/00021.png,church,church,0.9703198537096752;0.13531029389833876;0.2004258113666495;-0.15090571491940696;0.9864336203368737;0.06462335393455929;-0.18896255370177778;-0.09295072368859658;0.9775752228162644,-1.7796932441951614;0.7876570920102435;2.010742049417974\r\n",
      "test/church/images/00010.png,church,church,0.9972066655504208;-0.017944281423862205;-0.07250426846739785;0.013169608198821842;0.9977457129559786;-0.0658031435256371;0.07352161316035438;0.06466448052947851;0.9951949946398203,-0.57452061158639;0.9811282886752851;4.886404976406276\r\n",
      "test/church/images/00039.png,church,church,0.5819154267905067;0.24338272176625853;0.7759763442327804;-0.33453555256316836;0.9413377837562479;-0.044375003595843565;-0.7412559612809048;-0.23376917593755492;0.6291991514989498,-1.8557732264229778;0.7957275087229877;1.9956192977987064\r\n",
      "test/church/images/00011.png,church,church,0.9994518212664083;0.012622130794169918;0.030606188613978035;-0.012972764535164124;0.9998521680860959;0.011284917095336949;-0.030459224342959217;-0.011675777821982897;0.9994678143215403,0.2783902398147861;1.1708450629886469;4.228991118396239\r\n",
      "test/church/images/00013.png,church,church,0.9940403797768499;0.020021414040346696;0.10715813712882945;-0.007915260920931998;0.9936508902291185;-0.11222859257530592;-0.10872475347247092;0.11071156817020276;0.9878875830050933,0.6180606048336441;0.6340370037438268;4.332283408491552\r\n",
      "test/church/images/00006.png,church,church,0.9709360302632832;-0.0728284245412444;0.22798957369892386;0.08797238766680936;0.9944925558477072;-0.05696854721419115;-0.22258500431245898;0.07536960224344551;0.9719955446980647,0.2011928889351762;1.0554050238329633;5.332998824829624\r\n",
      "test/church/images/00012.png,church,church,0.9990900358909293;0.007779889162670498;-0.04193534914696985;-0.010092626818637703;0.9984230007946439;-0.05522363957688965;0.04143958333962128;0.05559662587635207;0.9975929912162592,0.1898663657455599;0.5444425914624429;2.818881249128701\r\n",
      "test/church/images/00029.png,church,church,0.9980053452009788;-0.0067861432956487135;0.06276367746910606;0.0056392306783359224;0.9998142012261173;0.018432636923065165;-0.0628771025702645;-0.018041931319898997;0.9978581856589726,-0.9580680789207194;0.4457930625290095;1.9663036652862143\r\n",
      "test/church/images/00001.png,church,church,0.94641993011226;0.10474717622081792;0.30547887809154634;-0.1224315619186914;0.9917000361510848;0.03926258962532666;-0.2988307690524095;-0.07455915350714554;0.9513890392977224,1.2740070988333954;1.666556936803467;5.070425883371407\r\n",
      "test/church/images/00098.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\r\n",
      "test/church/images/00072.png,church,church,0.7487128752600702;-0.17749293538558716;-0.638690291383865;0.3271087505625718;0.9369369532633053;0.12308131830244612;0.5765664712114222;-0.30107375092804534;0.759556252542842,3.2760290345876384;0.3953543538543749;0.4563290242763663\r\n",
      "test/church/images/00066.png,church,church,0.5434657447429658;-0.23427527224925276;-0.8060769697141285;0.3476309341023061;0.9368647350811905;-0.03791044468756476;0.7640665664365452;-0.2596142618844144;0.5905952227034004,4.631919505669864;0.398526664356886;0.9425253045167066\r\n",
      "test/church/images/00104.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\r\n",
      "test/church/images/00058.png,church,church,0.8782786785683763;0.2098391684071499;0.42964413899689435;0.14676033694471546;0.7368797331705252;-0.6599012519630563;-0.4550691884450764;0.6426319181652365;0.6163896912531944,0.4622257125437515;-0.736241363803962;0.7181371728461065\r\n",
      "test/church/images/00059.png,church,church,-0.14600944463077203;-0.26885320592788;-0.9520499964502468;0.5690472374563016;0.7643931387046287;-0.3031309470256012;0.809238211914122;-0.5860214016259024;0.0413815564526947,5.83596800050012;2.630137201676535;4.092510894891802\r\n",
      "test/church/images/00111.png,church,church,0.8651922743943343;0.18075159394090834;0.46772982545074016;-0.26920953843707934;0.954381413193505;0.12916014309864735;-0.42304675006771253;-0.23766568839146582;0.8743834786971658,-3.544117749589701;0.9770671754145903;1.4492294310922749\r\n",
      "test/church/images/00061.png,church,church,0.12062357842571181;0.3297943820485273;0.9363149138495052;-0.42985159850093;0.867542792825512;-0.25019413638753774;-0.8948058759272713;-0.372297150386275;0.24640875840823506,-5.55311244536757;2.274296623282288;3.953457995319264\r\n",
      "test/church/images/00060.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\r\n",
      "test/church/images/00074.png,church,church,0.8552426803365771;-0.16466171748368302;-0.49137203474152985;0.1550640128135466;0.9860477846440452;-0.06053856893527483;0.49448469102780895;-0.024418951531433933;0.8688432569487075,3.3779311833619543;0.16048502916626375;0.20338691096457817\r\n",
      "test/church/images/00102.png,church,church,0.9380448794163129;-0.06847940066827399;-0.3396798137731311;0.022696782795787636;0.9903152810314962;-0.1369689753419711;0.34576966357903044;0.12077340700511287;0.9305144404622754,-0.43648726858172165;0.5364350357751438;2.5048274688880574\r\n",
      "test/church/images/00076.png,church,church,0.46326171652346615;-0.24421229064227423;-0.8519089969609341;0.3076970913797976;0.9458048973748471;-0.10380556853159387;0.8310902971158693;-0.21404077462251983;0.5132986117639979,4.424368636222416;1.180500909729637;3.1758251433183795\r\n",
      "test/church/images/00063.png,church,church,0.8827420101201737;-0.12376735273149764;-0.45326392528727927;0.3942949183601102;0.7197797177062896;0.5713568721333876;0.2555348526608245;-0.6830803762197325;0.6841806330926941,0.173874578821243;0.2666775229369062;-0.07205422817407384\r\n"
     ]
    }
   ],
   "source": [
    "!cat submission.csv"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8143495,
     "sourceId": 71885,
     "sourceType": "competition"
    },
    {
     "datasetId": 2058261,
     "sourceId": 3414836,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3117886,
     "sourceId": 5373920,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 170475544,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 170565695,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 174129945,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175679956,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175684111,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 176463227,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 2663,
     "sourceId": 3736,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2742,
     "sourceId": 3840,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2747,
     "sourceId": 3846,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1097.671486,
   "end_time": "2024-05-23T06:52:46.975139",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-23T06:34:29.303653",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
