{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb3e3a6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-18T11:47:28.638779Z",
     "iopub.status.busy": "2024-04-18T11:47:28.638498Z",
     "iopub.status.idle": "2024-04-18T11:49:18.939807Z",
     "shell.execute_reply": "2024-04-18T11:49:18.938686Z"
    },
    "papermill": {
     "duration": 110.314049,
     "end_time": "2024-04-18T11:49:18.947021",
     "exception": false,
     "start_time": "2024-04-18T11:47:28.632972",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/hf-peft/peft-0.9.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (2.1.2)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (4.39.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (4.66.1)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (0.28.0)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (0.4.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (0.22.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (2024.2.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.9.0) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.9.0) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.9.0) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.9.0) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.9.0) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.9.0) (0.15.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.9.0) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.9.0) (1.3.0)\r\n",
      "Installing collected packages: peft\r\n",
      "Successfully installed peft-0.9.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing /kaggle/input/bitsandbytes/bitsandbytes-0.42.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.11.4)\r\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes==0.42.0) (1.26.4)\r\n",
      "Installing collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.42.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Processing /kaggle/input/transformers-4-39-2/transformers-4.39.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (0.22.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.2) (2024.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.2) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.39.2) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.2) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.2) (2024.2.2)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.39.3\r\n",
      "    Uninstalling transformers-4.39.3:\r\n",
      "      Successfully uninstalled transformers-4.39.3\r\n",
      "Successfully installed transformers-4.39.2\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ../input/hf-peft/peft-0.9.0-py3-none-any.whl\n",
    "%pip install ../input/bitsandbytes/bitsandbytes-0.42.0-py3-none-any.whl\n",
    "# %pip install ../input/sentence-transformers/sentence_transformers-2.5.1-py3-none-any.whl\n",
    "%pip install ../input/transformers-4-39-2/transformers-4.39.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95613f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T11:49:18.960670Z",
     "iopub.status.busy": "2024-04-18T11:49:18.960359Z",
     "iopub.status.idle": "2024-04-18T11:49:25.157984Z",
     "shell.execute_reply": "2024-04-18T11:49:25.157021Z"
    },
    "papermill": {
     "duration": 6.207091,
     "end_time": "2024-04-18T11:49:25.160322",
     "exception": false,
     "start_time": "2024-04-18T11:49:18.953231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e6e00c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T11:49:25.174119Z",
     "iopub.status.busy": "2024-04-18T11:49:25.173685Z",
     "iopub.status.idle": "2024-04-18T11:49:26.136215Z",
     "shell.execute_reply": "2024-04-18T11:49:26.134885Z"
    },
    "papermill": {
     "duration": 0.972503,
     "end_time": "2024-04-18T11:49:26.139127",
     "exception": false,
     "start_time": "2024-04-18T11:49:25.166624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/llm-prompt-recovery/test.csv\")\n",
    "!cp ../input/llm-prompt-recovery/test.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a4ba50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T11:49:26.153700Z",
     "iopub.status.busy": "2024-04-18T11:49:26.153367Z",
     "iopub.status.idle": "2024-04-18T11:49:26.164875Z",
     "shell.execute_reply": "2024-04-18T11:49:26.163875Z"
    },
    "papermill": {
     "duration": 0.021443,
     "end_time": "2024-04-18T11:49:26.166792",
     "exception": false,
     "start_time": "2024-04-18T11:49:26.145349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.py\n",
    "\n",
    "# !cp ../input/recovery-scripts/run.py .\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from peft import PeftModel, PeftConfig\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "# Create the argument parser\n",
    "parser = argparse.ArgumentParser(description=\"\")\n",
    "\n",
    "parser.add_argument(\"--model_path\", type=str, help=\"\")\n",
    "parser.add_argument(\"--peft_path\", type=str, help=\"\", default=\"\")\n",
    "parser.add_argument(\"--model_type\", type=str, help=\"\")\n",
    "parser.add_argument(\"--prime\", type=str, help=\"\", default=\"\")\n",
    "parser.add_argument(\"--magic\", type=str, help=\"\", default=\"\")\n",
    "parser.add_argument(\"--output\", type=str, help=\"\")\n",
    "parser.add_argument(\"--max_len\", type=int, help=\"\")\n",
    "parser.add_argument(\"--min_output_len\", type=int, help=\"\", default=2)\n",
    "parser.add_argument(\"--max_output_len\", type=int, help=\"\", default=100)\n",
    "parser.add_argument('--quantize', action='store_true')\n",
    "parser.add_argument('--do_sample', action='store_true')\n",
    "parser.add_argument('--test_path', type=str)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "test = pd.read_csv(args.test_path)\n",
    "magic = \"Transform the following text in a more vivid and descriptive way, while maintaining the original meaning and tone.\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "lucrarea = args.magic\n",
    "def predict_gemma(model, tokenizer, test, bad_words_ids=None):\n",
    "    if bad_words_ids is not None and len(bad_words_ids) == 0:\n",
    "        bad_words_ids = None\n",
    "    predictions = []\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(test.iterrows(), total=len(test)):\n",
    "            if row.original_text == row.rewritten_text:\n",
    "                predictions.append(\"Correct grammatical errors in this text.\")\n",
    "                continue\n",
    "            ot = \" \".join(str(row.original_text).split(\" \")[:args.max_len])\n",
    "            rt = \" \".join(str(row.rewritten_text).split(\" \")[:args.max_len])\n",
    "            prompt = f\"Find the orginal prompt that transformed original text to new text.\\n\\nOriginal text: {ot}\\n====\\nNew text: {rt}\"\n",
    "            conversation = [{\"role\": \"user\", \"content\": prompt }]\n",
    "            prime = args.prime\n",
    "            prompt = tokenizer.apply_chat_template(conversation, tokenize=False) + f\"<start_of_turn>model\\n{prime}\"\n",
    "            input_ids = tokenizer.encode(prompt, add_special_tokens=False, truncation=True, max_length=1536,padding=False,return_tensors=\"pt\")\n",
    "            x = model.generate(input_ids=input_ids.to(model.device), eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id, max_new_tokens=128, do_sample=args.do_sample, early_stopping=True, num_beams=1, bad_words_ids=bad_words_ids)\n",
    "            try:\n",
    "                x = tokenizer.decode(x[0]).split(\"<start_of_turn>model\")[1].split(\"<end_of_turn>\")[0].replace(\"<end_of_turn>\\n<eos>\",\"\").replace(\"<end_of_turn>\",\"\").replace(\"<start_of_turn>\",\"\").replace(\"<eos>\",\"\").replace(\"<bos>\",\"\").strip().replace('\"','').strip()\n",
    "                x = x.replace(\"Can you make this\",\"Make this\").replace(\"?\",\".\").replace(\"Revise\",\"Rewrite\")\n",
    "                x = x.split(\":\",1)[-1].strip()\n",
    "                if \"useruser\" in x:\n",
    "                    x = x.replace(\"user\",\"\")\n",
    "                if x[-1].isalnum():\n",
    "                    x += \".\"\n",
    "                else:\n",
    "                    x = x[:-1]+\".\"\n",
    "                x+= lucrarea\n",
    "                if len(x.split()) < args.max_output_len and len(x.split()) > args.min_output_len and (\"\\n\" not in x):\n",
    "                    print(x)\n",
    "                    predictions.append(x)\n",
    "                else:\n",
    "                    predictions.append(magic)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                predictions.append(magic)\n",
    "    return predictions\n",
    "\n",
    "def predict_mistral(model, tokenizer, test,prime=\"\"):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(test.iterrows(), total=len(test)):\n",
    "            ot = \" \".join(str(row.original_text).split(\" \")[:args.max_len])\n",
    "            rt = \" \".join(str(row.rewritten_text).split(\" \")[:args.max_len])\n",
    "            prompt = f'''\n",
    "Please find the prompt that was given to you to transform **original_text** to **new_text**. One clue is the prompt itself was short and concise.\n",
    "Answer in thist format: \"It's likely that the prompt that transformed original_text to new_text was: <the prompt>\" and don't add anything else.\n",
    "\n",
    "**original_text**:\n",
    "{ot}\n",
    "\n",
    "**new_text**:\n",
    "{rt}\n",
    "'''\n",
    "            conversation = [{\"role\": \"user\", \"content\": prompt }]\n",
    "            prompt = tokenizer.apply_chat_template(conversation, tokenize=False)+prime\n",
    "            input_ids = tokenizer.encode(prompt, add_special_tokens=False, truncation=True, max_length=1536,padding=False,return_tensors=\"pt\")\n",
    "            x = model.generate(input_ids=input_ids.to(model.device), eos_token_id=[13, tokenizer.eos_token_id], pad_token_id=tokenizer.eos_token_id, max_new_tokens=32, do_sample=args.do_sample, early_stopping=True, num_beams=1)\n",
    "            try:\n",
    "                x = tokenizer.decode(x[0]).split(\"[/INST]\")[-1].replace(\"</s>\",\"\").strip().split(\"\\n\",1)[0]\n",
    "                x = x.replace(\"Can you make this\",\"Make this\").replace(\"?\",\".\")\n",
    "                # print(x.split(\":\",1)[0])\n",
    "                x = x.split(\":\",1)[-1].strip()\n",
    "                if x[-1].isalnum():\n",
    "                    x += \".\"\n",
    "                else:\n",
    "                    x = x[:-1]+\".\"\n",
    "                x += lucrarea\n",
    "                if len(x.split()) < 50 and len(x.split()) > 2 and (\"\\n\" not in x):\n",
    "                    predictions.append(x)\n",
    "                else:\n",
    "                    predictions.append(magic)\n",
    "                print(predictions[-1])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                predictions.append(magic)\n",
    "    return predictions\n",
    "model_name = args.model_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "banned_ids = None\n",
    "    \n",
    "if args.quantize:\n",
    "    print(\"Use 4bit quantization\")\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 quantization_config=quantization_config,\n",
    "                                                 device_map=\"auto\",\n",
    "                                                 torch_dtype=torch.bfloat16)\n",
    "    if args.peft_path != \"\":\n",
    "        print(\"Use peft\")\n",
    "        model = PeftModel.from_pretrained(model,\n",
    "                                    args.peft_path,\n",
    "                                    quantization_config=quantization_config,\n",
    "                                    torch_dtype=torch.bfloat16,\n",
    "                                    device_map=\"auto\")\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 device_map=\"auto\",\n",
    "                                                 torch_dtype=torch.bfloat16)\n",
    "    if args.peft_path != \"\":\n",
    "        print(\"Use peft\")\n",
    "        model = PeftModel.from_pretrained(model,\n",
    "                                args.peft_path,\n",
    "                                torch_dtype=torch.bfloat16,\n",
    "                                device_map=\"auto\")\n",
    "        \n",
    "# model = model.merge_and_unload()\n",
    "model.eval()\n",
    "# print(model)\n",
    "if args.model_type == \"gemma\":\n",
    "    preds = predict_gemma(model, tokenizer, test, bad_words_ids=banned_ids)\n",
    "elif args.model_type == \"mistral\":\n",
    "    preds = predict_mistral(model, tokenizer, test, prime=args.prime)\n",
    "\n",
    "json.dump(preds, open(args.output,\"wt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6ce577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T11:49:26.179921Z",
     "iopub.status.busy": "2024-04-18T11:49:26.179668Z",
     "iopub.status.idle": "2024-04-18T11:52:22.031269Z",
     "shell.execute_reply": "2024-04-18T11:52:22.030150Z"
    },
    "papermill": {
     "duration": 175.860884,
     "end_time": "2024-04-18T11:52:22.033789",
     "exception": false,
     "start_time": "2024-04-18T11:49:26.172905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 4bit quantization\r\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\r\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [02:29<00:00, 37.36s/it]\r\n",
      "Use peft\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n",
      "  warnings.warn(\r\n",
      "2024-04-18 11:52:09.832046: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-04-18 11:52:09.832176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-04-18 11:52:09.962153: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Improve this text using the writing style of a sea shanty.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.41s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!python run.py --model_path /kaggle/input/gemma/transformers/7b-it/3/ --peft_path \"../input/gemma-7b-orca-68500/\" --model_type \"gemma\" --output \"pred2.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"General prompt: Improve this text using the writing style\"\n",
    "preds = json.load(open(\"pred2.json\"))\n",
    "# preds = [\"Please improve this text using the writing style with maintaining the original meaning but altering the tone.\",]*len(test)\n",
    "def remove_pp(x):\n",
    "    for w in x.split()[1:]:\n",
    "        if w.istitle():\n",
    "            return \"Please improve this text using the writing style.\"\n",
    "    return x\n",
    "preds = [remove_pp(x)[:-1]+\" with maintaining the original meaning but altering the tone.\" for x in preds]\n",
    "json.dump(preds, open(\"pred2.json\",\"wt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7099818f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T11:52:22.051222Z",
     "iopub.status.busy": "2024-04-18T11:52:22.050911Z",
     "iopub.status.idle": "2024-04-18T11:54:46.827625Z",
     "shell.execute_reply": "2024-04-18T11:54:46.826458Z"
    },
    "papermill": {
     "duration": 144.787329,
     "end_time": "2024-04-18T11:54:46.829990",
     "exception": false,
     "start_time": "2024-04-18T11:52:22.042661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 4bit quantization\r\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [02:06<00:00, 63.23s/it]\r\n",
      "Use peft\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\r\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\r\n",
      "\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n",
      "  warnings.warn(\r\n",
      "2024-04-18 11:54:41.687294: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-04-18 11:54:41.687356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-04-18 11:54:41.688772: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Rewrite the following text into a shanty.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.60s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!python run.py --model_path /kaggle/input/mistral/pytorch/7b-v0.1-hf/1 --peft_path \"../input/mistral-og-600\" --model_type \"mistral\" --output \"pred0.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Rewrite\" --magic \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00af0b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T11:54:46.847539Z",
     "iopub.status.busy": "2024-04-18T11:54:46.847202Z",
     "iopub.status.idle": "2024-04-18T11:56:41.751575Z",
     "shell.execute_reply": "2024-04-18T11:56:41.750319Z"
    },
    "papermill": {
     "duration": 114.916056,
     "end_time": "2024-04-18T11:56:41.754150",
     "exception": false,
     "start_time": "2024-04-18T11:54:46.838094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 4bit quantization\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [01:36<00:00, 32.29s/it]\r\n",
      "Use peft\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n",
      "  warnings.warn(\r\n",
      "2024-04-18 11:56:36.464770: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-04-18 11:56:36.464834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-04-18 11:56:36.466364: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Make this text into a shanty about a code competition.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.60s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!python run.py --model_path ../input/mistral-7b-it-v02/ --peft_path \"../input/mistral-gemmaonly\" --model_type \"mistral\" --output \"pred3.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Make this text\" --magic \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a921360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T11:56:41.774220Z",
     "iopub.status.busy": "2024-04-18T11:56:41.773820Z",
     "iopub.status.idle": "2024-04-18T11:58:53.932191Z",
     "shell.execute_reply": "2024-04-18T11:58:53.930904Z"
    },
    "papermill": {
     "duration": 132.170996,
     "end_time": "2024-04-18T11:58:53.934684",
     "exception": false,
     "start_time": "2024-04-18T11:56:41.763688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 4bit quantization\r\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\r\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:54<00:00, 28.62s/it]\r\n",
      "Use peft\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n",
      "  warnings.warn(\r\n",
      "2024-04-18 11:58:48.757202: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-04-18 11:58:48.757261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-04-18 11:58:48.758703: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Alter this into a sailor's shanty.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:05<00:00,  5.25s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!python run.py --model_path  /kaggle/input/gemma/transformers/7b-it/3 --peft_path \"../input/gemma-7b-orca-external/\" --model_type \"gemma\" --output \"pred1.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"General prompt: Alter\" --magic \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f24a2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T11:58:53.956431Z",
     "iopub.status.busy": "2024-04-18T11:58:53.956066Z",
     "iopub.status.idle": "2024-04-18T11:58:53.963717Z",
     "shell.execute_reply": "2024-04-18T11:58:53.962886Z"
    },
    "papermill": {
     "duration": 0.020579,
     "end_time": "2024-04-18T11:58:53.965588",
     "exception": false,
     "start_time": "2024-04-18T11:58:53.945009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Rewrite the following text into a shanty. Alter this into a sailor's shanty. Improve this text using the writing style of a sea shanty with maintaining the original meaning but altering the tone. Make this text into a shanty about a code competition.\"]\n"
     ]
    }
   ],
   "source": [
    "fns = [\"pred0.json\",\"pred1.json\", \"pred2.json\", \"pred3.json\"]\n",
    "preds = [json.load(open(x)) for x in fns]\n",
    "preds = [' '.join(list(x)) for x in zip(*preds)]\n",
    "print(preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "159a5005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T11:58:53.986003Z",
     "iopub.status.busy": "2024-04-18T11:58:53.985733Z",
     "iopub.status.idle": "2024-04-18T11:58:54.001753Z",
     "shell.execute_reply": "2024-04-18T11:58:54.001032Z"
    },
    "papermill": {
     "duration": 0.028375,
     "end_time": "2024-04-18T11:58:54.003656",
     "exception": false,
     "start_time": "2024-04-18T11:58:53.975281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "magic = \" 'it 's ' something Think A Human Plucrarealucrarealucrarealucrarealucrarealucrarealucrarealucrarea\"\n",
    "# magic = \"\"\n",
    "predictions = [x+magic for x in preds]\n",
    "\n",
    "sub = pd.read_csv(\"../input/llm-prompt-recovery/sample_submission.csv\")\n",
    "sub['rewrite_prompt'] = predictions\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7806901,
     "sourceId": 67121,
     "sourceType": "competition"
    },
    {
     "datasetId": 4524347,
     "sourceId": 7740846,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4524539,
     "sourceId": 7741042,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4634330,
     "sourceId": 7893017,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4645060,
     "sourceId": 7907523,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4663472,
     "sourceId": 7933949,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4689677,
     "sourceId": 7970245,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4703298,
     "sourceId": 7989504,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4746640,
     "sourceId": 8049229,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 3899,
     "sourceId": 5111,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8332,
     "sourceId": 28808,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 688.987641,
   "end_time": "2024-04-18T11:58:55.035056",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-18T11:47:26.047415",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
