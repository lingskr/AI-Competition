{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bueg5dvfuHQa",
        "outputId": "f6a012b8-33fb-4be8-87e3-a31a4032314f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK5F1XRmGwuA"
      },
      "outputs": [],
      "source": [
        "!unzip -q '/content/drive/MyDrive/predict student games/data/predict-student-performance-from-game-play.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohj4IwUkHCK-"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import pickle\n",
        "import polars as pl\n",
        "\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import pyarrow as pa\n",
        "\n",
        "# !pip uninstall catboost\n",
        "# !pip install -q catboost==1.1.1\n",
        "# from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import GroupKFold, KFold, train_test_split,StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeR4vDiKazvE"
      },
      "outputs": [],
      "source": [
        "CATS = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\n",
        "\n",
        "event_name_feature = ['cutscene_click', 'person_click', 'navigate_click',\n",
        "       'observation_click', 'notification_click', 'object_click',\n",
        "       'object_hover', 'map_hover', 'map_click', 'checkpoint',\n",
        "       'notebook_click']\n",
        "text_lists = ['tunic.historicalsociety.cage.confrontation', 'tunic.wildlife.center.crane_ranger.crane', 'tunic.historicalsociety.frontdesk.archivist.newspaper', 'tunic.historicalsociety.entry.groupconvo', 'tunic.wildlife.center.wells.nodeer', 'tunic.historicalsociety.frontdesk.archivist.have_glass', 'tunic.drycleaner.frontdesk.worker.hub', 'tunic.historicalsociety.closet_dirty.gramps.news', 'tunic.humanecology.frontdesk.worker.intro', 'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation', 'tunic.historicalsociety.basement.seescratches', 'tunic.historicalsociety.collection.cs', 'tunic.flaghouse.entry.flag_girl.hello', 'tunic.historicalsociety.collection.gramps.found', 'tunic.historicalsociety.basement.ch3start', 'tunic.historicalsociety.entry.groupconvo_flag', 'tunic.library.frontdesk.worker.hello', 'tunic.library.frontdesk.worker.wells', 'tunic.historicalsociety.collection_flag.gramps.flag', 'tunic.historicalsociety.basement.savedteddy', 'tunic.library.frontdesk.worker.nelson', 'tunic.wildlife.center.expert.removed_cup', 'tunic.library.frontdesk.worker.flag', 'tunic.historicalsociety.frontdesk.archivist.hello', 'tunic.historicalsociety.closet.gramps.intro_0_cs_0', 'tunic.historicalsociety.entry.boss.flag', 'tunic.flaghouse.entry.flag_girl.symbol', 'tunic.historicalsociety.closet_dirty.trigger_scarf', 'tunic.drycleaner.frontdesk.worker.done', 'tunic.historicalsociety.closet_dirty.what_happened', 'tunic.wildlife.center.wells.animals', 'tunic.historicalsociety.closet.teddy.intro_0_cs_0', 'tunic.historicalsociety.cage.glasses.afterteddy', 'tunic.historicalsociety.cage.teddy.trapped', 'tunic.historicalsociety.cage.unlockdoor', 'tunic.historicalsociety.stacks.journals.pic_2.bingo', 'tunic.historicalsociety.entry.wells.flag', 'tunic.humanecology.frontdesk.worker.badger', 'tunic.historicalsociety.stacks.journals_flag.pic_0.bingo', 'tunic.historicalsociety.closet.intro', 'tunic.historicalsociety.closet.retirement_letter.hub', 'tunic.historicalsociety.entry.directory.closeup.archivist', 'tunic.historicalsociety.collection.tunic.slip', 'tunic.kohlcenter.halloffame.plaque.face.date', 'tunic.historicalsociety.closet_dirty.trigger_coffee', 'tunic.drycleaner.frontdesk.logbook.page.bingo', 'tunic.library.microfiche.reader.paper2.bingo', 'tunic.kohlcenter.halloffame.togrampa', 'tunic.capitol_2.hall.boss.haveyougotit', 'tunic.wildlife.center.wells.nodeer_recap', 'tunic.historicalsociety.cage.glasses.beforeteddy', 'tunic.historicalsociety.closet_dirty.gramps.helpclean', 'tunic.wildlife.center.expert.recap', 'tunic.historicalsociety.frontdesk.archivist.have_glass_recap', 'tunic.historicalsociety.stacks.journals_flag.pic_1.bingo', 'tunic.historicalsociety.cage.lockeddoor', 'tunic.historicalsociety.stacks.journals_flag.pic_2.bingo', 'tunic.historicalsociety.collection.gramps.lost', 'tunic.historicalsociety.closet.notebook', 'tunic.historicalsociety.frontdesk.magnify', 'tunic.humanecology.frontdesk.businesscards.card_bingo.bingo', 'tunic.wildlife.center.remove_cup', 'tunic.library.frontdesk.wellsbadge.hub', 'tunic.wildlife.center.tracks.hub.deer', 'tunic.historicalsociety.frontdesk.key', 'tunic.library.microfiche.reader_flag.paper2.bingo', 'tunic.flaghouse.entry.colorbook', 'tunic.wildlife.center.coffee', 'tunic.capitol_1.hall.boss.haveyougotit', 'tunic.historicalsociety.basement.janitor', 'tunic.historicalsociety.collection_flag.gramps.recap', 'tunic.wildlife.center.wells.animals2', 'tunic.flaghouse.entry.flag_girl.symbol_recap', 'tunic.historicalsociety.closet_dirty.photo', 'tunic.historicalsociety.stacks.outtolunch', 'tunic.library.frontdesk.worker.wells_recap', 'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap', 'tunic.capitol_0.hall.boss.talktogramps', 'tunic.historicalsociety.closet.photo', 'tunic.historicalsociety.collection.tunic', 'tunic.historicalsociety.closet.teddy.intro_0_cs_5', 'tunic.historicalsociety.closet_dirty.gramps.archivist', 'tunic.historicalsociety.closet_dirty.door_block_talk', 'tunic.historicalsociety.entry.boss.flag_recap', 'tunic.historicalsociety.frontdesk.archivist.need_glass_0', 'tunic.historicalsociety.entry.wells.talktogramps', 'tunic.historicalsociety.frontdesk.block_magnify', 'tunic.historicalsociety.frontdesk.archivist.foundtheodora', 'tunic.historicalsociety.closet_dirty.gramps.nothing', 'tunic.historicalsociety.closet_dirty.door_block_clean', 'tunic.capitol_1.hall.boss.writeitup', 'tunic.library.frontdesk.worker.nelson_recap', 'tunic.library.frontdesk.worker.hello_short', 'tunic.historicalsociety.stacks.block', 'tunic.historicalsociety.frontdesk.archivist.need_glass_1', 'tunic.historicalsociety.entry.boss.talktogramps', 'tunic.historicalsociety.frontdesk.archivist.newspaper_recap', 'tunic.historicalsociety.entry.wells.flag_recap', 'tunic.drycleaner.frontdesk.worker.done2', 'tunic.library.frontdesk.worker.flag_recap', 'tunic.humanecology.frontdesk.block_0', 'tunic.library.frontdesk.worker.preflag', 'tunic.historicalsociety.basement.gramps.seeyalater', 'tunic.flaghouse.entry.flag_girl.hello_recap', 'tunic.historicalsociety.closet.doorblock', 'tunic.drycleaner.frontdesk.worker.takealook', 'tunic.historicalsociety.basement.gramps.whatdo', 'tunic.library.frontdesk.worker.droppedbadge', 'tunic.historicalsociety.entry.block_tomap2', 'tunic.library.frontdesk.block_nelson', 'tunic.library.microfiche.block_0', 'tunic.historicalsociety.entry.block_tocollection', 'tunic.historicalsociety.entry.block_tomap1', 'tunic.historicalsociety.collection.gramps.look_0', 'tunic.library.frontdesk.block_badge', 'tunic.historicalsociety.cage.need_glasses', 'tunic.library.frontdesk.block_badge_2', 'tunic.kohlcenter.halloffame.block_0', 'tunic.capitol_0.hall.chap1_finale_c', 'tunic.capitol_1.hall.chap2_finale_c', 'tunic.capitol_2.hall.chap4_finale_c', 'tunic.wildlife.center.fox.concern', 'tunic.drycleaner.frontdesk.block_0', 'tunic.historicalsociety.entry.gramps.hub', 'tunic.humanecology.frontdesk.block_1', 'tunic.drycleaner.frontdesk.block_1']\n",
        "room_lists = ['tunic.historicalsociety.entry', 'tunic.wildlife.center', 'tunic.historicalsociety.cage', 'tunic.library.frontdesk', 'tunic.historicalsociety.frontdesk', 'tunic.historicalsociety.stacks', 'tunic.historicalsociety.closet_dirty', 'tunic.humanecology.frontdesk', 'tunic.historicalsociety.basement', 'tunic.kohlcenter.halloffame', 'tunic.library.microfiche', 'tunic.drycleaner.frontdesk', 'tunic.historicalsociety.collection', 'tunic.historicalsociety.closet', 'tunic.flaghouse.entry', 'tunic.historicalsociety.collection_flag', 'tunic.capitol_1.hall', 'tunic.capitol_0.hall', 'tunic.capitol_2.hall']\n",
        "fqid_lists = ['worker', 'archivist', 'gramps', 'wells', 'toentry', 'confrontation', 'crane_ranger', 'groupconvo', 'flag_girl', 'tomap', 'tostacks', 'tobasement', 'archivist_glasses', 'boss', 'journals', 'seescratches', 'groupconvo_flag', 'cs', 'teddy', 'expert', 'businesscards', 'ch3start', 'tunic.historicalsociety', 'tofrontdesk', 'savedteddy', 'plaque', 'glasses', 'tunic.drycleaner', 'reader_flag', 'tunic.library', 'tracks', 'tunic.capitol_2', 'trigger_scarf', 'reader', 'directory', 'tunic.capitol_1', 'journals.pic_0.next', 'unlockdoor', 'tunic', 'what_happened', 'tunic.kohlcenter', 'tunic.humanecology', 'colorbook', 'logbook', 'businesscards.card_0.next', 'journals.hub.topics', 'logbook.page.bingo', 'journals.pic_1.next', 'journals_flag', 'reader.paper0.next', 'tracks.hub.deer', 'reader_flag.paper0.next', 'trigger_coffee', 'wellsbadge', 'journals.pic_2.next', 'tomicrofiche', 'journals_flag.pic_0.bingo', 'plaque.face.date', 'notebook', 'tocloset_dirty', 'businesscards.card_bingo.bingo', 'businesscards.card_1.next', 'tunic.wildlife', 'tunic.hub.slip', 'tocage', 'journals.pic_2.bingo', 'tocollectionflag', 'tocollection', 'chap4_finale_c', 'chap2_finale_c', 'lockeddoor', 'journals_flag.hub.topics', 'tunic.capitol_0', 'reader_flag.paper2.bingo', 'photo', 'tunic.flaghouse', 'reader.paper1.next', 'directory.closeup.archivist', 'intro', 'businesscards.card_bingo.next', 'reader.paper2.bingo', 'retirement_letter', 'remove_cup', 'journals_flag.pic_0.next', 'magnify', 'coffee', 'key', 'togrampa', 'reader_flag.paper1.next', 'janitor', 'tohallway', 'chap1_finale', 'report', 'outtolunch', 'journals_flag.hub.topics_old', 'journals_flag.pic_1.next', 'reader.paper2.next', 'chap1_finale_c', 'reader_flag.paper2.next', 'door_block_talk', 'journals_flag.pic_1.bingo', 'journals_flag.pic_2.next', 'journals_flag.pic_2.bingo', 'block_magnify', 'reader.paper0.prev', 'block', 'reader_flag.paper0.prev', 'block_0', 'door_block_clean', 'reader.paper2.prev', 'reader.paper1.prev', 'doorblock', 'tocloset', 'reader_flag.paper2.prev', 'reader_flag.paper1.prev', 'block_tomap2', 'journals_flag.pic_0_old.next', 'journals_flag.pic_1_old.next', 'block_tocollection', 'block_nelson', 'journals_flag.pic_2_old.next', 'block_tomap1', 'block_badge', 'need_glasses', 'block_badge_2', 'fox', 'block_1']\n",
        "DIALOGS = ['that', 'this', 'it', 'you','find','found','Found','notebook','Wells','wells','help','need', 'Oh','Ooh','Jo', 'flag', 'can','and','is','the','to']\n",
        "\n",
        "name_feature = ['basic', 'undefined', 'close', 'open', 'prev', 'next']\n",
        "LEVELS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
        "level_groups = [\"0-4\", \"5-12\", \"13-22\"]\n",
        "\n",
        "NUMS = [\n",
        "        'page',\n",
        "        'room_coor_x',\n",
        "        'room_coor_y',\n",
        "        'screen_coor_x',\n",
        "        'screen_coor_y',\n",
        "        'hover_duration',\n",
        "        'elapsed_time_diff']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u2I3Mmfa24Y"
      },
      "outputs": [],
      "source": [
        "columns = [\n",
        "    pl.col(\"page\").cast(pl.Float32),\n",
        "    (\n",
        "        (pl.col(\"elapsed_time\") - pl.col(\"elapsed_time\").shift(1))\n",
        "        .fill_null(0)\n",
        "        .clip(0, 1e9)\n",
        "        .over([\"session_id\", \"level\"])\n",
        "        .alias(\"elapsed_time_diff\")\n",
        "    ),\n",
        "    (\n",
        "        (pl.col(\"screen_coor_x\") - pl.col(\"screen_coor_x\").shift(1))\n",
        "        .abs()\n",
        "        .over([\"session_id\", \"level\"])\n",
        "    ),\n",
        "    (\n",
        "        (pl.col(\"screen_coor_y\") - pl.col(\"screen_coor_y\").shift(1))\n",
        "        .abs()\n",
        "        .over([\"session_id\", \"level\"])\n",
        "    ),\n",
        "    pl.col(\"fqid\").fill_null(\"fqid_None\"),\n",
        "    pl.col(\"text_fqid\").fill_null(\"text_fqid_None\")\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYkDZTVGkAQW"
      },
      "outputs": [],
      "source": [
        "def feature_engineer(x, grp, use_extra, feature_suffix):\n",
        "    aggs = [\n",
        "        pl.col(\"index\").count().alias(f\"session_number_{feature_suffix}\"),\n",
        "\n",
        "        *[pl.col('index').filter(pl.col('text').str.contains(c)).count().alias(f'word_{c}') for c in DIALOGS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).mean().alias(f'word_mean_{c}') for c in DIALOGS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).std().alias(f'word_std_{c}') for c in DIALOGS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).max().alias(f'word_max_{c}') for c in DIALOGS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).sum().alias(f'word_sum_{c}') for c in DIALOGS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c))).median().alias(f'word_median_{c}') for c in DIALOGS],\n",
        "\n",
        "        *[pl.col(c).drop_nulls().n_unique().alias(f\"{c}_unique_{feature_suffix}\") for c in CATS],\n",
        "\n",
        "        *[pl.col(c).mean().alias(f\"{c}_mean_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).std().alias(f\"{c}_std_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).min().alias(f\"{c}_min_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).max().alias(f\"{c}_max_{feature_suffix}\") for c in NUMS],\n",
        "        *[pl.col(c).median().alias(f\"{c}_median_{feature_suffix}\") for c in NUMS],\n",
        "\n",
        "        *[pl.col(\"fqid\").filter(pl.col(\"fqid\") == c).count().alias(f\"{c}_fqid_counts{feature_suffix}\")\n",
        "          for c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
        "          c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for\n",
        "          c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
        "          c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for\n",
        "          c in fqid_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
        "          c in fqid_lists],\n",
        "\n",
        "        *[pl.col(\"text_fqid\").filter(pl.col(\"text_fqid\") == c).count().alias(f\"{c}_text_fqid_counts{feature_suffix}\") for\n",
        "          c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
        "          c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for\n",
        "          c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
        "          c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for\n",
        "          c in text_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
        "          c in text_lists],\n",
        "\n",
        "        *[pl.col(\"room_fqid\").filter(pl.col(\"room_fqid\") == c).count().alias(f\"{c}_room_fqid_counts{feature_suffix}\")\n",
        "          for c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
        "          c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for\n",
        "          c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
        "          c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for\n",
        "          c in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
        "          c in room_lists],\n",
        "\n",
        "        *[pl.col(\"event_name\").filter(pl.col(\"event_name\") == c).count().alias(f\"{c}_event_name_counts{feature_suffix}\")\n",
        "          for c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\")for\n",
        "          c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for\n",
        "          c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
        "          c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for\n",
        "          c in event_name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for\n",
        "          c in event_name_feature],\n",
        "\n",
        "        *[pl.col(\"name\").filter(pl.col(\"name\") == c).count().alias(f\"{c}_name_counts{feature_suffix}\") for c in\n",
        "          name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in\n",
        "          name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in\n",
        "          name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in\n",
        "          name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for c in\n",
        "          name_feature],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in\n",
        "          name_feature],\n",
        "\n",
        "        *[pl.col(\"level\").filter(pl.col(\"level\") == c).count().alias(f\"{c}_LEVEL_count{feature_suffix}\") for c in LEVELS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in\n",
        "          LEVELS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in\n",
        "          LEVELS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in\n",
        "          LEVELS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for c in\n",
        "          LEVELS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in\n",
        "          LEVELS],\n",
        "\n",
        "        *[pl.col(\"level_group\").filter(pl.col(\"level_group\") == c).count().alias(f\"{c}_LEVEL_group_count{feature_suffix}\") for c in\n",
        "          level_groups],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in\n",
        "          level_groups],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in\n",
        "          level_groups],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in\n",
        "          level_groups],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for c in\n",
        "          level_groups],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in\n",
        "          level_groups],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter((pl.col(\"event_name\")==c)&(pl.col(\"name\")==d)&(pl.col(\"room_fqid\")==e)).sum().alias(f\"{c}_{d}_{e}_time_sum\") for c in event_name_feature for d in name_feature for e in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter((pl.col(\"event_name\")==c)&(pl.col(\"name\")==d)&(pl.col(\"level\")==e)).sum().alias(f\"{c}_{d}_{e}_time_sum\") for c in event_name_feature for d in name_feature for e in LEVELS],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(d))&(pl.col(\"room_fqid\")==e)).sum().alias(f\"{d}_{e}_time_sum\") for d in DIALOGS for e in room_lists],\n",
        "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(d))&(pl.col(\"text_fqid\")==e)).sum().alias(f\"{d}_{e}_time_sum\") for d in DIALOGS for e in text_lists]\n",
        "    ]\n",
        "\n",
        "    df = x.groupby(['session_id'], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
        "\n",
        "    if use_extra:\n",
        "        if grp == '5-12':\n",
        "            aggs = [\n",
        "                pl.col(\"elapsed_time\").filter((pl.col(\"text\")==\"Here's the log book.\")\n",
        "                                              |(pl.col(\"fqid\")=='logbook.page.bingo'))\n",
        "                    .apply(lambda s: s.max()-s.min() if s.len() > 0 else 0).alias(\"logbook_bingo_duration\"),\n",
        "                pl.col(\"index\").filter(\n",
        "                    (pl.col(\"text\") == \"Here's the log book.\") | (pl.col(\"fqid\") == 'logbook.page.bingo')).apply(\n",
        "                    lambda s: s.max() - s.min() if s.len() > 0 else 0 ).alias(\"logbook_bingo_indexCount\"),\n",
        "                pl.col(\"elapsed_time\").filter(\n",
        "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader')) | (\n",
        "                                pl.col(\"fqid\") == \"reader.paper2.bingo\")).apply(lambda s: s.max() - s.min() if s.len() > 0 else 0 ).alias(\n",
        "                    \"reader_bingo_duration\"),\n",
        "                pl.col(\"index\").filter(((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader')) | (\n",
        "                            pl.col(\"fqid\") == \"reader.paper2.bingo\")).apply(lambda s: s.max() - s.min() if s.len() > 0 else 0 ).alias(\n",
        "                    \"reader_bingo_indexCount\"),\n",
        "                pl.col(\"elapsed_time\").filter(\n",
        "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals')) | (\n",
        "                                pl.col(\"fqid\") == \"journals.pic_2.bingo\")).apply(lambda s: s.max() - s.min() if s.len() > 0 else 0 ).alias(\n",
        "                    \"journals_bingo_duration\"),\n",
        "                pl.col(\"index\").filter(((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals')) | (\n",
        "                            pl.col(\"fqid\") == \"journals.pic_2.bingo\")).apply(lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\n",
        "                    \"journals_bingo_indexCount\"),\n",
        "            ]\n",
        "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
        "            df = df.join(tmp, on=\"session_id\", how='left')\n",
        "\n",
        "        if grp == '13-22':\n",
        "            aggs = [\n",
        "                pl.col(\"elapsed_time\").filter(\n",
        "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader_flag')) | (\n",
        "                                pl.col(\"text_fqid\") == \"tunic.library.microfiche.reader_flag.paper2.bingo\")).apply(\n",
        "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"reader_flag_duration\"),\n",
        "                pl.col(\"index\").filter(\n",
        "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader_flag')) | (\n",
        "                                pl.col(\"fqid\") == \"tunic.library.microfiche.reader_flag.paper2.bingo\")).apply(\n",
        "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"reader_flag_indexCount\"),\n",
        "                pl.col(\"elapsed_time\").filter(\n",
        "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals_flag')) | (\n",
        "                                pl.col(\"fqid\") == \"journals_flag.pic_0.bingo\")).apply(\n",
        "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"journalsFlag_bingo_duration\"),\n",
        "                pl.col(\"index\").filter(\n",
        "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals_flag')) | (\n",
        "                                pl.col(\"fqid\") == \"journals_flag.pic_0.bingo\")).apply(\n",
        "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"journalsFlag_bingo_indexCount\")\n",
        "            ]\n",
        "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
        "            df = df.join(tmp, on=\"session_id\", how='left')\n",
        "\n",
        "    return df.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcpJbTLDZfW8"
      },
      "outputs": [],
      "source": [
        "def time_feature(train):\n",
        "    train[\"year\"] = train[\"session_id\"].apply(lambda x: int(str(x)[:2])).astype(np.uint8)\n",
        "    train[\"month\"] = train[\"session_id\"].apply(lambda x: int(str(x)[2:4])+1).astype(np.uint8)\n",
        "    train[\"day\"] = train[\"session_id\"].apply(lambda x: int(str(x)[4:6])).astype(np.uint8)\n",
        "    train[\"hour\"] = train[\"session_id\"].apply(lambda x: int(str(x)[6:8])).astype(np.uint8)\n",
        "    train[\"minute\"] = train[\"session_id\"].apply(lambda x: int(str(x)[8:10])).astype(np.uint8)\n",
        "    train[\"second\"] = train[\"session_id\"].apply(lambda x: int(str(x)[10:12])).astype(np.uint8)\n",
        "    return train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObgxZGIHaf-w",
        "outputId": "16701fd1-0752-4c66-9845-d72ced721c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df1 done (23562, 7803)\n",
            "df2 done (23562, 7809)\n",
            "df3 done (23562, 7807)\n",
            "CPU times: user 2h 30min 48s, sys: 2min 25s, total: 2h 33min 14s\n",
            "Wall time: 20min 25s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# we prepare the dataset for the training by level :\n",
        "df = pl.read_csv(\"/content/train.csv\").drop([\"fullscreen\", \"hq\", \"music\"])\n",
        "df1 = df.filter(pl.col(\"level_group\")=='0-4').sort(by=['session_id','index']).with_columns(columns)\n",
        "df2 = df.filter((pl.col(\"level_group\")=='0-4') | (pl.col(\"level_group\")=='5-12')).sort(by=['session_id','elapsed_time']).with_columns(columns)\n",
        "df3 = df.filter((pl.col(\"level_group\")=='0-4') | (pl.col(\"level_group\")=='5-12') | (pl.col(\"level_group\")=='13-22')).sort(by=['session_id','elapsed_time']).with_columns(columns)\n",
        "df1.shape,df2.shape,df3.shape\n",
        "\n",
        "\n",
        "df1 = feature_engineer(df1, grp='0-4', use_extra=True, feature_suffix='')\n",
        "print('df1 done',df1.shape)\n",
        "df2 = feature_engineer(df2, grp='5-12', use_extra=True, feature_suffix='')\n",
        "print('df2 done',df2.shape)\n",
        "df3 = feature_engineer(df3, grp='13-22', use_extra=True, feature_suffix='')\n",
        "print('df3 done',df3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FMn3O1DbNq8",
        "outputId": "e113b402-9b13-4bb1-c438-94409fa514bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6744 5860 4885\n",
            "*********df1 DONE*********\n",
            "*********df2 DONE*********\n",
            "*********df3 DONE*********\n"
          ]
        }
      ],
      "source": [
        "# some cleaning...\n",
        "null1 = df1.isnull().sum().sort_values(ascending=False) / len(df1)\n",
        "null2 = df2.isnull().sum().sort_values(ascending=False) / len(df1)\n",
        "null3 = df3.isnull().sum().sort_values(ascending=False) / len(df1)\n",
        "\n",
        "drop1 = list(null1[null1>0.9].index)\n",
        "drop2 = list(null2[null2>0.9].index)\n",
        "drop3 = list(null3[null3>0.9].index)\n",
        "print(len(drop1), len(drop2), len(drop3))\n",
        "\n",
        "for col in df1.columns:\n",
        "    if df1[col].nunique()==1 and col != 'session_id':\n",
        "        drop1.append(col)\n",
        "print(\"*********df1 DONE*********\")\n",
        "for col in df2.columns:\n",
        "    if df2[col].nunique()==1 and col != 'session_id' :\n",
        "        drop2.append(col)\n",
        "print(\"*********df2 DONE*********\")\n",
        "for col in df3.columns:\n",
        "    if df3[col].nunique()==1 and col != 'session_id' :\n",
        "        drop3.append(col)\n",
        "print(\"*********df3 DONE*********\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY7SrTrfbUM9"
      },
      "outputs": [],
      "source": [
        "df1 = time_feature(df1)\n",
        "df2 = time_feature(df2)\n",
        "df3 = time_feature(df3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iafpFm8GbW3Q",
        "outputId": "e237a6bb-165d-4ceb-b3ba-60a1803dd443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We will train with 836 1836 2923 features\n",
            "We will train with 23562 users info\n"
          ]
        }
      ],
      "source": [
        "df1 = df1.set_index('session_id')\n",
        "df2 = df2.set_index('session_id')\n",
        "df3 = df3.set_index('session_id')\n",
        "\n",
        "FEATURES1 = [c for c in df1.columns if c not in drop1+['level_group']]\n",
        "FEATURES2 = [c for c in df2.columns if c not in drop2+['level_group']]\n",
        "FEATURES3 = [c for c in df3.columns if c not in drop3+['level_group']]\n",
        "df1 , df2 , df3 = df1[FEATURES1],df2[FEATURES2],df3[FEATURES3]\n",
        "print('We will train with', len(FEATURES1), len(FEATURES2), len(FEATURES3) ,'features')\n",
        "ALL_USERS = df1.index.unique()\n",
        "print('We will train with', len(ALL_USERS) ,'users info')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBVetjMDbYKZ"
      },
      "outputs": [],
      "source": [
        "# With previous training notebook (Kfold with 20 folds as performed in others notebooks) :\n",
        "estimators_lgb = [1000] * 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaBLF1DHbbub"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "targets = pd.read_csv('./train_labels.csv')\n",
        "targets['session'] = targets.session_id.apply(lambda x: int(x.split('_')[0]))\n",
        "targets['q'] = targets.session_id.apply(lambda x: int(x.split('_')[-1][1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUJgIGIdy1G3"
      },
      "outputs": [],
      "source": [
        "def f1_metric(train_data,preds):\n",
        "    labels = train_data\n",
        "    preds = [1 if i > 0.61 else 0 for i in preds]\n",
        "    return 'f1', f1_score(labels, preds, average='macro'), True\n",
        "\n",
        "\n",
        "def f1_score_metric(labels,preds,thresh=0.61):\n",
        "    preds = [1 if i > thresh else 0 for i in preds]\n",
        "    return f1_score(labels, preds, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/predict student games/weight_ensemble/weight_ensemble_v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH5B3cgVq4Da",
        "outputId": "a79cd517-5764-4cf6-aabb-4551982aab7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/predict’: File exists\n",
            "mkdir: cannot create directory ‘games/weight_ensemble/weight_ensemble_v1’: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_P1VsgEAX2V",
        "outputId": "f23c778e-a4ea-4c27-9f1a-acdc581f22af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/predict student games/weight_ensemble/weight_ensemble_v1/important_features.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import joblib\n",
        "important_features = {}\n",
        "for q in range(1, 19):\n",
        "    # USE THIS TRAIN DATA WITH THESE QUESTIONS\n",
        "    if q <= 3:\n",
        "        grp = '0-4'\n",
        "        df = df1\n",
        "        FEATURES = FEATURES1\n",
        "\n",
        "    elif q <= 13:\n",
        "        grp = '5-12'\n",
        "        df = df2\n",
        "        FEATURES = FEATURES2\n",
        "    elif q <= 22:\n",
        "        grp = '13-22'\n",
        "        df = df3\n",
        "        FEATURES = FEATURES3\n",
        "    important_features[q] = FEATURES\n",
        "\n",
        "# joblib.dump(study.best_trial.params,f'/content/drive/MyDrive/predict student games/weight/xgb_weight_1/best_threshold.pkl')\n",
        "joblib.dump(important_features,f'/content/drive/MyDrive/predict student games/weight_ensemble/weight_ensemble_v1/important_features.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lgb = np.zeros((df1.shape[0], 18))\n",
        "n_splits = 5\n",
        "gkf = StratifiedKFold(n_splits=n_splits,random_state=19990829,shuffle=True)\n",
        "\n",
        "xgb_importances_features = joblib.load('/content/drive/MyDrive/predict student games/weight/xgb_weight_new_v1/important_features_xgb.pkl')\n",
        "\n",
        "oof_dict = {}\n",
        "val_score = {}\n",
        "\n",
        "\n",
        "for q in range(1, 19):\n",
        "    # USE THIS TRAIN DATA WITH THESE QUESTIONS\n",
        "    if q <= 3:\n",
        "        grp = '0-4'\n",
        "        df = df1\n",
        "        FEATURES = FEATURES1\n",
        "        # FEATURES = xgb_importances_features[q-1][:200]\n",
        "    elif q <= 13:\n",
        "        grp = '5-12'\n",
        "        df = df2\n",
        "        FEATURES = FEATURES2\n",
        "        # FEATURES = xgb_importances_features[q-1][:500]\n",
        "    elif q <= 22:\n",
        "        grp = '13-22'\n",
        "        df = df3\n",
        "        FEATURES = FEATURES3\n",
        "        # FEATURES = xgb_importances_features[q-1][:500]\n",
        "\n",
        "    # cat_params['n_estimators'] = estimators_lgb[q - 1]\n",
        "    df = df.reset_index()\n",
        "    df['pred'] = -1\n",
        "    df = df.merge(targets.loc[targets.q == q , ['session','correct']] , left_on=['session_id'],right_on=['session'])\n",
        "    sample_label_rate = len(df.loc[df['correct']==1]) / len(df)\n",
        "\n",
        "    # FEATURES = features[str(q)]\n",
        "    class_weights = [sample_label_rate,1-sample_label_rate]\n",
        "\n",
        "    xgb_params = {\n",
        "        'booster': 'gbtree',\n",
        "        'objective': 'binary:logistic',\n",
        "        'tree_method': 'gpu_hist', #Change4\n",
        "        'eval_metric':'logloss',\n",
        "        'learning_rate': 0.02,\n",
        "        'alpha': 8,\n",
        "        'max_depth': 4,\n",
        "        'n_estimators': 9999,\n",
        "        'early_stopping_rounds': 90,\n",
        "        'subsample':0.8,\n",
        "        'colsample_bytree': 0.5,\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    # TRAIN DATA\n",
        "    print(f'Train model for question {q} Data length {len(df)} Sample Label rate {sample_label_rate}')\n",
        "    val_score[q] = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(gkf.split(df,df['correct'])):\n",
        "        df_train = df.loc[train_idx] #.reset_index(drop=True)\n",
        "        train_users = df_train.session_id.values\n",
        "        train_y = df.loc[train_idx,'correct']\n",
        "\n",
        "        df_val = df.loc[val_idx] #.reset_index(drop=True)\n",
        "        val_users = df_val.session_id.values\n",
        "        # val_y = targets[targets['session'].isin(list(val_users))].loc[targets.q == q].set_index('session')\n",
        "        val_y = df.loc[val_idx,'correct']\n",
        "\n",
        "        # train_pool = Pool(df_train[FEATURES].astype(np.float32), df_train[f\"correct\"])\n",
        "        # valid_pool = Pool(df_val[FEATURES].astype(np.float32), df_val[f\"correct\"])\n",
        "\n",
        "        # cat_params['eval_metric'] = f1_metric\n",
        "        clf =  XGBClassifier(**xgb_params)\n",
        "\n",
        "        clf.fit(df_train[FEATURES].astype('float32'), train_y,\n",
        "                eval_set=[(df_val[FEATURES].astype('float32'), val_y)],\n",
        "                verbose=0)\n",
        "        joblib.dump(clf,f'/content/drive/MyDrive/predict student games/weight/xgb_weight_new_v4/xgb_question{q}_fold{fold}.pkl')\n",
        "        clf = joblib.load(f'/content/drive/MyDrive/predict student games/weight/xgb_weight_new_v4/xgb_question{q}_fold{fold}.pkl')\n",
        "        df.loc[val_idx,'pred'] = clf.predict_proba(df_val[FEATURES].astype('float32'))[:,1]\n",
        "        print(f'question {q} fold {fold}',f1_score_metric(df.loc[val_idx,'correct'],df.loc[val_idx,'pred']))\n",
        "        val_score[q].append(f1_score_metric(df.loc[val_idx,'correct'],df.loc[val_idx,'pred']))\n",
        "    print('average' , f1_score_metric(df.loc[:,'correct'],df.loc[:,'pred']))\n",
        "    val_score[q].append(f1_score_metric(df.loc[:,'correct'],df.loc[:,'pred']))\n",
        "    oof_dict[q] = df\n",
        "    print('*'*100)"
      ],
      "metadata": {
        "id": "boS8tyt-tCtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_oof = pd.DataFrame()\n",
        "for q in range(1,19):\n",
        "  tmp = oof_dict[q]\n",
        "  tmp['q'] = q\n",
        "  tmp = tmp[['session_id','pred','q']]\n",
        "  valid_oof = pd.concat([valid_oof,tmp],axis=0)\n",
        "valid_oof = targets.merge(valid_oof,left_on=['session','q'],right_on=['session_id','q'])\n",
        "\n",
        "best_score = 0\n",
        "best_threshold = 0\n",
        "\n",
        "for thresh in np.arange(0.4,0.81,0.01):\n",
        "  score = f1_score_metric(valid_oof['correct'],valid_oof['pred'],thresh)\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_threshold = thresh\n",
        "print('best_score',best_score,'best_threshold',best_threshold)"
      ],
      "metadata": {
        "id": "oSTl_aqX7-ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_importances_dict = {}\n",
        "for i in range(18):\n",
        "  models = models_list[i]\n",
        "  feature_names = importances_dict[i+1]\n",
        "  feature_importances = np.array([0] * len(feature_names))\n",
        "  for model in models:\n",
        "    feature_importances = feature_importances + model.feature_importances_ / 5\n",
        "  features_importances_dict[i] = np.array(feature_names)[feature_importances.argsort()[::-1]]\n",
        "joblib.dump(features_importances_dict,'/content/drive/MyDrive/predict student games/weight/xgb_weight_new_v1/important_features_xgb.pkl')"
      ],
      "metadata": {
        "id": "xLggZQBX9EQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwTa2v5sz_aQ"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lgb = np.zeros((df1.shape[0], 18))\n",
        "n_splits = 5\n",
        "gkf = StratifiedKFold(n_splits=n_splits,random_state=19990829,shuffle=True)\n",
        "\n",
        "xgb_importances_features = joblib.load('/content/drive/MyDrive/predict student games/weight/xgb_weight_new_v1/important_features_xgb.pkl')\n",
        "\n",
        "oof_dict = {}\n",
        "val_score = {}\n",
        "\n",
        "\n",
        "for q in range(1, 19):\n",
        "    # USE THIS TRAIN DATA WITH THESE QUESTIONS\n",
        "    if q <= 3:\n",
        "        grp = '0-4'\n",
        "        df = df1\n",
        "        FEATURES = FEATURES1\n",
        "        FEATURES = xgb_importances_features[q-1][:200]\n",
        "    elif q <= 13:\n",
        "        grp = '5-12'\n",
        "        df = df2\n",
        "        FEATURES = FEATURES2\n",
        "        FEATURES = xgb_importances_features[q-1][:500]\n",
        "    elif q <= 22:\n",
        "        grp = '13-22'\n",
        "        df = df3\n",
        "        FEATURES = FEATURES3\n",
        "        FEATURES = xgb_importances_features[q-1][:500]\n",
        "\n",
        "    # cat_params['n_estimators'] = estimators_lgb[q - 1]\n",
        "    df = df.reset_index()\n",
        "    df['pred'] = -1\n",
        "    df = df.merge(targets.loc[targets.q == q , ['session','correct']] , left_on=['session_id'],right_on=['session'])\n",
        "    sample_label_rate = len(df.loc[df['correct']==1]) / len(df)\n",
        "\n",
        "    # FEATURES = features[str(q)]\n",
        "    class_weights = [sample_label_rate,1-sample_label_rate]\n",
        "\n",
        "    xgb_params = {\n",
        "        'booster': 'gbtree',\n",
        "        'objective': 'binary:logistic',\n",
        "        'tree_method': 'gpu_hist', #Change4\n",
        "        'eval_metric':'logloss',\n",
        "        'learning_rate': 0.02,\n",
        "        'alpha': 8,\n",
        "        'max_depth': 4,\n",
        "        'n_estimators': 9999,\n",
        "        'early_stopping_rounds': 90,\n",
        "        'subsample':0.8,\n",
        "        'colsample_bytree': 0.5,\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    # TRAIN DATA\n",
        "    print(f'Train model for question {q} Data length {len(df)} Sample Label rate {sample_label_rate}')\n",
        "    val_score[q] = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(gkf.split(df,df['correct'])):\n",
        "        df_train = df.loc[train_idx] #.reset_index(drop=True)\n",
        "        train_users = df_train.session_id.values\n",
        "        train_y = df.loc[train_idx,'correct']\n",
        "\n",
        "        df_val = df.loc[val_idx] #.reset_index(drop=True)\n",
        "        val_users = df_val.session_id.values\n",
        "        # val_y = targets[targets['session'].isin(list(val_users))].loc[targets.q == q].set_index('session')\n",
        "        val_y = df.loc[val_idx,'correct']\n",
        "\n",
        "        # train_pool = Pool(df_train[FEATURES].astype(np.float32), df_train[f\"correct\"])\n",
        "        # valid_pool = Pool(df_val[FEATURES].astype(np.float32), df_val[f\"correct\"])\n",
        "\n",
        "        # cat_params['eval_metric'] = f1_metric\n",
        "        clf =  XGBClassifier(**xgb_params)\n",
        "\n",
        "        clf.fit(df_train[FEATURES].astype('float32'), train_y,\n",
        "                eval_set=[(df_val[FEATURES].astype('float32'), val_y)],\n",
        "                verbose=0)\n",
        "        joblib.dump(clf,f'/content/drive/MyDrive/predict student games/weight_ensemble/weight_ensemble_v1/xgb_question{q}_fold{fold}.pkl')\n",
        "        clf = joblib.load(f'/content/drive/MyDrive/predict student games/weight_ensemble/weight_ensemble_v1/xgb_question{q}_fold{fold}.pkl')\n",
        "        df.loc[val_idx,'pred'] = clf.predict_proba(df_val[FEATURES].astype('float32'))[:,1]\n",
        "        print(f'question {q} fold {fold}',f1_score_metric(df.loc[val_idx,'correct'],df.loc[val_idx,'pred']))\n",
        "        val_score[q].append(f1_score_metric(df.loc[val_idx,'correct'],df.loc[val_idx,'pred']))\n",
        "    print('average' , f1_score_metric(df.loc[:,'correct'],df.loc[:,'pred']))\n",
        "    val_score[q].append(f1_score_metric(df.loc[:,'correct'],df.loc[:,'pred']))\n",
        "    oof_dict[q] = df\n",
        "    print('*'*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teOvUPickC-V",
        "outputId": "c79dbb09-233a-40e0-d7a8-d3f3bee51938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train model for question 1 Data length 23562 Sample Label rate 0.7274849333672863\n",
            "question 1 fold 0 0.6679218757067861\n",
            "question 1 fold 1 0.6845672725148724\n",
            "question 1 fold 2 0.6770856548763012\n",
            "question 1 fold 3 0.6751107542151373\n",
            "question 1 fold 4 0.675382594640836\n",
            "average 0.6760045998271489\n",
            "****************************************************************************************************\n",
            "Train model for question 2 Data length 23562 Sample Label rate 0.9788218317630082\n",
            "question 2 fold 0 0.5286825437979185\n",
            "question 2 fold 1 0.4943133047210301\n",
            "question 2 fold 2 0.5219942186568965\n",
            "question 2 fold 3 0.5387166953541486\n",
            "question 2 fold 4 0.5312758843140442\n",
            "average 0.5233394824670865\n",
            "****************************************************************************************************\n",
            "Train model for question 3 Data length 23562 Sample Label rate 0.9340039045921399\n",
            "question 3 fold 0 0.5291430267687431\n",
            "question 3 fold 1 0.5186455624308732\n",
            "question 3 fold 2 0.5120863491137285\n",
            "question 3 fold 3 0.524360906368729\n",
            "question 3 fold 4 0.5295235655260735\n",
            "average 0.5227643985243061\n",
            "****************************************************************************************************\n",
            "Train model for question 4 Data length 23562 Sample Label rate 0.7982344452932688\n",
            "question 4 fold 0 0.6811895661018488\n",
            "question 4 fold 1 0.6781701259841195\n",
            "question 4 fold 2 0.6901655731775606\n",
            "question 4 fold 3 0.6769254762729325\n",
            "question 4 fold 4 0.6894452784058627\n",
            "average 0.6831821991393026\n",
            "****************************************************************************************************\n",
            "Train model for question 5 Data length 23562 Sample Label rate 0.5482556659027247\n",
            "question 5 fold 0 0.6455935264108518\n",
            "question 5 fold 1 0.6533362058594037\n",
            "question 5 fold 2 0.6463735357007684\n",
            "question 5 fold 3 0.6522766787478267\n",
            "question 5 fold 4 0.6493096771345136\n",
            "average 0.6493867794016568\n",
            "****************************************************************************************************\n",
            "Train model for question 6 Data length 23562 Sample Label rate 0.7759528053645701\n",
            "question 6 fold 0 0.6438588101942865\n",
            "question 6 fold 1 0.6530516325582976\n",
            "question 6 fold 2 0.6432210876818653\n",
            "question 6 fold 3 0.6449109436022575\n",
            "question 6 fold 4 0.6641673094707149\n",
            "average 0.6498522402448691\n",
            "****************************************************************************************************\n",
            "Train model for question 7 Data length 23562 Sample Label rate 0.7360580595874714\n",
            "question 7 fold 0 0.6359948076528288\n",
            "question 7 fold 1 0.6286830741296875\n",
            "question 7 fold 2 0.6248936524695794\n",
            "question 7 fold 3 0.6444745654448241\n",
            "question 7 fold 4 0.6468473320467866\n",
            "average 0.6361440438239848\n",
            "****************************************************************************************************\n",
            "Train model for question 8 Data length 23562 Sample Label rate 0.6172226466344113\n",
            "question 8 fold 0 0.5820599300963957\n",
            "question 8 fold 1 0.5783394454118826\n",
            "question 8 fold 2 0.5842895892959145\n",
            "question 8 fold 3 0.5637965767405905\n",
            "question 8 fold 4 0.5806369860501426\n",
            "average 0.5778925436264462\n",
            "****************************************************************************************************\n",
            "Train model for question 9 Data length 23562 Sample Label rate 0.7362702656820304\n",
            "question 9 fold 0 0.6361133496509208\n",
            "question 9 fold 1 0.6451774495508404\n",
            "question 9 fold 2 0.642827699659336\n",
            "question 9 fold 3 0.6447506957524548\n",
            "question 9 fold 4 0.6288147356369587\n",
            "average 0.6395535642573947\n",
            "****************************************************************************************************\n",
            "Train model for question 10 Data length 23562 Sample Label rate 0.5054324760207113\n",
            "question 10 fold 0 0.6030443687526437\n",
            "question 10 fold 1 0.6133676581480432\n",
            "question 10 fold 2 0.6025626192877185\n",
            "question 10 fold 3 0.6060462437987244\n",
            "question 10 fold 4 0.6008495218170951\n",
            "average 0.6052491003851452\n",
            "****************************************************************************************************\n",
            "Train model for question 11 Data length 23562 Sample Label rate 0.6436210847975554\n",
            "question 11 fold 0 0.6258191596990504\n",
            "question 11 fold 1 0.6190857166483561\n",
            "question 11 fold 2 0.6112709121808224\n",
            "question 11 fold 3 0.6129597938588418\n",
            "question 11 fold 4 0.6185166525322019\n",
            "average 0.6175275408827574\n",
            "****************************************************************************************************\n",
            "Train model for question 12 Data length 23562 Sample Label rate 0.8629573041337747\n",
            "question 12 fold 0 0.5234160724914062\n",
            "question 12 fold 1 0.5249925778166331\n",
            "question 12 fold 2 0.523385661110212\n",
            "question 12 fold 3 0.5093480907096365\n",
            "question 12 fold 4 0.5369216098383778\n",
            "average 0.5235830513193549\n",
            "****************************************************************************************************\n",
            "Train model for question 13 Data length 23562 Sample Label rate 0.27510398098633393\n",
            "question 13 fold 0 0.48700837958277055\n",
            "question 13 fold 1 0.4981892538735667\n",
            "question 13 fold 2 0.49641456328113\n",
            "question 13 fold 3 0.5014182208044169\n",
            "question 13 fold 4 0.49245803709470964\n",
            "average 0.4951175130857801\n",
            "****************************************************************************************************\n",
            "Train model for question 14 Data length 23562 Sample Label rate 0.7076648841354723\n",
            "question 14 fold 0 0.6435344415782938\n",
            "question 14 fold 1 0.6412446999168461\n",
            "question 14 fold 2 0.6604349565402021\n",
            "question 14 fold 3 0.6382989262343352\n",
            "question 14 fold 4 0.6338317791388475\n",
            "average 0.6434317221801212\n",
            "****************************************************************************************************\n",
            "Train model for question 15 Data length 23562 Sample Label rate 0.4810287751464222\n",
            "question 15 fold 0 0.6366972691160029\n",
            "question 15 fold 1 0.6301302238919324\n",
            "question 15 fold 2 0.6224610569390734\n",
            "question 15 fold 3 0.6273102124660759\n",
            "question 15 fold 4 0.637021183566199\n",
            "average 0.6307221596647119\n",
            "****************************************************************************************************\n",
            "Train model for question 16 Data length 23562 Sample Label rate 0.7348697054579407\n",
            "question 16 fold 0 0.523730998751983\n",
            "question 16 fold 1 0.5280298023192934\n",
            "question 16 fold 2 0.5221699800239015\n",
            "question 16 fold 3 0.5185319396151418\n",
            "question 16 fold 4 0.5083423156944827\n",
            "average 0.5202220398200059\n",
            "****************************************************************************************************\n",
            "Train model for question 17 Data length 23562 Sample Label rate 0.6878023936847466\n",
            "question 17 fold 0 0.5572546337798588\n",
            "question 17 fold 1 0.5580427744961545\n",
            "question 17 fold 2 0.5686589549786792\n",
            "question 17 fold 3 0.5479774257641138\n",
            "question 17 fold 4 0.5574200245847574\n",
            "average 0.557898630154886\n",
            "****************************************************************************************************\n",
            "Train model for question 18 Data length 23562 Sample Label rate 0.9506408624055683\n",
            "question 18 fold 0 0.5034194398813877\n",
            "question 18 fold 1 0.5118005463812798\n",
            "question 18 fold 2 0.4998685777930635\n",
            "question 18 fold 3 0.49091753276902744\n",
            "question 18 fold 4 0.507607204555207\n",
            "average 0.5027469927302719\n",
            "****************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_oof = pd.DataFrame()\n",
        "for q in range(1,19):\n",
        "  tmp = oof_dict[q]\n",
        "  tmp['q'] = q\n",
        "  # if q in (2,18):\n",
        "  #     tmp['q'] = 1\n",
        "  tmp = tmp[['session_id','pred','q']]\n",
        "  valid_oof = pd.concat([valid_oof,tmp],axis=0)\n",
        "valid_oof = targets.merge(valid_oof,left_on=['session','q'],right_on=['session_id','q'])\n",
        "\n",
        "best_score = 0\n",
        "best_threshold = 0\n",
        "\n",
        "for thresh in np.arange(0.4,0.81,0.01):\n",
        "  score = f1_score_metric(valid_oof['correct'],valid_oof['pred'],thresh)\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_threshold = thresh\n",
        "print('best_score',best_score,'best_threshold',best_threshold)\n",
        "print('score_group1',f1_score_metric(valid_oof.loc[valid_oof.q <= 3 , 'correct'],valid_oof.loc[valid_oof.q <= 3 ,'pred'],best_threshold))\n",
        "print('score_group2',f1_score_metric(valid_oof.loc[(valid_oof.q > 3) & (valid_oof.q <= 13) , 'correct'],valid_oof.loc[(valid_oof.q > 3) & (valid_oof.q <= 13) ,'pred'],best_threshold))\n",
        "print('score_group3',f1_score_metric(valid_oof.loc[(valid_oof.q > 13) & (valid_oof.q <= 18) , 'correct'],valid_oof.loc[(valid_oof.q > 13) & (valid_oof.q <= 18) ,'pred'],best_threshold))"
      ],
      "metadata": {
        "id": "LEjnWfeMkDAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38930f61-80c9-4395-d882-776439e8e2a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_score 0.7017594218494563 best_threshold 0.6200000000000002\n",
            "score_group1 0.6986048427682099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j-9yyOs6yZRZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}